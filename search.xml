<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AboutMe</title>
    <url>/2020/02/17/AboutMe/</url>
    <content><![CDATA[<h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2>]]></content>
  </entry>
  <entry>
    <title>灾备切换</title>
    <url>/2020/07/15/DRP/</url>
    <content><![CDATA[<ul>
<li><a href="https://zhuanlan.zhihu.com/p/159435704">https://zhuanlan.zhihu.com/p/159435704</a></li>
</ul>
]]></content>
      <tags>
        <tag>DRP</tag>
        <tag>灾备</tag>
      </tags>
  </entry>
  <entry>
    <title>如何持续改进业务连续性管理体系</title>
    <url>/2020/07/15/BCM/</url>
    <content><![CDATA[<ul>
<li><a href="https://zhuanlan.zhihu.com/p/157287793">https://zhuanlan.zhihu.com/p/157287793</a></li>
</ul>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>大多数企业在某些时候，不得不对可能破坏或威胁其日常业务运作的事件作出回应。<strong>一个成功的企业连续性管理（BCM）的程序，能够应对任何潜在的干扰反应，是组织必不可少的；</strong>完善的业务连续性管理系统（BCMS）不仅能帮助你的组织从灾难中恢复，也将防止可能出现的任何运作中断（例如错过了最后交期，困扰客户，或者直接的经济损失而带来的声誉损失等）。</p>
<span id="more"></span>
<h1 id="一、服务概述"><a href="#一、服务概述" class="headerlink" title="一、服务概述"></a>一、服务概述</h1><p>ISO 22301 业务连续性管理体系框架能够帮助企业制定一套一体化的管理流程计划，使企业对潜在的灾难加以辨别分析，帮助其确定可能发生的冲击对企业运作造成的威胁，并提供一个有效的管理机制来阻止或抵消这些威胁，减少灾难事件给企业带来的损失。</p>
<p>整体BCM方案必须通过确定范围、风险评估、业务连续性管理战略、业务连续性目标、开发计划、教育训练、演习、测试、审查和连续改进等活动得到管理。</p>
<p>BCMS也应包括风险评估（RA）和业务影响分析（BIA），这是 ISO 22301 的内在组成部分和基本组成部分，是确定优先活动、受依赖和资源应支持的关键产品和服务，他们的失败将对组织产生的影响。</p>
<h1 id="二、ISO-22301-管理点"><a href="#二、ISO-22301-管理点" class="headerlink" title="二、ISO 22301 管理点"></a>二、ISO 22301 管理点</h1><p>成本效益分析应该用来检讨所有业务连续性安排，如服务等级协议、共享空间、劳动力，紧急事件中或紧急事件后的替代工艺和技术。因为每年需要进行计划的基于业务优先级和风险的一系列演练，所以建议进行定期评审整体的管理中断事件的业务连续性能力。这样设计是为了突出任一弱点区域，给你一个更好的能力管理所有类型的潜在事故或灾难。</p>
<h1 id="三、具体内容（BCM的过程包括六个步骤）"><a href="#三、具体内容（BCM的过程包括六个步骤）" class="headerlink" title="三、具体内容（BCM的过程包括六个步骤）"></a>三、具体内容（BCM的过程包括六个步骤）</h1><p><img src="/images/v2-db53d9d1fc35915a6fd14b8366d16f8f_r.jpg"></p>
<p><img src="/images/v2-db53d9d1fc35915a6fd14b8366d16f8f_1440w.jpg"></p>
<h2 id="步骤1-BCM方案管理"><a href="#步骤1-BCM方案管理" class="headerlink" title="步骤1 - BCM方案管理"></a>步骤1 - BCM方案管理</h2><p>方案管理的建立（如果有必要）和维护组织的业务连续性能力，并与组织的规模和复杂程度相适宜。<br>在这第一步骤中，文件化BCM的范围和BCM核心团队及其角色和职责被批准是关键问题。</p>
<h2 id="步骤2-理解组织"><a href="#步骤2-理解组织" class="headerlink" title="步骤2 - 理解组织"></a>步骤2 - 理解组织</h2><p>与此步骤相关的活动是提供决定组织产品和服务的优先次序的信息，识别关键的支持活动及其资源，业务影响分析（BIA）和风险评估也是这个阶段的关键部分。</p>
<h2 id="步骤3-确定业务连续性管理策略"><a href="#步骤3-确定业务连续性管理策略" class="headerlink" title="步骤3 - 确定业务连续性管理策略"></a>步骤3 - 确定业务连续性管理策略</h2><p>允许选择适当的响应优先的业务互动，使得组织遇到中断后能够在预先设定的时间内恢复和继续提供产品和服务，在开发持续计划之前需预先定义的关键词，如MAO、MBCO、MTPD、RTO和RPO。</p>
<h2 id="步骤4-开发和实施BCM响应"><a href="#步骤4-开发和实施BCM响应" class="headerlink" title="步骤4 - 开发和实施BCM响应"></a>步骤4 - 开发和实施BCM响应</h2><p>涵盖了开发应急响应、危机管理和业务连续性计划，详细阐明在中断中及中断后采取维持和复原优先级业务过程，或运营到预先定义水平的步骤。</p>
<h2 id="步骤5-演练、维护和评审BCM安排"><a href="#步骤5-演练、维护和评审BCM安排" class="headerlink" title="步骤5 - 演练、维护和评审BCM安排"></a>步骤5 - 演练、维护和评审BCM安排</h2><p>在计划的时间间隔内进行演练，以达到业务连续性目标和识别改进机会，可以让组织证明其战略及计划和与目标的符合性。</p>
<h2 id="步骤6-嵌入BCM在组织文化中"><a href="#步骤6-嵌入BCM在组织文化中" class="headerlink" title="步骤6- 嵌入BCM在组织文化中"></a>步骤6- 嵌入BCM在组织文化中</h2><p>这使BCM成为组织的核心价值的一部分，在组织所有相关方的各层级中灌输信心，应对中断；企业需要培训那些负责执行BCM、响应中断，以及受计划影响的相关人员；组织不仅应把计划落到实处，还应定期审查计划得到更新，确保其有效性；组织可以考虑将多个管理体系整合，以最大限度的提高效率。</p>
<h1 id="四、认证益处"><a href="#四、认证益处" class="headerlink" title="四、认证益处"></a>四、认证益处</h1><p>ISO 22301 验审活动有助于通过各层级有计划、有效的BCM改善业务，包括：</p>
<ol>
<li>组织内识别和理解关键业务过程及其中断的影响</li>
<li>增强组织的弹性、恢复能力以及持续生存能力水平</li>
<li>具备超越弹性较弱的竞争对手的优势</li>
<li>正面的讯息传达给媒体和利益相关者，以应对危机处理</li>
<li>提升保险公司对组织风险管理的印象，从而降低保费</li>
<li>符合监管机构、保险公司、商业合作伙伴和其他主要利益相关者的期望</li>
<li>在事故、破坏甚至灾难发生时显著降低财务影响</li>
<li>增加组织和员工双方的生存机会</li>
<li>通过展示具备专业的管理中断的方法而保持甚至提升声誉</li>
<li>如合同或协议的承诺，在可接受的预先定义的级别，及时和有序应对事件和业务中断，保证业务连续运营</li>
<li>鼓励跨团队和跨组织的协调</li>
<li>通过场景演练，展示可信的响应能力</li>
<li>以可见的证据证明整体风险管理的管理承诺</li>
</ol>
]]></content>
      <tags>
        <tag>BCM</tag>
      </tags>
  </entry>
  <entry>
    <title>ITIL思想</title>
    <url>/2020/03/07/ITIL%E6%80%9D%E6%83%B3/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>ITIL</tag>
      </tags>
  </entry>
  <entry>
    <title>LVS</title>
    <url>/2020/04/13/LVS/</url>
    <content><![CDATA[<ul>
<li><a href="https://blog.csdn.net/liwei0526vip/article/details/103104393">https://blog.csdn.net/liwei0526vip/article/details/103104393</a></li>
<li><a href="https://blog.csdn.net/liwei0526vip/article/details/103104483">https://blog.csdn.net/liwei0526vip/article/details/103104483</a></li>
<li><a href="https://blog.csdn.net/liwei0526vip/article/details/103104496">https://blog.csdn.net/liwei0526vip/article/details/103104496</a></li>
</ul>
<h1 id="一、netfilter基本原理"><a href="#一、netfilter基本原理" class="headerlink" title="一、netfilter基本原理"></a>一、netfilter基本原理</h1><p>LVS 是基于 Linux 内核中 netfilter 框架实现的负载均衡系统，所以要学习 LVS 之前必须要先简单了解 netfilter 基本工作原理。netfilter 其实很复杂也很重要，平时我们说的 Linux 防火墙就是 netfilter，不过我们平时操作的都是 iptables，iptables 只是用户控件编写和传递规则的工具而已，真正工作的是 netfilter。通过下图可以简单了解下 netfilter 的工作机制：</p>
<p><img src="/images/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2xpd2VpMDUyNnZpcC9ibG9naW1nL21hc3Rlci9sYjAwM25ldGZpbHRlci5wbmc.jfif"></p>
<p>netfilter 是内核态的 Linux 防火墙机制，作为一个通用、抽象的框架，提供了一整套的 hook 函数管理机制，提供诸如数据包过滤、网络地址转换、基于协议类型的连接跟踪的功能。</p>
<p>通俗点将，就是 netfilter 提供了一种机制，可以在数据包流经过程中，根据规则设置若干个关卡（hook函数）来执行相关的操作。netfilter 总共设置了5个点，包括：PREROUTING\INPUT\FORWARD\OUTPUT\POSTROUTING</p>
<ul>
<li>PREROUTING：刚刚进入网络层，还未进行路由查找的包，通过此处</li>
<li>INPUT：通过路由查找，确定发往本机的包，通过此处</li>
<li>FORWARD：经过路由查找，要转发的包，在 POSTROUTING之前</li>
<li>OUTPUT：从本机进程刚发的包，通过此处</li>
<li>POSTROUTING：进入网络层已经经过路由查找，确定转发，将要离开本设备的包，通过此处</li>
</ul>
<p>当一个数据包进入网卡，经过链路层之后进入网络层就会到达 PREROUTING ，接着根据目标IP地址进行路由查找，如果目标IP是本机，数据包继续传递到 INPUT 上，经过协议栈后根据目标端口将数据送到响应的应用程序；应用程序处理请求后将响应数据包发送到 OUTPUT 上，最终通过 POSTROUTING 后发送出网卡。如果目标IP不是本机，而且服务器开启了forward 参数，就会将数据包传递送给 FORWARD 上，最后通过 POSTROUTING 后发送出网卡。</p>
<h1 id="二、LVS基本原理"><a href="#二、LVS基本原理" class="headerlink" title="二、LVS基本原理"></a>二、LVS基本原理</h1><p>LVS是基于 netfilter 框架，主要工作于 INPUT 链上，在 INPUT 上注册 <code>ip_vs_in</code> HOOK 函数，进行 IPVS 主流程，大概原理如图所示：</p>
<p><img src="/images/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2xpd2VpMDUyNnZpcC9ibG9naW1nL21hc3Rlci9sYjAwNGlwdnMucG5n.jfif"></p>
<ul>
<li>当用户访问 <a href="http://www.sina.com.cn/">www.sina.com.cn</a> 时，用户数据通过层层网络，最后通过交换机进入 LVS 服务器网卡，并进入内核网络层。</li>
<li>进入 PREROUTING 后经过路由查找，确定访问的目的 VIP 是本机 IP 地址，所以数据包进入到 INPUT 链上</li>
<li>IPVS 是工作在 INPUT 链上，会根据访问的 <code>vip+port</code> 判断请求是否 IPVS 服务，如果是则调用注册的 IPVS HOOK 函数，进行 IPVS 相关主流程，强行修改数据包的相关数据，并将数据包发往 POSTROUTING 链上。</li>
<li>POSTROUTING  上收到数据包后，根据目标 IP 地址（后端服务器），通过路由选路，将数据包最终发往后端的服务器上。</li>
</ul>
<p>开源 LVS 版本有3中工作模式，每种模式的工作原理截然不同，各种模式有各自的优缺点，分别适合不同的应用场景，不过最终本质的功能都是能够实现均衡的流量调度和良好的扩展性。主要包括以下三种模式：</p>
<ul>
<li>DR模式</li>
<li>NAT模式</li>
<li>Tunnel模式</li>
</ul>
<p>另外必要要说的模式是FULLNAT，这个模式在开源版本中是没有的，代码没有合并进入内核主线版本。后面单独介绍 FullNAT 模式。</p>
<h1 id="三、DR-模式实现原理"><a href="#三、DR-模式实现原理" class="headerlink" title="三、DR 模式实现原理"></a>三、DR 模式实现原理</h1><p>LVS 基本原理中描述的比较简单，表述的是比较通用的流程。下面针对 DR 模式的具体实现原理，详细阐述 DR 模式是如何工作的。</p>
<p><img src="/images/20200319150450262.png"></p>
<p>其实 DR 模式是最常用的工作模式，因为它强大的性能。下边以一次请求和响应数据流的过程来秒数 DR 模式的具体原理</p>
<h2 id="实现原理与过程"><a href="#实现原理与过程" class="headerlink" title="实现原理与过程"></a>实现原理与过程</h2><ul>
<li>① 当客户端请求 <a href="http://www.sina.com.cn/">www.sina.com.cn</a> 主页，经过 DNS 解析到 IP 后，向新浪服务器发送请求数据，数据包经过层层网络到达新浪负载均衡 LVS 服务器，到达 LVS 网卡时的数据包：源 IP 是客户端 IP 地址 CIP，目的 IP 是新浪对外的服务器 IP 地址，也就是说 VIP；此时源 MAC 地址是 CMAC，其实是 LVS 连接的路由器的 MAC 地址（为了容易理解记为 CMAC），目标 MAC 地址是 VIP 对应的 MAC，记为 VMAC。</li>
<li>② 数据包到达网卡后，经过链路层到达 PREROUTING 未知（刚进入网络层），查找路由发现目标 IP 是 LVS 的 VIP，就会递送到 INPUT 链上，此时数据包 MAC、IP、Port 都没有修改。</li>
<li>③ 数据包到达 INPUT 链，INPUT 是 LVS 主要工作的位置。此时 LVS 会根据目的 IP 和 Port 来确认是否是 LVS 定义的服务，如果是定义过的 VIP 服务，就会根据配置的 Service 信息，从 RealServer 中选择一个作为后端服务器 RS1，然后以 RS1 作为目标查找 Out 方向的路由，确定下一跳信息以及数据包要通过哪个网卡发出。最后将数据包通过 INET_HOOK 到 OUTPUT 链上（Out 方向刚从四层进入网络层）。</li>
<li>④ 数据包通过 POSTROUTING 链后，从网络层转到链路层，将目的 MAC 地址修改为 RealServer 服务器 MAC 地址，记为 RMAC；而源 MAC 地址修改为 LVS与 RS 同网段的 selfIP 对应的 MAC 地址，记为 DMAC。此时，数据包通过交换机转发给 RealServer 服务器。</li>
<li>⑤ 请求数据包到达 RealServer 服务器后，链路层检查目的 MAC 是自己网卡地址。到了网络层，查找路由，目的 IP 是 VIP（lo 上配置 VIP），判定是本机主机的数据包。经过协议栈后拷贝至应用程序（比如这里是 nginx 服务器），nginx 响应请求后，产生响应数据包。以目的 VIP 为 dst 查找 Out 路由，确定下一跳信息和发送网卡设备信息，发送数据包。此时数据包源、目的 IP 分别是 VIP、CIP，而源 MAC 地址是 RS1 的RMAC，目的 MAC 地址是下一跳（路由器）的 MAC地址，记为 CMAC，然后数据包通过 RS1 相连的路由器转发给真正客户端。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">从整个过程可以看出，DR模式LVS逻辑非常简单，数据包通过路由方式直接转发给RS，而且响应数据包时由RS服务器直接发送给客户端，不经过LVS。我们知道一般请求数据包会比较小，响应报文较大，经过LVS的数据包基本上都是小包，上述几条因素是LVS的DR模式性能强大的主要原因。</span><br></pre></td></tr></table></figure>

<h2 id="优缺点和使用场景"><a href="#优缺点和使用场景" class="headerlink" title="优缺点和使用场景"></a>优缺点和使用场景</h2><ul>
<li>DR模式的优点<ol>
<li>响应数据不经过lvs，性能高</li>
<li>对数据包修改小，信息保存完整（携带客户端源IP）</li>
</ol>
</li>
<li>DR模式的缺点<ol>
<li>lvs与rs必须在同一个物理网络，不支持跨机房</li>
<li>rs上必须配置lo和其他内核参数</li>
<li>不支持端口映射</li>
</ol>
</li>
<li>DR模式的使用场景<br>  如果对性能要求非常高，可以首选DR模式，而且可以穿透客户端源IP地址。</li>
</ul>
<h1 id="四、NAT-模式实现原理"><a href="#四、NAT-模式实现原理" class="headerlink" title="四、NAT 模式实现原理"></a>四、NAT 模式实现原理</h1><p>lvs的第二种工作模式是NAT模式，下图详细介绍了数据包从客户端进入lvs后转发到rs，后经rs再次将响应数据转发给lvs，由lvs将数据包回复给客户端的整个过程。<br><img src="/images/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2xpd2VpMDUyNnZpcC9ibG9naW1nL21hc3Rlci9sYjAwNm5hdC5wbmc.jfif"></p>
<h2 id="实现原理与过程-1"><a href="#实现原理与过程-1" class="headerlink" title="实现原理与过程"></a>实现原理与过程</h2><ul>
<li>① 用户请求数据包经过层层网络，到达lvs网卡，此时数据包源IP是CIP，目的IP是VIP。</li>
<li>② 经过网卡进入网络层 PREROUTING 位置，根据目的的IP查找路由，确认是本机IP，将数据包转发到 INPUT 上，此时源、目的的IP都未发生变化。</li>
<li>③ 到达lvs后，通过目的的IP和目的port查找是否为LVS服务。若是IPVS服务，则会选择一个RS作为后端服务器，将数据包的目的IP修改为RIP，并以RIP为目的IP查找路由信息，确定下一跳和出口信息，将数据包转发至output上。</li>
<li>④ 修改后的数据包经过 POSTROUTING 和链路层处理后，到达RS服务器，此时的数据包源IP是CIP，目的IP是RIP</li>
<li>⑤ 到达RS服务器的数据包经过链路层和网络层检查后，被送往用户空间nginx程序。nginx程序处理完毕，发送响应数据包，由于RS上默认网关配置为lvs设备IP，所以nginx服务器会将数据包转发至下一跳，也就是说lvs服务器。此时数据包源IP是RIP，目的IP是CIP。</li>
<li>⑥ lvs服务器收到RS响应数据包后，根据路由查找，发现目的IP不是本机IP，而且lvs服务器开启了转发模式，所以讲数据包转发给forward链，此处数据包未作修改。</li>
<li>⑦ lvs收到响应数据保护后，根据目的IP和目的port查找服务和连接表，将源IP改为VIP，通过路由查找，确定下一跳和出口信息，将数据包发送至网关，经过复杂的网络到达用户客户端，最红完成了一次请求和响应的交互。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">NAT模式双向流量都经过LVS，因此NAT模式性能会存在一定的瓶颈。不过与其他模式区别的事，NAT支持端口映射，且支持Windows操作系统。</span><br></pre></td></tr></table></figure>

<h2 id="优缺点和使用场景-1"><a href="#优缺点和使用场景-1" class="headerlink" title="优缺点和使用场景"></a>优缺点和使用场景</h2><ul>
<li>NAT模式的优点<ol>
<li>能够支持windows操作系统</li>
<li>支持端口映射。如果rs端口与vport不一致，lvs除了修改目的ip，也会修改dport以支持端口映射</li>
</ol>
</li>
<li>NAT模式的缺点<ol>
<li>后端rs需要配置网关</li>
<li>双向流量对lvs负载压力比较大</li>
</ol>
</li>
<li>NAT模式的使用场景<br>  windows系统，使用lvs，必须选择lvs模式。</li>
</ul>
<h1 id="五、Tunnel-模式实现原理"><a href="#五、Tunnel-模式实现原理" class="headerlink" title="五、Tunnel 模式实现原理"></a>五、Tunnel 模式实现原理</h1><p>Tunnel 模式在国内使用比较少。它也是一种单臂的模式，只有请求数据会经过lvs，响应数据直接从后端服务器发送给客户端，性能也很强大，同时支持跨机房。</p>
<p><img src="/images/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL2xpd2VpMDUyNnZpcC9ibG9naW1nL21hc3Rlci9sYjAwN3R1bm5lbC5wbmc.jfif"></p>
<h2 id="实现原理与过程-2"><a href="#实现原理与过程-2" class="headerlink" title="实现原理与过程"></a>实现原理与过程</h2><ul>
<li>① 用户请求数据包经过层层网络，到达lvs网卡，此时数据包源IP是cip，目的IP是VIP。</li>
<li>② 经过网卡进入网络层 PREROUTIONG 位置，根据目的ip查找路由，确认是本机IP，将数据包转发到INPUT链上，到达lvs，此时源、目的ip<br>都未发生变化。</li>
<li>③ 到达lvs后，通过目的ip和目的port查找是否为IPVS服务。若是IPVS服务，则会选择一个rs作为后端服务器，以rip为目的ip查找路由信息，确定下一跳、dev等信息，然后IP头部前边额外增加一个IP头（以dip为源，rip为目的ip），将数据包转发至OUTPUT 上。</li>
<li>④ 数据包根据路由信息最终经过lvs网卡，发送至路由器网关，通过网络到达后端服务器。</li>
<li>⑤ 后端服务器收到数据包后，ipip模块将Tunnel头部卸载，正常看到源ip是cip，目的ip是vip，由于在tunl0上配置vip，路由查找后判定为本机ip，送往应用程序。应用程序nginx正常响应数据后以vip为源ip，cip为目的ip的数据包发送至网卡，最终到达客户。</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Tunnel模式具备DR模式的高性能，又支持跨机房访问，听起来比较完美。不过国内运营商有一定特色型，比如RS的响应数据包的源IP为VIP，VIP与后端服务器可能存在跨运营商的情况，有可能被运营商的策略封掉。</span><br></pre></td></tr></table></figure>

<h2 id="优缺点和使用场景-2"><a href="#优缺点和使用场景-2" class="headerlink" title="优缺点和使用场景"></a>优缺点和使用场景</h2><ul>
<li>Tunnel模式的优点<ol>
<li>单臂模式，对lvs负载压力小</li>
<li>对数据包修改脚下，信息保存完整</li>
<li>可跨机房（国内实现有难度）</li>
</ol>
</li>
<li>Tunnel模式的缺点<ol>
<li>需要在后端服务器安装配置ipip模块</li>
<li>需要在后端服务器tunl0配置vip</li>
<li>隧道头部的加入可能导致分片，影响服务器性能</li>
<li>隧道头部IP地址固定，后端服务器网卡hash可能不均</li>
<li>不支持端口映射</li>
</ol>
</li>
<li>Tunnel模式的使用场景<br>  理论上，如果对转发性能要求很高，且有跨机房需求，Tunnel可能是较好的选择。</li>
</ul>
<h1 id="六、涉及的概念术语"><a href="#六、涉及的概念术语" class="headerlink" title="六、涉及的概念术语"></a>六、涉及的概念术语</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CIP：Client IP，表示的是客户端 IP 地址。</span><br><span class="line">VIP：Virtual IP，表示负载均衡对外提供访问的 IP 地址，一般负载均衡 IP 都会通过 Virtual IP 实现高可用。</span><br><span class="line">RIP：RealServer IP，表示负载均衡后端的真实服务器 IP 地址。</span><br><span class="line">DIP：Director IP，表示负载均衡与后端服务器通信的 IP 地址。</span><br><span class="line">CMAC：客户端的 MAC 地址，准确的应该是 LVS 连接的路由器的 MAC 地址。</span><br><span class="line">VMAC：负载均衡 LVS 的 VIP 对应的 MAC 地址。</span><br><span class="line">DMAC：负载均衡 LVS 的 DIP 对应的 MAC 地址。</span><br><span class="line">RMAC：后端真实服务器的 RIP 地址对应的 MAC 地址。</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>LVS操作实践</title>
    <url>/2020/04/19/LVS%E6%93%8D%E4%BD%9C%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<ul>
<li><a href="https://blog.csdn.net/liwei0526vip/article/details/103104496">https://blog.csdn.net/liwei0526vip/article/details/103104496</a></li>
</ul>
<h1 id="一、实验环境说明"><a href="#一、实验环境说明" class="headerlink" title="一、实验环境说明"></a>一、实验环境说明</h1><p>我们知道 LVS 工作模式有DR、NAT、Tunnel 模式，这篇文章中会针对每个模式进行实践操作，通过实验来更深入理解工作原理。我们需要3台服务器来进行模拟。</p>
<p>&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</p>
<span id="more"></span>
<ul>
<li>操作系统<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CentOS release 6.7 (Final)</span><br></pre></td></tr></table></figure></li>
<li>内核版本<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">2.6.32-573.el6.x86_64</span><br></pre></td></tr></table></figure></li>
<li>相关服务器<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1 台 LVS 服务器</span><br><span class="line">2 台后端真实服务器（nginx）</span><br></pre></td></tr></table></figure></li>
</ul>
<h1 id="二、LVS相关组件"><a href="#二、LVS相关组件" class="headerlink" title="二、LVS相关组件"></a>二、LVS相关组件</h1><ul>
<li>IPVS。LV 是基于内核态的 netfilter 框架实现的 IPVS 功能，工作在内核态。那么用户是如何配置 VIP 等相关信息并传递到 IPVS 的呢，就需要用到 ipvsadm 工具。</li>
<li>ipvsadm 工具。ipvsadm 是 LVS 用户态的配套工具，可以实现 LVS 和 RS 的增删改查功能。它是基于 netlink 或 raw socket 方式与内核 LVS 进行通信，如果 LVS 类比 netfilter ，那么 ipvsadm 就是类似 iptables 工具的地位。</li>
<li>keepalived。ipvsadm 是一个命令行工具，如果 LVS 需要配置的业务非常复杂，ipvsadm 就很不方便了。Keepalived 最早就是为了 LVS 而生的，非官方开发的，它提供了配置文件的形式配置管理（持久化），服务的增删改查，操作非常方便。另外，Keepalived 支持配置虚拟的 VIP，能够实现 LVS 的高可用，实际生产环境中一般离不开它。</li>
</ul>
<p>虽然 Keepalived 使用起来非常方便，不过在实验环境中为了简化步骤，也能够更清楚的理解操作步骤和原理，所有的操作都是用 ipvsadm 命令来进行的。后边也会简单介绍使用 keepalived 进行 LVS 服务的操作管理。</p>
<h1 id="三、DR-模式操作实践"><a href="#三、DR-模式操作实践" class="headerlink" title="三、DR 模式操作实践"></a>三、DR 模式操作实践</h1><h2 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h2><ul>
<li>LVS：DIP&#x3D;192.168.111.10 VIP&#x3D;192.168.111.100</li>
<li>nginx1&#x3D;192.168.111.11</li>
<li>nginx2&#x3D;192.168.111.12</li>
</ul>
<h2 id="LVS服务器配置"><a href="#LVS服务器配置" class="headerlink" title="LVS服务器配置"></a>LVS服务器配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables -F</span><br><span class="line">service iptables save</span><br><span class="line">ifconfig eth0:1 192.168.111.100 netmask 255.255.255.255 broadcast 192.168.111.100 up</span><br><span class="line">yum install ipvsadm -y</span><br><span class="line">cat &gt; lvs_dr.sh &lt;&lt;EOF</span><br><span class="line">/sbin/ipvsadm -C                                                   # 清除原有规则</span><br><span class="line">/sbin/ipvsadm -A -t 192.168.111.100:80 -s rr			           # 添加vip:80的tcp服务</span><br><span class="line">/sbin/ipvsadm -a -t 192.168.111.100:80 -r 192.168.111.11:80 -g     # 添加nginx1服务器</span><br><span class="line">/sbin/ipvsadm -a -t 192.168.111.100:80 -r 192.168.111.12:80 -g     # 添加nginx2服务器</span><br><span class="line">EOF</span><br><span class="line">/bin/bash lvs_dr.sh</span><br><span class="line">ipvsadm -ln -t 192.168.111.100:80</span><br></pre></td></tr></table></figure>

<h2 id="后端服务器配置"><a href="#后端服务器配置" class="headerlink" title="后端服务器配置"></a>后端服务器配置</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">iptables -F</span><br><span class="line">service iptables save</span><br><span class="line">yum install wget -y</span><br><span class="line">wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-6.repo</span><br><span class="line">yum install nginx -y</span><br><span class="line">chkconfig nginx on</span><br><span class="line">/etc/init.d/nginx start</span><br><span class="line">ifconfig lo:0 192.168.111.100 broadcast 192.168.111.100 netmask 255.255.255.255 up</span><br><span class="line">echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_ignore</span><br><span class="line">echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/lo/arp_announce</span><br><span class="line">echo &quot;1&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_ignore</span><br><span class="line">echo &quot;2&quot; &gt;/proc/sys/net/ipv4/conf/all/arp_announce</span><br><span class="line">ifconfig lo:0</span><br></pre></td></tr></table></figure>

<h2 id="执行测试"><a href="#执行测试" class="headerlink" title="执行测试"></a>执行测试</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl http://192.168.111.100</span><br></pre></td></tr></table></figure>

<h2 id="DR-配置信息的含义"><a href="#DR-配置信息的含义" class="headerlink" title="DR 配置信息的含义"></a>DR 配置信息的含义</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Linux 内核参数arp_ignore和arp_announce作用说明</span><br><span class="line">arp_ignore - INTEGER</span><br><span class="line">0：默认值，表示可使用本地任意接口上配置的任意地址进行arp应答；</span><br><span class="line">1：仅当请求的目标IP配置在本地主机接受到报文的接口上时，才给予响应；</span><br><span class="line">2：仅当目的IP配置在收到报文的接口上，且arp请求的源IP和该接口同一网段，才响应arp请求；</span><br><span class="line">3：如ARP请求的IP作用域是主机则不响应，如果作用域是全局或者链路则响应ARP</span><br><span class="line">4-7：保留</span><br><span class="line">8：不应答所有本地IP</span><br><span class="line">arp_announce - INTEGER</span><br><span class="line">0：默认值，允许使用本机上所有接口的IP作ARP通告。</span><br><span class="line">1：尽量避免使用本地IP向非本网卡直接连接网络进行ARP通告。</span><br><span class="line">2：必须避免使用本地IP向非本网卡直接连接网络进行ARP通告。</span><br></pre></td></tr></table></figure>

<ul>
<li><p>为什么需要在 lo 接口上配置 vip ？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">我们知道如果服务器收到的数据包IP地址不是本机地址（没有开启转发模式），就会丢弃。后端服务器收到 DR 模式的数据包，此时目标 IP 是 VIP，服务器会认为此数据包不是发送给本机的，会丢弃。而我们再 lo 接口上配置 VIP 后，服务器就能够正常接收到此数据包，发送给响应的应用程序。因为，在 lo 上配置 vip 能够完成接收数据包并将结果返回给客户端。</span><br></pre></td></tr></table></figure>
</li>
<li><p>为什么需要配置 arp_ignore 参数？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">我们在 lo 上配置了 vip，正常情况下，其他设备发送 vip 的 arp 请求时，除了负载均衡设备会响应 arp 请求之外，后端服务器也会响应 vip 的 arp 请求，这样请求设备不知道哪个是准确的。我们在参数中配置 arp_ignore=1 之后，后端服务器只会响应目标IP地址为接收网卡上的本地地址的arp请求。由于vip配置在lo上，所以其他接口收到相关的arp请求都会忽略掉，这样保证了arp请求正确性。这也说明了为什么vip必须配置在lo接口上而不是其他接口上了。</span><br></pre></td></tr></table></figure>
</li>
<li><p>为什么需要配置 arp_announce 参数？</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">当后端服务器向客户端发送响应数据包时，源地址和目的地址确定，通过目的地址查找路由后，发送网卡也是确认的，也就是说源MAC地址是确认的，目的MAC地址还不确定，需要获取下一跳IP对应的MAC地址，就需要发送arp请求了。发送的arp请求目的IP就是下一跳的地址，而源IP是什么呢？系统通常默认会取数据包的源IP作为arp的源IP。我们认真想一下，源IP不就是VIP吗，假设以VIP为源IP发送arp请求，那下一跳学习到的MAC地址和IP地址对应关系就会错乱，因为负载均衡设备上的VIP对应的MAC地址肯定与这个不同，导致整个系统的arp紊乱。</span><br><span class="line">而我们配置 arp_announce=2 后，操作系统在发送arp请求选择源IP时，就会忽略数据包的源地址，而选择发送网卡上最适合的本地地址作为arp请求的源地址。</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <tags>
        <tag>lvs</tag>
      </tags>
  </entry>
  <entry>
    <title>MarkDown常用语法</title>
    <url>/2020/02/20/MarkDown%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<p>一、标题<br><code># 一级标题</code><br><code>## 二级标题</code><br><code>### 三级标题</code><br><code>### 四级标题</code><br><code>##### 五级标题</code><br><code>###### 六级标题</code><br><strong>【最多六级】</strong><br>二、字体<br><strong>加粗文字</strong><br><em>倾斜文字</em><br><em><strong>加粗倾斜文字</strong></em><br><del>删除线文字</del></p>
<p>三、引用</p>
<blockquote>
<p>引用</p>
<blockquote>
<p>引用</p>
<blockquote>
<p>引用</p>
</blockquote>
</blockquote>
</blockquote>
<p>四、分割线</p>
<hr>
<hr>
<hr>
<hr>
<p>五、图片</p>
<p>![图片alt](图片地址 ‘’图片title’’)</p>
<p><img src="https://ss0.bdstatic.com/70cFvHSh_Q1YnxGkpoWK1HF6hhy/it/u=702257389,1274025419&fm=27&gp=0.jpg" alt="blockchain" title="区块链"></p>
<p>六、超链接<br><a href="%E8%B6%85%E9%93%BE%E6%8E%A5%E5%9C%B0%E5%9D%80" title="超链接title">超链接名</a><br>title可加可不加</p>
<p>七、列表<br>无序列表</p>
<ul>
<li>a</li>
</ul>
<ul>
<li>b</li>
</ul>
<ul>
<li>c</li>
</ul>
<p>有序列表</p>
<ol>
<li>a</li>
<li>b</li>
<li>c</li>
</ol>
<p>注意序号和内容之间的空格</p>
<p>列表嵌套：上下级列表使用3个空格</p>
<ul>
<li>a<ul>
<li>b<ul>
<li>c</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>十、流程图</p>
<p>&#96;&#96; <div id="flowchart-0" class="flow-chart"></div></p>
<div id="flowchart-1" class="flow-chart"></div>

<p>十一、表格</p>
<pre><code>|  表头   | 表头  |
|  ----  | ----  |
| 单元格  | 单元格 |
| 单元格  | 单元格 |
```&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/raphael/2.2.7/raphael.min.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/flowchart/1.6.5/flowchart.min.js&quot;&gt;&lt;/script&gt;<textarea id="flowchart-0-code" style="display: none">`` st=>start: 开始
`` op=>operation: My Operation
`` cond=>condition: Yes or No?
`` e=>end
`` st->op->cond
`` cond(yes)->e
`` cond(no)->op
`` &</textarea><textarea id="flowchart-0-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-0-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-0-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-0", options);</script><textarea id="flowchart-1-code" style="display: none">st=>start: 开始
op=>operation: My Operation
cond=>condition: Yes or No?
e=>end
st->op->cond
cond(yes)->e
cond(no)->op
&</textarea><textarea id="flowchart-1-options" style="display: none">{"scale":1,"line-width":2,"line-length":50,"text-margin":10,"font-size":12}</textarea><script>  var code = document.getElementById("flowchart-1-code").value;  var options = JSON.parse(decodeURIComponent(document.getElementById("flowchart-1-options").value));  var diagram = flowchart.parse(code);  diagram.drawSVG("flowchart-1", options);</script>
</code></pre>
]]></content>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>MongoDB学习笔记</title>
    <url>/2020/02/22/MongoDB%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL中的几种日志</title>
    <url>/2020/05/08/MySQL%E4%B8%AD%E7%9A%84%E5%87%A0%E7%A7%8D%E6%97%A5%E5%BF%97/</url>
    <content><![CDATA[<ul>
<li><a href="https://www.cnblogs.com/myseries/p/10728533.html">https://www.cnblogs.com/myseries/p/10728533.html</a></li>
</ul>
<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>Mysql中有以下日志文件，分别是：</p>
<ol>
<li>重做日志（redo log）</li>
<li>回滚日志（undo log）</li>
<li>二进制日志（binlog）</li>
<li>错误日志（errorlog）</li>
<li>慢查询日志（slow query log）</li>
<li>一般查询日志（general log）</li>
<li>中继日志（relay log）</li>
</ol>
<p>其中 重做日志和回滚日志 与事务操作息息相关，二进制日志也与事务操作有一定的关系，这三种日志，对理解MySQL中的事务操作有着重要的意义。</p>
<h1 id="一、重做日志（redo-log）"><a href="#一、重做日志（redo-log）" class="headerlink" title="一、重做日志（redo log）"></a>一、重做日志（redo log）</h1><p>作用：<br>    确保事务的持久性。redo日志记录事务执行后的状态，用来恢复未写入data file的已成功事务更新的数据。防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。</p>
<p>内容：<br>    物理格式的日志，记录的是物理数据页面的修改的信息，其redo log是顺序写入redo log file的物理文件中去的。</p>
<p>什么时候产生：<br>    事务开始之后就产生redo log，redo log的落盘并不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo log文件中。</p>
<p>什么时候释放：<br>    当对应事务的脏页写入磁盘之后，redo log的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。</p>
<p>对应的物理文件：<br>    默认情况下，对应的物理文件位于数据库的data目录下的 ib_logfile1 &amp; ib_logfile2，<br>    innodb_log_group_home_dir 指定日志文件组所在的路径，默认 .&#x2F; ，表示在数据库的数据目录下。<br>    innodb_log_files_in_group 指定重做日志文件组中文件的数量，默认2</p>
<p>关于文件的大小和数量，由以下两个参数配置：<br>    innodb_log_file_size 重做日志文件的大小。<br>    innodb_mirrored_log_groups 指定了日志镜像文件组的数量，默认1</p>
<p>其他：<br>    很重要一点，redo log是什么时候写盘的？前面说了是在事务开始之后逐步写盘的。<br>    之所以说重做日志是在事务开始之后逐步写入重做日志文件，而不一定是事务提交才写入重做日志缓存，原因就是，重做日志有一个缓存区 innodb_log_buffer，innodb_log_buffer 的默认大小为8M，innodb存储引擎先将重做日志写入innodb_log_buffer中。<br>    然后会通过以下三种方式将innodb日志缓冲区的日志刷新到磁盘。<br>    Master Thread 每秒一次执行刷新innodb_log_buffer到重做日志文件。<br>    每个事务提交时会将重做日志刷新到重做日志文件。<br>    当重做日志缓存可用空间少于一半时，重做日志缓存被刷新到重做日志文件。<br>    由此可以看出，重做日志通过不止一种方式写入到磁盘，尤其是对于第一种方式，innodb_log_buffer 到重做日志文件是Master Thread线程的定时任务。<br>    因此重做日志的写盘，并不一定是随着事务的提交才写入重做日志文件的，而是随着事务的开始，逐步开始的。<br>    另外引用《MySQL技术内幕 innodb 存储引擎》（page37上的原话：<br>    即使某个事务还没有提交，innodb存储引擎仍然每秒会将重做日志缓存刷新到重做日志文件。<br>    这一点是必须要知道的，因为这可以很好的解释再大的事务的提交（commit）的时间也是很短暂的。</p>
<pre><code>---
</code></pre>
<h1 id="二、回滚日志（undo-log）"><a href="#二、回滚日志（undo-log）" class="headerlink" title="二、回滚日志（undo log）"></a>二、回滚日志（undo log）</h1><p>作用：<br>    保证数据的原子性，保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读</p>
<p>内容：<br>     逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从屋里页面上操作实现的，这一点是不同于redo log的。</p>
<p>什么时候产生：<br>    事务开始之前，将当前是的版本生成undo log，undo 也会产生redo来保证undo log的可靠性</p>
<p>什么时候释放：<br>    当事务提交之后，undo log并不能立马被删除，而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo段中标的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。</p>
<p>对应的物理文件：<br>    MySQL5.6之前，undo表空间位于共享表中间的回滚段中，共享表空间的默认名称是ibdata，位于数据文件目录中。<br>    MySQL5.6之后，undo表空间可以配置成独立的文件，但是前提需要在配置文件中配置，完成数据库初始化后生效且不可改变undo log文件的个数。<br>    如果初始化数据库之前没有进行相关的配置，那么就无法配置成独立的表空间了。</p>
<p>关于MySQL5.7之后的独立undo表空间配置参数如下：<br>    innodb_undo_directory &#x3D; &#x2F;data&#x2F;undospace - undo独立表空间的存放目录<br>    innodb_undo_logs &#x3D; 128 - 回滚段为 128KB<br>    innodb_undo_tablespaces &#x3D; 4 -指定有4个undo log文件<br>    如果undo使用的共享表空间，这个共享表空间中又不仅仅是存储了undo的信息，共享表空间的默认为MySQL的数据库目录下面，其属性由参数 innodb_data_file_path 配置。<br>其他：<br>    undo是在事务开始之前保存的被修改数据的一个版本，产生undo日志的时候，同样会伴随类似于保护事务持久化机制的redolog的产生。<br>    默认情况下undo文件是保持在共享表空间的，也即ibdatafile文件中，当数据库中发生一些大的事务性操作的时候，要产生大量的undo信息，全部保存在共享表空间中。<br>    因此共享表空间可能会变的很大，默认情况下，也就是undo日志使用共享表空间的时候，被“撑大”的共享表空间是不会也不能自动收缩的。<br>    因为，MySQL5.7之后的“独立undo表空间“的配置就显得很有必要了。</p>
<h1 id="三、二进制日志（binlog）"><a href="#三、二进制日志（binlog）" class="headerlink" title="三、二进制日志（binlog）"></a>三、二进制日志（binlog）</h1><p>作用：<br>    用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。<br>    用于数据库的基于时间点的还原。<br>内容：<br>    逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。<br>    但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息，也就意味着delete对应着delete本身和其反向的insert；update对应着update执行前后的版本的信息；insert对应着delete和insert本身的信息。<br>    在使用mysqlbinlog解析binlog之后一切都会真相大白。<br>    因此可以基于binlog做到类似于Oracle的闪回功能，其实都是依赖于binlog中的日志记录。<br>什么时候产生：<br>    事务提交的时候，一次性将事务中的sql语句（一个事务可能对应多个sql语句）按照一定的格式记录到binlog中。<br>    这里与redo log很明显的差异就是redo log并不一定是在事务提交的时候刷新到磁盘，redo log是在事务开始之后就逐步开始写入磁盘。<br>    因此对于事务的提交，即便是较大的事务，提交都是很快的，但是在开启了binlog的情况下，对于较大事务的提交，可能会变的比较慢一些。<br>    这是因为binlog是在事务提交的时候一次性写入造成的，这些可以通过测试验证。<br>什么时候释放：<br>    binlog的默认保持时间是由参数 expire_logs_day 配置，也就是说对于非活动的日志文件，在生成时间超过 expire_logs_day 的配置天数之后，会被自动删除。<br>都应的物理文件：<br>    配置文件的路径为log_bin_basename，binlog日志文件按照指定大小，当日志文件达到指定的最大的大小之后，进行滚动更新，生成新的日志文件。<br>    对于每个binlog日志文件，通过一个统一的index文件来组织。<br>其他：<br>    二进制日志的作用之一是还原数据库，这与redolog很类似，很多人混淆过，但是两者有本质的不同。<br>    作用不同：redo log是保持事务的持久性，是事务层面的，binlog作为还原的功能，是数据库层面的（当然也可以精确到事务层面），虽然都有还原的意思，但是其保护数据库的层次不一样。<br>    内容不同：redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句<br>    另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。<br>    恢复数据时候的效率，基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog。<br>    关于事务提交时，redo log和binlog的写入顺序，为了保证主从复制时候的主从一致（当然也包括使用binlog进行基于时间点还原的情况），是要严格一致的，MySQL通过两阶段提交过程来完成事务的一致性的，也机redolog和binlog的一致性，理论上是先写redolog，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。</p>
<h1 id="四、错误日志"><a href="#四、错误日志" class="headerlink" title="四、错误日志"></a>四、错误日志</h1><pre><code>错误日志记录着mysqld启动和停止，以及服务器在运行过程中发生的错误的相关信息，在默认情况下，系统记录错粗日志的功能时关闭的，错误信息被输出到标准错误输出。
指定日志路径两种写法：
    编辑 my.cnf 写入 log-error=[path]
    通过命令参数错误日志 mysqld_safe=mysql -log-error=[path]
</code></pre>
<h1 id="五、普通查询日志（general-query-log）"><a href="#五、普通查询日志（general-query-log）" class="headerlink" title="五、普通查询日志（general query log）"></a>五、普通查询日志（general query log）</h1><pre><code>记录了服务器接收到的每一个查询或命令，无论这些查询或是命令是否正确甚至是否包含语法错误，general log都会将其记录下来，记录的格式为 &#123;Time, Id, Command, Argument&#125;。也正因为mysql服务器需要不断地记录日志，开启general log会产生不小的系统开销。因此，MySQL默认是吧general log关闭的。
如果设置 set global log_output=&#39;table&#39; 的话，则日志结果会记录到general_log的表中，这表的默认引擎是CSV
如果设置 set global log_output=file，设置 general log的日志文件路径：
    set global general_log_file = &#39;、tmp/general.log&#39;
    开启general log：set global general_log=on;
    关闭general log：set global general_log=off;
</code></pre>
<h1 id="六、慢查询日志"><a href="#六、慢查询日志" class="headerlink" title="六、慢查询日志"></a>六、慢查询日志</h1><pre><code>慢日志记录执行时间过长和没有使用索引的查询语句，报错select、update、delete以及insert语句，慢日志只会记录执行成功的语句。
1. 查看慢查询时间：
   show variables like “long_query_time”;默认1s
2. 查看慢查询配置情况：
   show status like “%slow_queries%”;
3. 查看慢查询日志路径：
   show variables like “%slow%”;
</code></pre>
<h1 id="七、中继日志"><a href="#七、中继日志" class="headerlink" title="七、中继日志"></a>七、中继日志</h1><pre><code>从服务器I/O线程将主服务器的二进制日志读取过来记录到从服务器本地文件，然后从服务器SQL线程会读取replay-log日志的内容并应用到从服务器，从而是从服务器和主服务器的数据保持一致。
查看relay-log配置参数：
show variables like &#39;%relay%&#39;;
- max_relay_log_size
    relay log允许的最大值，如果该值为0，则默认值为 max_binlog_size(1G)
    如果不为0，则 max_relay_log_size 则为最大的relay_log文件大小
- relay_log
    定义relay_log的位置和名称，如果值为空，则默认位置在数据文件的目录
- relay_log_info_file
    定义relay-log.info 的位置和名称
    relay-log.info 记录 master 主库的binlog 的恢复位置和从库 relay_log 的位置
- relay_log_purge
    是否自动清空中继日志，默认值为1（启用）
- relay_log_recovery
    当slave从库当即后，加入relay-log损坏了，导致一部分中继日志没有处理，则自动放弃所有未执行的relay-log，并且重新从master上获取日志，这样就保证了relay-log的完整性。默认情况下改功能时关闭的，将relay_log_recovery的值设置为1时，可以再slave从库上开启改功能，建议开启。
- sync_relay_log
    当设置为1是，slave的I/O线程每次接收到master发送过来的binlog日志都要写入系统缓冲区，然后刷入relay_log中继日志里，这样是最安全的，因为在崩溃的时候，你最多会丢失一个事务，但会造成磁盘的大量I/O
    当设置为0时，并不是马上就刷入中继日志里，而是由操作系统决定何时来写入，虽然安全降低了，但减少了大量的磁盘I/O操作，这个值默认是0，可以动态修改
- sync_relay_log_info
    这个参数和 sync_relay_log 参数一样
</code></pre>
]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL二进制日志复制、GTID 复制与半同步复制</title>
    <url>/2020/04/28/MySQL%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%97%A5%E5%BF%97%E5%A4%8D%E5%88%B6%E3%80%81GTID-%E5%A4%8D%E5%88%B6%E4%B8%8E%E5%8D%8A%E5%90%8C%E6%AD%A5%E5%A4%8D%E5%88%B6/</url>
    <content><![CDATA[<ul>
<li><a href="https://juejin.im/post/5e12eac4e51d454139376fbd">https://juejin.im/post/5e12eac4e51d454139376fbd</a></li>
</ul>
<h1 id="一、日志格式"><a href="#一、日志格式" class="headerlink" title="一、日志格式"></a>一、日志格式</h1><h2 id="1、二进制日志格式"><a href="#1、二进制日志格式" class="headerlink" title="1、二进制日志格式"></a>1、二进制日志格式</h2><p>MySQL 二进制日志是进行主从复制的基础，它记录了所有对 MySQL 数据库的修改时间，包括增删改查和表结构的修改。当前 MySQL 一共支持三种二进制日志格式，可以通过binlog-format 参数来进行控制，其可选值如下：</p>
<ul>
<li>STATEMENT ：段格式。是 MySQL 最早支持的二进制日志格式。其记录的是实际执行修改的 SQL 语句，因此在进行批量修改时其所需要记录的数据量比较小，但对于 UUID() 或者其他依赖上下文的执行语句，可能会在主备上产生不一样的结果。</li>
<li>ROW：行格式，是 MySQL 5.7 版本之后默认的二进制日志格式。其记录的事修改前后的数据，因此在批量修改时其需要记录的数据量比较大，但其安全性比较高，不会导致主备出现不一致的情况。同时因为 ROW 格式是在从库上直接应用更改后的数据，其还能减少锁的使用。</li>
<li>MIXED：是以上两种日志的混合方式，默认采用段格式进行记录，当段格式不适用（如 UUID())，则默认采用ROW格式。</li>
</ul>
<p>  通常在主备之间网络情况良好的时候，可以优先考虑使用ROW格式，此时数据一致性最高，其次是 MIXED 格式。在设置 ROW 格式时，还有一个非常重要的参数 binlog_row_image:</p>
<h2 id="2、binlog-row-image"><a href="#2、binlog-row-image" class="headerlink" title="2、binlog_row_image"></a>2、binlog_row_image</h2><p>  binlog_row_image 有以下三个可选值：</p>
<pre><code>- full：默认值，记录行在修改前后所有列的值
- minimal：只记录修改涉及列的值
- noblob：与 full 类似，但如果 BLOB 或 TEXT 列没有修改，则不对其进行记录
</code></pre>
<p>binlog-format 与 binlog_row_image 的默认值可能在不同版本存在差异，可以使用以下命令进行查看。通常情况下，为了减少在主备复制中需要传输的数据量，可以将binlog_row_image  的值设置为 minimal 或 noblob。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">show variables like &#x27;binlog_format&#x27;;</span><br><span class="line">show variables like &#x27;binlog_row_image&#x27;;</span><br></pre></td></tr></table></figure>

<h1 id="二、基于二进制日志的复制"><a href="#二、基于二进制日志的复制" class="headerlink" title="二、基于二进制日志的复制"></a>二、基于二进制日志的复制</h1><h2 id="1、复制原理"><a href="#1、复制原理" class="headerlink" title="1、复制原理"></a>1、复制原理</h2><p>MySQL 的复制原理如下图所示：</p>
<ul>
<li>主库首先将变更写入到自己的二进制日志中；</li>
<li>备库会启动一个 IO 线程，然后主动去主库节点上获取变更日志，并写入到自己的中继日志中</li>
<li>之后从中继日志中读取变更事件，在从库上执行变更</li>
<li>当备库和主库数据状态一直，备库的 IO 线程就会进入睡眠。当主库再次发生变更时，其会向备库发出信号，唤醒 IO 线程并再次进行工作</li>
</ul>
<p>如果没有进行任何配置，主库将在变更写入到二进制日志后，就会返回对客户端的响应，因此默认情况下的复制是完全异步的，主备之间可能会短暂存在数据不一致的情况。</p>
<p><img src="/images/16f79e5f936e3bb1.jpg"></p>
<h2 id="2、配置步骤"><a href="#2、配置步骤" class="headerlink" title="2、配置步骤"></a>2、配置步骤</h2><p>首先主节点需要开启二进制日志，并且在同一个复制环境下，主备节点的 server-id 需要不一样</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">server-id = 226</span><br><span class="line"># 开启二进制日志</span><br><span class="line">log-bin=mysql-bin</span><br></pre></td></tr></table></figure>

<p>在从节点配置中继日志：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">server-id = 227</span><br><span class="line"># 配置中继日志</span><br><span class="line">relay_log  = mysql-relay-bin</span><br><span class="line"># 为了保证数据的一致性，从节点应该设置为只读</span><br><span class="line">read_only = 1</span><br><span class="line"># 以下两个配置代表是否开启二进制日志，如果该从节点还作为其他备库的主节点，则开启，否则不用配置</span><br><span class="line">log-bin = mysql-bin</span><br><span class="line"># 是否将中继节点收到的复制事件写到自己的二进制日志中</span><br><span class="line">log_slave_updates = 1</span><br></pre></td></tr></table></figure>

<p>登录主节点 MySQL 服务，创建用于进行复制的账号，并为其授予权限：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CREATE USER &#x27;repl&#x27;@&#x27;192.168.0.%&#x27; IDENTIFIED WITH mysql_native_password BY &#x27;123456&#x27;; </span><br><span class="line">GRANT REPLICATION SLAVE on *.* TO &#x27;repl&#x27;@&#x27;192.168.0.%&#x27; ;</span><br></pre></td></tr></table></figure>

<p>查看主节点二进制日志的状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW MASTER STATUS;</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br><span class="line">| mysql-bin.000001 |      887 |              |                  |                   |</span><br><span class="line">+------------------+----------+--------------+------------------+-------------------+</span><br></pre></td></tr></table></figure>
<p>基于日志和偏移量，建立复制链路：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CHANGE MASTER TO MASTER_HOST=&#x27;192.168.0.226&#x27;,\</span><br><span class="line">         MASTER_USER=&#x27;repl&#x27;,    \</span><br><span class="line">        MASTER_PASSWORD=&#x27;123456&#x27;,\</span><br><span class="line">        MASTER_LOG_FILE=&#x27;mysql-bin.000001&#x27;,\</span><br><span class="line">        MASTER_LOG_POS=887;</span><br></pre></td></tr></table></figure>

<p>开始复制：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">START SLAVE;</span><br></pre></td></tr></table></figure>

<p>查看从节点复制状态，主要参数有 Slave_IO_Running 和 Slave_SQL_Running，其状态都为 Yes 标识用于复制的 IO 线程已经开启。Seconds_behind_Master 参数表示从节点复制的延迟量。此时可以再主库上进行任意更改，并在备库上查看情况</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW SLAVE STATUS\G</span><br><span class="line">*************************** 1. row ***************************</span><br><span class="line">               Slave_IO_State: Waiting for master to send event</span><br><span class="line">                  Master_Host: 192.168.0.226</span><br><span class="line">                  Master_User: repl</span><br><span class="line">                  Master_Port: 3306</span><br><span class="line">                Connect_Retry: 60</span><br><span class="line">              Master_Log_File: mysql-bin.000001</span><br><span class="line">          Read_Master_Log_Pos: 887</span><br><span class="line">               Relay_Log_File: mysql-relay-bin.000002</span><br><span class="line">                Relay_Log_Pos: 322</span><br><span class="line">        Relay_Master_Log_File: mysql-bin.000001</span><br><span class="line">        #    Slave_IO_Running: Yes</span><br><span class="line">        #   Slave_SQL_Running: Yes</span><br><span class="line">              Replicate_Do_DB:</span><br><span class="line">          Replicate_Ignore_DB:</span><br><span class="line">           Replicate_Do_Table:</span><br><span class="line">       Replicate_Ignore_Table:</span><br><span class="line">      Replicate_Wild_Do_Table:</span><br><span class="line">  Replicate_Wild_Ignore_Table:</span><br><span class="line">                   Last_Errno: 0</span><br><span class="line">                   Last_Error:</span><br><span class="line">                 Skip_Counter: 0</span><br><span class="line">          Exec_Master_Log_Pos: 887</span><br><span class="line">              Relay_Log_Space: 530</span><br><span class="line">              Until_Condition: None</span><br><span class="line">               Until_Log_File:</span><br><span class="line">                Until_Log_Pos: 0</span><br><span class="line">           Master_SSL_Allowed: No</span><br><span class="line">           Master_SSL_CA_File:</span><br><span class="line">           Master_SSL_CA_Path:</span><br><span class="line">              Master_SSL_Cert:</span><br><span class="line">            Master_SSL_Cipher:</span><br><span class="line">               Master_SSL_Key:</span><br><span class="line">     #  Seconds_Behind_Master: 0</span><br><span class="line">Master_SSL_Verify_Server_Cert: No</span><br><span class="line">                Last_IO_Errno: 0</span><br><span class="line">                Last_IO_Error:</span><br><span class="line">               Last_SQL_Errno: 0</span><br><span class="line">               Last_SQL_Error:</span><br><span class="line">  Replicate_Ignore_Server_Ids:</span><br><span class="line">             Master_Server_Id: 226</span><br><span class="line">   #              Master_UUID: e1148574-bdd0-11e9-8873-0800273acbfd</span><br><span class="line">             Master_Info_File: mysql.slave_master_info</span><br><span class="line">                    SQL_Delay: 0</span><br><span class="line">          SQL_Remaining_Delay: NULL</span><br><span class="line">      Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates</span><br><span class="line">           Master_Retry_Count: 86400</span><br><span class="line">                  Master_Bind:</span><br><span class="line">      Last_IO_Error_Timestamp:</span><br><span class="line">     Last_SQL_Error_Timestamp:</span><br><span class="line">               Master_SSL_Crl:</span><br><span class="line">           Master_SSL_Crlpath:</span><br><span class="line">           Retrieved_Gtid_Set:</span><br><span class="line">            Executed_Gtid_Set:</span><br><span class="line">                Auto_Position: 0</span><br><span class="line">         Replicate_Rewrite_DB:</span><br><span class="line">                 Channel_Name:</span><br><span class="line">           Master_TLS_Version:</span><br><span class="line">       Master_public_key_path:</span><br><span class="line">        Get_master_public_key: 0</span><br><span class="line">            Network_Namespace:</span><br><span class="line">1 row in set (0.00 sec)</span><br></pre></td></tr></table></figure>

<h2 id="3、优缺点"><a href="#3、优缺点" class="headerlink" title="3、优缺点"></a>3、优缺点</h2><p>基于二进制日志的复制是 MySQL 最早使用的复制技术，因此 MySQL 对其支持比较完善，对执行修改的 SQL 语句几乎没有任何限制。其主要缺点是在一主多从的高可用复制架构中，如果主库发生宕机，此时想要自动通过从库的日志和偏移量来确定新的主库比较困难。</p>
<h1 id="三、基于-GTID-的复制"><a href="#三、基于-GTID-的复制" class="headerlink" title="三、基于 GTID 的复制"></a>三、基于 GTID 的复制</h1><h2 id="1、GTID-简介"><a href="#1、GTID-简介" class="headerlink" title="1、GTID 简介"></a>1、GTID 简介</h2><p>MySQL 5.6 版本之后提供了一个新的复制模式：基于 GTID的复制。GTID 全称为 Global Transaction ID，即全局事务 ID。它由每个服务节点的唯一标识和其上的事务ID共同组成，格式为：server_uuid:transaction_id。通过 GTID，可以保证在主库上的每一个事务都能在备库上得到执行，不会存在任何疏漏。</p>
<h2 id="2、配置步骤-1"><a href="#2、配置步骤-1" class="headerlink" title="2、配置步骤"></a>2、配置步骤</h2><p>主从服务器均增加以下 GTID 的配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gtid-mode = ON</span><br><span class="line"># 防止执行不受支持的语句，下文会有说明</span><br><span class="line">enforce-gtid-consistency = ON</span><br></pre></td></tr></table></figure>

<p>如果配置过上面的基于二进制日志的复制，还需要再从服务器上执行一下命令，关闭原有复制链路：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">STOP SLAVE IO_THREAD FOR CHANNEL &#x27;&#x27;;</span><br></pre></td></tr></table></figure>

<p>建立新的基于 GTID 的复制链路，指定 MASTER_AUTO_POSITION &#x3D; 1 表示由程序来自动确认开始同步的GTID的位置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">CHANGE MASTER TO MASTER_HOST=&#x27;192.168.0.226&#x27;,\</span><br><span class="line">        MASTER_USER=&#x27;repl&#x27;,</span><br><span class="line">        MASTER_PASSWORD=&#x27;123456&#x27;,</span><br><span class="line">        MASTER_AUTO_POSITION=1;</span><br></pre></td></tr></table></figure>

<p>开始复制：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">START SLAVE;</span><br></pre></td></tr></table></figure>

<p>在主节点上执行任意修改操作，并查看从节点状态，关键的输入如下：Retrieved_Gtid_Set 代表从主节点上接收到的两个事务，Executed_Gtid_Set 表示这两个事务已经在从库上得到执行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW SLAVE STATUS\G</span><br><span class="line">....</span><br><span class="line">Master_UUID            : e1148574-bdd0-11e9-8873-0800273acbfd</span><br><span class="line">Retrieved_Gtid_Set    : e1148574-bdd0-11e9-8873-0800273acbfd:1-2</span><br><span class="line">Executed_Gtid_Set    : e1148574-bdd0-11e9-8873-0800273acbfd:1-2</span><br><span class="line">.....</span><br></pre></td></tr></table></figure>

<h2 id="3、优缺点-1"><a href="#3、优缺点-1" class="headerlink" title="3、优缺点"></a>3、优缺点</h2><p>GTID 复制的有点在于程序可以自动确认开始复制的GTID点。但其仍然存在以下限制：</p>
<ul>
<li>不支持 Create Table … Select 语句。因为在 ROW 格式下，改语句将会被记录为具有不同 GTID的两个事务，此时从服务器将无法正确处理。</li>
<li>事务，过程，函数和触发器内部的 Create Temporary Table 和 Drop Temporary Table 语句均不受支持。</li>
</ul>
<p>为防止执行不受支持的语句，建议配置和上文配置一样，开启 enforce-gtid-consistency 属性，开启后在从库上执行以下不受支持的语句豆浆抛出一样并提示。</p>
<h1 id="四、半同步复制"><a href="#四、半同步复制" class="headerlink" title="四、半同步复制"></a>四、半同步复制</h1><p>在上面我们介绍过，不论是基于二进制日志的复制还是基于 GTID 的复制，其本质上都是异步复制，假设从节点上还没有获取到二进制日志信息时主节点就宕机了，那么就会存在数据不一致的情况。想要解决这个问题可以通过配置半同步复制来实现：进行半同步复制时，主节点会等待至少一个从节点获取到二进制日志后才将事务的执行结果返回给客户端。具体配置步骤如下：</p>
<h2 id="1、安装插件"><a href="#1、安装插件" class="headerlink" title="1、安装插件"></a>1、安装插件</h2><p>MySQL 从 5.5 之后开始以插件的形式支持半同步复制，所以需要先进行插件安装，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">-- 主节点上执行</span><br><span class="line">mysql&gt; INSTALL PLUGIN rpl_semi_sync_master SONAME &#x27;semisync_master.so&#x27;;</span><br><span class="line">-- 从节点上执行</span><br><span class="line">mysql&gt; INSTALL PLUGIN rpl_semi_sync_slave SONAME &#x27;semisync_slave.so&#x27;;</span><br></pre></td></tr></table></figure>

<p>如果你的复制是基于高可用架构的，即从节点可能会在主节点宕机后成为新的主节点，而源主节点可能在失败恢复后成为从节点，那么为了保证半同步复制仍然有效，此时可以再主从节点上都安装主从插件。安装后使用以下命令查看是否安装成功：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; SELECT PLUGIN_NAME, PLUGIN_STATUS FROM INFORMATION_SCHEMA.PLUGINS WHERE PLUGIN_NAME LIKE &#x27;%semi%&#x27;;</span><br><span class="line">+----------------------+---------------+</span><br><span class="line">| PLUGIN_NAME          | PLUGIN_STATUS |</span><br><span class="line">+----------------------+---------------+</span><br><span class="line">| rpl_semi_sync_master | ACTIVE        |</span><br><span class="line">| rpl_semi_sync_slave  | ACTIVE        |</span><br><span class="line">+----------------------+---------------+</span><br></pre></td></tr></table></figure>

<h2 id="2、配置半同步复制"><a href="#2、配置半同步复制" class="headerlink" title="2、配置半同步复制"></a>2、配置半同步复制</h2><p>半同步复制可以基于日志复制或 GTID 复制开启，只需要在其原有配置上增加以下配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 主节点上增加如下配置：</span><br><span class="line">plugin-load=rpl_semi_sync_master=semisync_master.so</span><br><span class="line">rpl_semi_sync_master_enabled=1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 从节点上增加如下配置：</span><br><span class="line">plugin-load=rpl_semi_sync_slave=semisync_slave.so</span><br><span class="line">rpl_semi_sync_slave_enabled=1</span><br><span class="line"></span><br><span class="line"># 和上面提到的一样，如果是高可用架构下，则主从节点都可以增加主从配置：</span><br><span class="line">plugin-load = &quot;rpl_semi_sync_master=semisync_master.so;rpl_semi_sync_slave=semisync_slave.so&quot;</span><br><span class="line">rpl-semi-sync-master-enabled = 1</span><br><span class="line">rpl-semi-sync-slave-enabled = 1</span><br></pre></td></tr></table></figure>

<h2 id="3、启动复制"><a href="#3、启动复制" class="headerlink" title="3、启动复制"></a>3、启动复制</h2><p>安装二进制日志或 GTID 的方式正常启动复制极客，此时可以使用如下命令查看半同步日志是否正在执行：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 主节点</span><br><span class="line">mysql&gt; SHOW STATUS LIKE &#x27;Rpl_semi_sync_master_status&#x27;;</span><br><span class="line">+-----------------------------+-------+</span><br><span class="line">| Variable_name               | Value |</span><br><span class="line">+-----------------------------+-------+</span><br><span class="line">| Rpl_semi_sync_master_status | ON    |</span><br><span class="line">+-----------------------------+-------+</span><br><span class="line"></span><br><span class="line"># 从节点</span><br><span class="line">mysql&gt; SHOW STATUS LIKE &#x27;Rpl_semi_sync_slave_status&#x27;;</span><br><span class="line">+----------------------------+-------+</span><br><span class="line">| Variable_name              | Value |</span><br><span class="line">+----------------------------+-------+</span><br><span class="line">| Rpl_semi_sync_slave_status | ON    |</span><br><span class="line">+----------------------------+-------+</span><br></pre></td></tr></table></figure>

<p>值为 ON 代表半同步复制配置成功。</p>
<h2 id="4、可选配置"><a href="#4、可选配置" class="headerlink" title="4、可选配置"></a>4、可选配置</h2><p>半同步日志还有以下两个可选配置，一个是 rpl_semi_sync_master_wait_for_slave_count，它表示主节点需要至少等待几个从节点复制完成，默认值为1，因为等待过多从节点可能会导致长时间的延迟，所以通常使用默认值即可。另一个常用参数是 rpl_semi_sync_master_wait_point ,他主要用于控制等待的时间点，它有以下两个可选值：</p>
<ul>
<li>AFTER_SYNC（默认值）：主服务器将每个事务写入其二进制日志，并将二进制日志同步到磁盘后开始进行等待。在收到从节点的确认后，才将事务提交给存储引擎并将结果返回给客户端。</li>
<li>AFTER_COMMIT：主服务器将每个事务写入其二进制日志并同步到磁盘，然后将事务提交到存储引擎，提交后再进行等待。在收到从节点的确认后，才将结果返回给客户端。</li>
</ul>
<p>第二种方式是 MySQL 5.7.2 之前的默认方式，但这种方式会导致数据的丢失，所以在 5.7.2 版本之后引入了第一种方式作为默认方式，它可以实现无损复制（lossless replication），数据基本无丢失，因此 rpl_semi_sync_master_wait_point 参数通常也不用进行修改，采用默认值即可。想要查看当前版本该参数的值，可以使用如下命令：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mysql&gt; SHOW VARIABLES LIKE &#x27;rpl_semi_sync_master_wait_point&#x27;;</span><br><span class="line">+---------------------------------+------------+</span><br><span class="line">| Variable_name                   | Value      |</span><br><span class="line">+---------------------------------+------------+</span><br><span class="line">| rpl_semi_sync_master_wait_point | AFTER_SYNC |</span><br><span class="line">+---------------------------------+------------+</span><br></pre></td></tr></table></figure>
<p>虽然半同步复制能够最大程度的避免数据的丢失，但是因为网络通讯会导致额外的等待时间的开销，所以尽量在低延迟的网络环境下使用，如处于同一机房的主机之间。</p>
<h1 id="五、高可用架构"><a href="#五、高可用架构" class="headerlink" title="五、高可用架构"></a>五、高可用架构</h1><p>无论是主主复制架构，还是一主多从结构，单存依靠复制只能解决数据可靠性的问题，并不能解决系统高可用的问题，想要保证高可用，系统必须能够自动进行故障转移，即在主库宕机时，主动讲其它备库升级为主库。常用的有以下两种解决方案：</p>
<h2 id="1、MMM"><a href="#1、MMM" class="headerlink" title="1、MMM"></a>1、MMM</h2><p>MMM（Master-Master replication manager for MySQL）是由 Perl 语言开发的一套支持双主故障切换以及双主日常管理的第三方软件。它包含两类角色：writer和reader，分别对应读写节点和只读节点。使用 MMM 管理的双主节点在同一时间上只允许一个进行写入操作，当 writer 节点出现宕机（假设是 Master1），程序会自动移除该节点上的读写 VIP，然后切换到 Master2，并设置 Master2 的read_only &#x3D; 0，即关闭只读限制，同时将所有Slave节点重新指向 Master2.</p>
<p>除了管理双主节点，MMM也负责管理所有Slave节点，在出现宕机、复制延迟或复制错误，MMM会移除该节点的VIP，直至节点恢复正常、MMM高可用的架构示例图如下：<br><img src="/images/16f79e66883b446a.jpg"></p>
<p>MMM结构的缺点在于虽然其能实现自动切换，但不会主动补齐丢失的数据，所以会存在数据不一致的风险。另外 MMM 的发布时间比较早，所以其也不支持 MySQL 最新的基于 GTID 的复制，如果你是用的是基于 GTID 的复制，则只能采用 MHA。</p>
<h2 id="2、MHA"><a href="#2、MHA" class="headerlink" title="2、MHA"></a>2、MHA</h2><p>MHA（Master High Availability）是由Perl实现的一款高可用程序，相对于 MMM，它能尽量避免数据不一致的问题。它监控的是一主多从的复制架构，架构如下图所示：<br><img src="/images/16f79e6bd3da08a0.jpg"></p>
<p>在 Master 节点宕机后，其处理流程如下：</p>
<ol>
<li>尝试从宕机Master中保存二进制日志</li>
<li>找到含有最新中继日志的Slave</li>
<li>把最新中继日志应用到其他实例，保证各实例数据一直</li>
<li>应用从Master保存的二进制日志事件</li>
<li>提升一个Slave为Master</li>
<li>其他Slave向该新Master同步</li>
</ol>
<p>按照以上的处理流程，MHA能够最大程度的避免数据不一致的问题。但如果 Master 所在的服务器也宕机了，那么过程的第一步就会失败。在 MySQL 5.5 后，可以开启半同步复制来避免这个问题，从而可以保证数据的一致性和几乎无丢失。当然 MHA 集群也存在以下一些缺点：</p>
<ul>
<li>集群中所有节点之间需要开启SSH服务，所以会存在一定的安全影响</li>
<li>没有实现 Slave 的高可用</li>
<li>自带的脚本不足，例如虚拟IP的配置需要自己通过命令或者其他第三方软件来实现</li>
<li>需要手动清理中继日志</li>
</ul>
<p>以上就是MMM和MHA架构的简单介绍。</p>
]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx</title>
    <url>/2020/04/15/Nginx/</url>
    <content><![CDATA[<ul>
<li><a href="https://www.nginx.cn/doc/index.html">https://www.nginx.cn/doc/index.html</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Nginx优化长连接</title>
    <url>/2020/03/13/Nginx%E4%BC%98%E5%8C%96%E9%95%BF%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<p>引用：</p>
<ul>
<li><a href="https://www.cnblogs.com/sunsky303/p/10648861.html">https://www.cnblogs.com/sunsky303/p/10648861.html</a></li>
</ul>
<h1 id="一、nginx之tcp-nooush、tcp-nodelay、snefile"><a href="#一、nginx之tcp-nooush、tcp-nodelay、snefile" class="headerlink" title="一、nginx之tcp_nooush、tcp_nodelay、snefile"></a>一、nginx之tcp_nooush、tcp_nodelay、snefile</h1><h2 id="1-TCP-NODELAY"><a href="#1-TCP-NODELAY" class="headerlink" title="1. TCP_NODELAY"></a>1. TCP_NODELAY</h2><p>怎么强制socket在它的缓冲区里发送数据？<br>一个解决方案是TCP堆栈的TCP_NODELAY选项。这样就可以使缓冲区中的数据立即发送出去。<br>Nginx的TCP_NODELAY选项使得在打开一个新的socket时增加了TCP_NODELAY选项。但这时会造成一种情况：<br>终端应用程序每产生一次操作就会发送一个包，而典型情况下下一个包会拥有一个字节的数据以及40个字节长的包头，于是产生4000%的过载，很轻易的能令网络发生拥塞。<strong>为了避免这种情况，TCP堆栈实现了等待数据0.2秒钟，因此操作后它不会发送一个数据包，而是将这段时间内的数据打成一个大的包。这一机制是由Nagle算法保证。</strong></p>
<p>Nagle化后来成了一种标准并且立即在因特网上得以实现。它现在已经成为默认配置了，但是有些场合下把这一选项关掉也是合乎需要的。现在假设某个应用程序发出了一个请求，希望发送小块数据。我们可以选择立即发送数据或者等待产生更多的数据然后再一次发送两种策略。<br>如果我们马上发送数据，那么交互性的以及客户&#x2F;服务器型的应用程序将极大地受益。如果请求立即发出那么响应时间也会快一些。**以上操作可以通过设置套接字的TCP_NODELAY &#x3D; on 选项来完成，这样就禁用了Nagle算法。（不需要等待0.2s）</p>
<h2 id="2-TCP-NOPUSH"><a href="#2-TCP-NOPUSH" class="headerlink" title="2. TCP_NOPUSH"></a>2. TCP_NOPUSH</h2><p>在nginx中，tcp_nopush配置和tcp_nodelay“互斥”。<strong>它可以配置一次发送数据的包大小。也就是说，它不是按时间累计0.2秒后发送包，而是当宝累计到一定大小后就发送。</strong><br><strong>注：在nginx中，tcp_nopush必须和sendfile搭配使用。</strong></p>
<h2 id="3-sendfile"><a href="#3-sendfile" class="headerlink" title="3. sendfile"></a>3. sendfile</h2><p>现在流行的web服务器里面都提供sendfile选项用来提高服务器性能，那到底sendfile是什么，怎么影响性能的呢？<br>sendfile实际上是linux2.0+以后推出的一个系统调用，web服务器可以通过调整自身的配置来决定是否利用sendfile这个系统调用。先来看一下不用sendfile的传统网络传输过程：<br>read(file,tmp_buf,len);<br>write(socket,rmp_buf,len);</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">硬盘 &gt;&gt; kernel buffer &gt;&gt; user buffer &gt;&gt; kernel socket buffer &gt;&gt; 协议栈</span><br></pre></td></tr></table></figure>
<p>1）一般来说一个网络应用是通过读硬盘数据，然后写数据到socket来完成网络传输的。上面的2行代码解释了这一点，不过上面2行简单的代码掩盖了低层的很多操作。来看看低层是怎么执行的：</p>
<ol>
<li>系统调用read()产生一个上下文切换，从user mode切换到kernel mode，然后DMA执行拷贝，把文件数据从硬盘读到一个kernel buffer里。</li>
<li>数据从kernel buffer拷贝到user mode，然后系统调用read()返回，这时又产生一个上下文切换：从kernel mode切换到user mode。</li>
<li>系统调用write()产生一个上下文切换：从user mode切换到kernel mode，然后把步骤2读到user buffer的数据拷贝到kernel buffer（数据第2次拷贝到kernel buffer），不过这次是个不同的kernel buffer，这个buffer和socket相关联。</li>
<li>系统调用write()返回，产生一个上下文切换：从kernel mode切换到user mode（第4次切换了），然后DMA从kernel buffer拷贝数据到协议产（第4次拷贝了）。<br>上面4个步骤有4次切换，4次拷贝，我们发现如果能减少切换次数和拷贝次数将会有效提升性能。在kernel2.0+版本中，系统调用sendfile()就是用来简化上面步骤提升性能的。<br><strong>sendfile()不但能减少切换次数，还能减少拷贝次数。</strong><br>2）再来看下sendfile()来进行网络传输的过程：<br>sendfile(socket,file,len);<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">硬盘 &gt;&gt; kernel buffer（快速拷贝到kernel socket buffer） &gt;&gt; 协议栈</span><br></pre></td></tr></table></figure></li>
<li>系统调用sendfile()通过DMA把硬盘数据拷贝到kernel buffer，然后数据被kernel直接拷贝到另外一个socket相关的kernel buffer。这里没有user mode和kernel mode之间的切换，在kernel中直接完成了一个buffer到另一个buffer的拷贝。</li>
<li>DMA把数据从kernel buffer直接拷贝给协议栈，没有切换，也不需要数据从user mode拷贝到kernel mode，因为数据就在kernel中。<br>  步骤减少了，切换减少了，拷贝减少了，自然性能就提升了。这就是为什么说在Nginx配置文件里打开sendfile on选项能提高web server性能的原因。</li>
</ol>
<p>  <strong>综上，这三个参数都应该配置成on：sendfile on;tcp_nopush on;tcp_nodelay on;</strong></p>
<h1 id="二、-nginx长连接-keepalive"><a href="#二、-nginx长连接-keepalive" class="headerlink" title="二、 nginx长连接-keepalive"></a>二、 nginx长连接-keepalive</h1><p>当时用nginx作为反向代理时，为了支持长连接，需要做到两点：</p>
<ul>
<li>从client到nginx的连接是长连接</li>
<li>从nginx到server的连接是长连接</li>
</ul>
<h2 id="1、保持和client的长连接："><a href="#1、保持和client的长连接：" class="headerlink" title="1、保持和client的长连接："></a>1、保持和client的长连接：</h2><p>默认情况下，nginx已经自动开启了对client连接的keep alive支持（同时client发送的HTTP请求要求keep alive）。一般场景可以直接使用，但是对于一些比较特殊的场景，还是有必要调整个别参数（keepalive_timeout和keepalive_requests）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http&#123;</span><br><span class="line">    keepalive_timeout 120s 120s;</span><br><span class="line">    keepalive_requests 10000;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1）keepalive_timeout</p>
<ol>
<li>第一个参数：设置keep-alive客户端连接在服务器端保持开启的超时值（默认75s）；值为0会禁用keep-alive客户端连接；</li>
<li>第二个参数：可选，在响应的header域中设置一个值“Keep-Alive: timeout&#x3D;time”；通常可以不用设置；<br>注：keepalive_timeout默认75s，一般情况下够用，对于一些请求比较大的内部服务器通讯的场景，适当加大为120s或者300s；</li>
</ol>
<p>2）keepalive_requests<br>keepalive_requests指令用于<strong>设置一个keep-alive连接上可以服务的请求的最大数量，当最大请求数量达到时，连接被关闭。默认是100</strong>。这个参数的真实含义，是指一个keep alive建立之后，nginx就会为这个连接设置一个计数器，记录这个keep alive的长连接上已经接收并处理的客户端请求的数量。如果达到这个参数设置的最大值时，则nginx会强行关闭这个长连接，逼迫客户端不得不重新建立新的长连接。<br>大多数情况下当QPS（每秒请求数）不是很高时，默认值100凑活够用。但是，对于一些QPS比较高（比如超过10000QPS，甚至达到3k，5k设置更高）的场景，默认的100就显得太低。<br>简单计算一下，QPS&#x3D;10000时，客户端每秒发送10000个请求（通常建立有多个长连接），每个连接只能最多跑1000次请求，意味着平均每秒钟重新新建100个连接。因此，就会发现有大量的TIME_WAIT的socket连接（即使此时keep alive已经在client和nginx之间生效）。<strong>因为对于QPS较高的场景，非常有必要加大这个参数，以避免大量连接被生成再抛弃的情况，减少TIME_WAIT。</strong></p>
<h2 id="2、保持和server的长连接"><a href="#2、保持和server的长连接" class="headerlink" title="2、保持和server的长连接"></a>2、保持和server的长连接</h2><p>为了让nginx和后端server（nginx成为upstream）之间保持长连接，典型设置如下：（<strong>默认nginx访问后端都是用的短连接（HTTP1.0），一个请求来了，nginx新开一个端口和后端建立连接，后端执行完毕后主动关闭该链接</strong>）</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    upstream  BACKEND &#123;</span><br><span class="line">        server   192.168.0.1：8080  weight=1 max_fails=2 fail_timeout=30s;</span><br><span class="line">        server   192.168.0.2：8080  weight=1 max_fails=2 fail_timeout=30s;</span><br><span class="line">        keepalive 300;        // 这个很重要！</span><br><span class="line">    &#125;</span><br><span class="line">server &#123;</span><br><span class="line">        listen 8080 default_server;</span><br><span class="line">        server_name &quot;&quot;;</span><br><span class="line">        location /  &#123;</span><br><span class="line">            proxy_pass http://BACKEND;</span><br><span class="line">            proxy_set_header Host  $Host;</span><br><span class="line">            proxy_set_header x-forwarded-for $remote_addr;</span><br><span class="line">            proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">            add_header Cache-Control no-store;</span><br><span class="line">            add_header Pragma  no-cache;</span><br><span class="line">            proxy_http_version 1.1;         // 这两个最好也设置</span><br><span class="line">            proxy_set_header Connection &quot;&quot;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1）location中有两个参数需要设置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    server &#123;</span><br><span class="line">        location /  &#123;</span><br><span class="line">            proxy_http_version 1.1; // 这两个最好也设置</span><br><span class="line">            proxy_set_header Connection &quot;&quot;;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>HTTP协议中对长连接的支持是从1.1版本之后才有的，因为最好通过proxy_http_version指令设置为“1.1；<br>而“Connection” header应该被清理。清理的意思，我的理解，是清理从client过来的http header，因为即使client和nginx之间是短连接，nginx和upstream之间也是可以开启长连接的。这种情况下必须清理来自client请求中的“Connection” header。</p>
<p>2）upstream中的keepalive设置<br>次数keepalive的含义不是开启、关闭长连接的开关，也不是用来设置超时的timeout；更不是设置长连接池的最大连接数。官方解释：</p>
<ol>
<li>The connections parameter sets the maximum number of idle keepalive connections to upstream servers connections（<strong>设置到upstream服务器的空闲keepalive连接的最大数量</strong>）</li>
<li>When this number is exceeded, the least recently used connections are closed. （<strong>当这个数量被突破时，最近使用最少的连接将被关闭</strong>）</li>
<li>It should be particularly noted that the keepalive directive does not limit the total number of connections to upstream servers that an nginx worker process can open.（<strong>特别提醒：keepalive指令不会限制一个nginx worker进程到upstream服务器连接的总数量</strong>）<br>我们先假设一个场景：有一个HTTP服务，作为upstream服务器接收请求，响应时间为100毫秒。如果要达到10000 QPS的性能，就需要在nginx和upstream服务器之间建立大约1000条HTTP连接。nginx为此建立连接池，然后请求过来时为每个请求分配一个连接，请求结束时回收连接到连接池中，连接的状态也就更改为idle。我们再假设这个upstream服务器的keepalive参数值比较小，比如常见的10：<br>A、假设请求和响应是均匀而平稳的，那么这1000条连接应该都是一放回连接池就立即被后续请求申请使用，线程池中的idle线程会非常少，趋近于零，不会造成连接数量反复震荡。<br>B、现实中请求和响应不可能平稳，我们以10毫秒为一个单位，来看连接的情况（逐一场景是1000个线程+100毫秒响应时间，每秒有10000个请求完成），我们假设应答始终都是平稳的，只是请求不平稳，第一个10毫秒只有50，第二个10毫秒有150：</li>
<li>下一个10毫秒，有100个连接结束请求回收连接到连接池，但是假设此时请求不均匀10毫秒内没有预计的100个请求进来，而是只有50个请求。注意此时连接池回收了100个连接又分配出去50个连接，因此连接池内有50个空闲连接。</li>
<li>然后注意看keepalive&#x3D;10的设置，这意味着连接池中最多容许保留有10个控线连接。因为nginx不得不将这50个空闲连接中的40个关闭，只保留10个。</li>
<li>再下一个10毫秒，有150个请求进来，有100个请求结束任务释放连接。150-100&#x3D;50，孔雀50个连接，减掉前面连接池保留的10个空闲连接，nginx不得不新建40个新连接来满足要求。<br>C、同样，如果假设响应不均衡也会出现上面的连接数波动情况。</li>
</ol>
<p>造成连接数量反复震荡的一个推手，就是keepalive这个最大空闲连接数。毕竟连接池中的1000个连接在频繁利用时，出现短时间内多余10个空闲连接的概率是在太高。<strong>因此为了避免出现上面的连接震荡，必须考虑加大这个参数</strong>，比如上面的场景如果将keepalive设置为100或者200，就可以非常有效的缓冲请求和应答不均。</p>
<p>总结：<br>keepalive这个参数一定要小心设置，尤其对于QPS比较高的场景，推荐先做一下估算（<strong>容量规划</strong>），根据QPS和平均响应时间答题能计算出需要的长连接的数量。比如前面1000QPS和100毫秒响应时间就可以推算出需要的长连接数量大概是1000.然后将keepalive设置为这个长连接数量的10%到30%。比较懒的同学，可以直接设置为keepalive&#x3D;1000之类的，一般都是OK的。</p>
<h2 id="3、综上，出现大量TIME-WAIT的情况："><a href="#3、综上，出现大量TIME-WAIT的情况：" class="headerlink" title="3、综上，出现大量TIME_WAIT的情况："></a>3、综上，出现大量TIME_WAIT的情况：</h2><p>1）导致nginx端出现大量TIME_WAIT的情况有两种：</p>
<ul>
<li>keepalive_requests设置比较小，高并发下超过此值后nginx会强制关闭和客户端保持的keepalive长连接；（主动关闭连接后导致nginx出现TIME_WAIT）</li>
<li>keepalive设置的比较小（空闲数太小），导致高并发下nginx会频繁的出现连接数震荡（超过该值会关闭连接），不停的关闭、开启和后端server保持的keepalive长连接<br>2）导致后端server端出现大量TIME_WAIT的情况：<br>nginx没有打开和后端的长连接，即：没有设置proxy_http_version 1.1和proxy_set_header Connection “”；从而导致后端server每次关闭连接，高并发下就会出现server端出现大量TIME_WAIT。</li>
</ul>
]]></content>
      <tags>
        <tag>nginx</tag>
        <tag>keepalive</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx健康检查</title>
    <url>/2020/03/11/Nginx%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</url>
    <content><![CDATA[<p>引用：</p>
<ul>
<li><a href="http://ningg.top/nginx-series-health-check/">http://ningg.top/nginx-series-health-check/</a></li>
</ul>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p><strong>服务治理</strong>的一个重要任务是感知服务街店的变更，完成<strong>服务自动注册</strong>和<strong>异常节点的自动摘除</strong>。这就需要服务治理平台能够：<strong>及时</strong>，<strong>准确</strong>的感知service节点的健康状况。</p>
<h1 id="方案概述"><a href="#方案概述" class="headerlink" title="方案概述"></a>方案概述</h1><p>Nginx提供了三种服务健康检查方案：</p>
<ol>
<li>TCP层检查方案：定时与后端服务简历一条<strong>tcp连接</strong>，连接建立成功则认为服务节点是健康的；</li>
<li>HTTP层检查方案：TCP层检查有一定的局限性：<ol>
<li>很多HTTP服务是有状态的，端口处于listen状态并不能代表服务已经完全预热（可以对外提供服务）；</li>
<li>不能真实反映服务内部处理逻辑是否拥堵；</li>
<li>这时可以选择http层健康检查，会向服务发送一个http请求<strong>GET &#x2F; HTTP&#x2F;1.0\r\n\r\n</strong>，返回状态是2xx或3xx时认为后端服务正常；</li>
</ol>
</li>
<li>自定义方案：可根据下文描述自定义检查方案。</li>
</ol>
<h1 id="配置参数详解"><a href="#配置参数详解" class="headerlink" title="配置参数详解"></a>配置参数详解</h1><p>一个常用的健康检查配置如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">check fall=3 interval=3000 rise=2 timeout=2000 type=http;</span><br><span class="line">check_http_expect_alive http_2xx http_3xx ;</span><br><span class="line">check_http_send &quot;GET /checkAlive HTTP/1.0\r\n\r\n&quot; ;</span><br></pre></td></tr></table></figure>
<p>下面针对每个配置，进行详细介绍：</p>
<h2 id="check"><a href="#check" class="headerlink" title="check"></a>check</h2><p>check字段参数如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Syntax: check interval=milliseconds [fall=count] [rise=count] [timeout=milliseconds] [default_down=true|false] [type=tcp|http|ssl_hello|mysql|ajp] [port=check_port]</span><br><span class="line">Default: 如果没有配置参数，默认值是：interval=30000 fall=5 rise=2 timeout=1000 default_down=true type=tcp</span><br></pre></td></tr></table></figure>
<p>check字段各个参数含义如下：</p>
<ul>
<li>interval：向后端发送的健康检查包的间隔</li>
<li>fall(fall_count)：如果连续失败次数达到fall_count，服务器就被认为是down</li>
<li>rise(rise_count)：如果连续成功次数达到rise_count，服务器就被人为是up</li>
<li>timeout：后端健康请求的超时时间</li>
<li>default_down：设定初始时服务器的状态，如果是true，就说ing默认是down的，如果是false，就是up的。默认是true，也就是一开始服务器认为是不可用，要等健康检查包达到一定成功次数才会被认为是健康的</li>
<li>type：健康检查包的类型，<ul>
<li>tcp：简单的tcp连接，如果连接成功，就说明后端正常</li>
<li>ssl_hello：发送一个初始的SSL hello包并接受服务器的SSL hello包</li>
<li>http：发送HTTP请求，通过后端的回复包的状态来判断后端是否存活</li>
<li>mysql：向mysql服务器连接，通过接收服务器的greeting包来判断后端是否存活</li>
<li>ajp：向后端发送AJP协议的Cping包，通过接收Cpong包来判断后端是否存活</li>
<li>port：指定后端服务器的检查端口。可以指定不同于真实服务的后端服务器的端口，比如后端提供的是443端口的应用，你可以去检查80端口的状态来判断后端健康状况。默认是0，标识跟后端server提供真实服务的端口一样。</li>
</ul>
</li>
</ul>
<h2 id="check-http-expext-alive"><a href="#check-http-expext-alive" class="headerlink" title="check_http_expext_alive"></a>check_http_expext_alive</h2><p>check_http_expect_alive 指定主动健康检查时HTTP回复的成功状态：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Syntax: check_http_expect_alive [ http_2xx | http_3xx | http_4xx | http_5xx ]</span><br><span class="line">Default: http_2xx | http_3xx</span><br></pre></td></tr></table></figure>

<h2 id="check-http-send"><a href="#check-http-send" class="headerlink" title="check_http_send"></a>check_http_send</h2><p>check_http_send 配置HTTP健康检查包发送的请求内容<br>为了减少传输数据量，推荐采用“HEAD”方法。当采用长连接进行健康检查时，需要在该指令中添加keep-alive请求头，如：“HEAD &#x2F; HTTP&#x2F;1.1\r\nConnection:keep-alive\r\n\r\n”。同时，在采用”GET“方法的情况下，请求uri的size不宜过大，确保可以在1个interval内传输完成，否则会被健康检查模块视为后端服务器或网络异常。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Syntax: check_http_send http_packet</span><br><span class="line">Default: &quot;GET / HTTP/1.0\r\n\r\n&quot;</span><br></pre></td></tr></table></figure>

<h2 id="完整示例"><a href="#完整示例" class="headerlink" title="完整示例"></a>完整示例</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http &#123;</span><br><span class="line">    upstream cluster1 &#123;</span><br><span class="line">        # simple round-robin</span><br><span class="line">        server 192.168.0.1:80;</span><br><span class="line">        server 192.168.0.2:80;</span><br><span class="line">        check interval=3000 rise=2 fall=5 timeout=1000 type=http;</span><br><span class="line">        check_http_send &quot;HEAD / HTTP/1.0\r\n\r\n&quot;;</span><br><span class="line">        check_http_expect_alive http_2xx http_3xx;</span><br><span class="line">    &#125;</span><br><span class="line">    upstream cluster2 &#123;</span><br><span class="line">        # simple round-robin</span><br><span class="line">        server 192.168.0.3:80;</span><br><span class="line">        server 192.168.0.4:80;</span><br><span class="line">        check interval=3000 rise=2 fall=5 timeout=1000 type=http;</span><br><span class="line">        check_keepalive_requests 100;</span><br><span class="line">        check_http_send &quot;HEAD / HTTP/1.1\r\nConnection: keep-alive\r\n\r\n&quot;;</span><br><span class="line">        check_http_expect_alive http_2xx http_3xx;</span><br><span class="line">    &#125;</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 80;</span><br><span class="line">        location /1 &#123;</span><br><span class="line">            proxy_pass http://cluster1;</span><br><span class="line">        &#125;</span><br><span class="line">        location /2 &#123;</span><br><span class="line">            proxy_pass http://cluster2;</span><br><span class="line">        &#125;</span><br><span class="line">        location /status &#123;</span><br><span class="line">            check_status;</span><br><span class="line">            access_log   off;</span><br><span class="line">            allow SOME.IP.ADD.RESS;</span><br><span class="line">            deny all;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx原理</title>
    <url>/2020/03/10/Nginx%E5%8E%9F%E7%90%86/</url>
    <content><![CDATA[<p>引用：</p>
<ul>
<li><a href="http://ningg.top/nginx-series-principle/">http://ningg.top/nginx-series-principle/</a></li>
</ul>
<h1 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h1><h2 id="Nginx的进程模型"><a href="#Nginx的进程模型" class="headerlink" title="Nginx的进程模型"></a>Nginx的进程模型</h2><p><img src="/images/nginx-multi-progress-model.png"><br>Nginx服务器，正常运行过程中：</p>
<ol>
<li><strong>多进程</strong>：1个Master进程、多个Worker进程；</li>
<li><strong>Master进程</strong>：管理多个Worker进程：<ol>
<li>对外接口：接收<strong>外部操作</strong>信号</li>
<li>对内转发：根据<strong>外部操作</strong>的不同，通过<strong>信号</strong>管理Worker</li>
<li>监控：监控Worker进程的运行状态，Worker进程异常终止后，自动重启Worker进程</li>
</ol>
</li>
<li><strong>Worker</strong>进程：所有Worker进程都是平等的：<ol>
<li>实际处理：网络请求，又Worker进程处理；</li>
<li>Worker进程数量：在nginx.conf中配置，一般设置为<strong>CPU核心数</strong>，充分利用CPU资源，同时避免进程数量过多，避免进程之间竞争CPU资源，增加上下文切换的损耗。</li>
</ol>
</li>
</ol>
<p>思考：</p>
<ol>
<li>请求是连接到Nginx，Master进程负责处理和转发？</li>
<li>如何选定哪个Worker进程处理请求？请求的处理结果，是否还要经过Master进程？</li>
</ol>
<p><img src="/images/nginx-master-worker-details.png"></p>
<p>HTTP连接建立和请求处理过程：</p>
<ol>
<li>Nginx启动时，Master进程，加载配置文件；</li>
<li>Master进程，初始化监听的socket；</li>
<li>Master进程，fork出多个Worker进程；</li>
<li>Worker进程，竞争新的连接，获胜方通过三次握手，建立Socket连接，并处理请求。</li>
</ol>
<p>Nginx高性能、高并发：</p>
<ol>
<li>Nginx采用：<strong>多进程</strong>+<strong>异步非阻塞</strong>方式（<strong>IO多路复用</strong>epool）</li>
<li>请求的完整过程：<ol>
<li>建立连接</li>
<li>读取请求：解析请求</li>
<li>处理请求</li>
<li>相应请求</li>
</ol>
</li>
<li>请求的完整过程，对应到低层，就是：读写socket事件</li>
</ol>
<h2 id="Nginx的事件处理模型"><a href="#Nginx的事件处理模型" class="headerlink" title="Nginx的事件处理模型"></a>Nginx的事件处理模型</h2><p>request：Nginx中的http请求。<br>基本的HTTP Web Server工作模式：</p>
<ol>
<li><strong>接受请求</strong>：逐行读取<strong>请求行</strong>和<strong>请求头</strong>，判断有请求体后，读取<strong>请求体</strong></li>
<li><strong>处理请求</strong></li>
<li><strong>返回响应</strong>：根据处理结果，生成相应的HTTP请求（<strong>响应行</strong>、<strong>响应头</strong>、<strong>响应体</strong>）</li>
</ol>
<p>Nginx也是这个套路，整体流程一致：<br><img src="/images/nginx-request-process-model.png"></p>
<h2 id="模块化体系结构"><a href="#模块化体系结构" class="headerlink" title="模块化体系结构"></a>模块化体系结构</h2><p><img src="/images/nginx-architecture.png"><br>nginx的模块根据其功能基本上可以分为以下几种类型：</p>
<ul>
<li><strong>event module</strong>：搭建了独立于操作系统的事件处理的框架，及提供了各具体时间的处理。包括ngx_events_module,ngx_event_core_module和ngx_epool_module等。nginx具体使用何种时间处理模块，这依赖于具体的操作系统和编译选项。</li>
<li><strong>phase handler</strong>：此类型的模块也被直接成为handler模块。主要负责处理客户端请求并产生待相应内容，比如ngx_http_static_module模块，负责客户端的静态页面请求处理并将对应的磁盘文件准备为响应内容输出。</li>
<li><strong>output filter</strong>：也被成为filter模块，主要是负责对输出的内容进行处理，可以对输出进行修改。例如，可以实现对输出的所有html页面增加预定义的footbar一类的工作，或者对输出的图片的URL进行替换之类的工作。</li>
<li><strong>upstream</strong>：upstream模块实现反向代理的功能，将真正的请求转发到后端服务器上，并从后端服务器上读取相应，发回客户端。upstream模块是一种特殊的handler，只不过相应内容不是真正由自己产生的，而是从后端服务器上读取的。</li>
<li><strong>load-balancer</strong>：负载均衡，实现特定的算法，在众多的后端服务器中，选择一个服务器出来作为某个请求的转发服务器。</li>
</ul>
]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx路径匹配</title>
    <url>/2020/03/11/Nginx%E8%B7%AF%E5%BE%84%E5%8C%B9%E9%85%8D/</url>
    <content><![CDATA[<p>引用：</p>
<ul>
<li><a href="http://ningg.top/nginx-series-practice-location/">http://ningg.top/nginx-series-practice-location/</a></li>
</ul>
<h1 id="1-目标"><a href="#1-目标" class="headerlink" title="1. 目标"></a>1. 目标</h1><p>nginx反向代理，路径映射过程是什么？如何配置路径映射规则？</p>
<h1 id="2-location路径匹配"><a href="#2-location路径匹配" class="headerlink" title="2. location路径匹配"></a>2. location路径匹配</h1><h2 id="2-1-匹配规则"><a href="#2-1-匹配规则" class="headerlink" title="2.1 匹配规则"></a>2.1 匹配规则</h2><p>location路径正则匹配</p>
<table>
<thead>
<tr>
<th>符号</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>~</td>
<td>正则匹配，区分大小写</td>
</tr>
<tr>
<td>~*</td>
<td>正则匹配，不区分大小写</td>
</tr>
<tr>
<td>^~</td>
<td>普通字符匹配，如果该选项匹配，则，只匹配该选项，不再向下匹配其他选项</td>
</tr>
<tr>
<td>&#x3D;</td>
<td>普通字符匹配，精确匹配</td>
</tr>
<tr>
<td>@</td>
<td>定义一个命名的location，用于内部定向，例如error_page,try_files</td>
</tr>
</tbody></table>
<h2 id="2-2-匹配优先级"><a href="#2-2-匹配优先级" class="headerlink" title="2.2 匹配优先级"></a>2.2 匹配优先级</h2><p>路径匹配，优先级：（跟location的书写顺序关系不大）</p>
<ol>
<li><strong>精确匹配</strong>：&#x3D;前缀的指令严格匹配这个查询。如果找到，停止搜索；</li>
<li><strong>普通字符匹配</strong>：所有剩下的常规字符串，最长的匹配。如果这个匹配使用^~前缀，搜索停止；</li>
<li><strong>正则匹配</strong>：正则表达式，在配置文件中定义的顺序，匹配到一个结果，搜索停止；</li>
<li><strong>默认匹配</strong>：如果第3条规则产生匹配的话，结果被使用。否则，如同从第2条规则被使用。</li>
</ol>
<h2 id="2-3-举例"><a href="#2-3-举例" class="headerlink" title="2.3 举例"></a>2.3 举例</h2><p>通过一个实例，简单说明一下匹配优先级：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">location  = / &#123;</span><br><span class="line">  # 精确匹配 / ，主机名后面不能带任何字符串</span><br><span class="line">  [ configuration A ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location  / &#123;</span><br><span class="line">  # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求</span><br><span class="line">  # 但是正则和最长字符串会优先匹配</span><br><span class="line">  [ configuration B ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location /documents/ &#123;</span><br><span class="line">  # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">  [ configuration C ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location ~ /documents/Abc &#123;</span><br><span class="line">  # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索</span><br><span class="line">  # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条</span><br><span class="line">  [ configuration CC ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location ^~ /images/ &#123;</span><br><span class="line">  # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。</span><br><span class="line">  [ configuration D ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location ~* \.(gif|jpg|jpeg)$ &#123;</span><br><span class="line">  # 匹配所有以 gif,jpg或jpeg 结尾的请求</span><br><span class="line">  # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则</span><br><span class="line">  [ configuration E ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location /images/ &#123;</span><br><span class="line">  # 字符匹配到 /images/，继续往下，会发现 ^~ 存在</span><br><span class="line">  [ configuration F ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location /images/abc &#123;</span><br><span class="line">  # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在</span><br><span class="line">  # F与G的放置顺序是没有关系的</span><br><span class="line">  [ configuration G ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location ~ /images/abc/ &#123;</span><br><span class="line">  # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用</span><br><span class="line">    [ configuration H ]</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">location ~* /js/.*/\.js</span><br></pre></td></tr></table></figure>
<p>按照上面的location写法，以下的匹配示例成立：</p>
<ol>
<li>&#x2F; -&gt; config A：精确完全匹配，即使&#x2F;index.html也匹配不了</li>
<li>&#x2F;downloads&#x2F;download.html -&gt; config B：匹配B以后，往下没有任何匹配，采用B</li>
<li>&#x2F;images&#x2F;1.gif -&gt; configuration D：匹配到F，往下匹配到D，停止往下</li>
<li>&#x2F;images&#x2F;abc&#x2F;def -&gt; config D：最长匹配到G，往下匹配D，停止往下你可以看到 任何以&#x2F;images&#x2F;开头的都会匹配到D并停止，FG写在这里是没有任何意义的，H是永远轮不到的，这里只是为了说明匹配顺序</li>
<li>&#x2F;documents&#x2F;document.html -&gt; config C：匹配到C，往下没有任何匹配，采用C</li>
<li>&#x2F;documents&#x2F;1.jpg -&gt; configuration E：匹配到C，往下正则匹配到E</li>
<li>&#x2F;documents&#x2F;Abc.jpg -&gt; config CC：最长匹配到C，往下正则顺序匹配到CC，不会往下到E</li>
</ol>
]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx配置</title>
    <url>/2020/03/10/Nginx%E9%85%8D%E7%BD%AE/</url>
    <content><![CDATA[<h1 id="查看Nginx配置"><a href="#查看Nginx配置" class="headerlink" title="查看Nginx配置"></a>查看Nginx配置</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nginx -V</span><br><span class="line">nginx version: nginx/1.8.0</span><br><span class="line">built by clang 7.0.0 (clang-700.0.72)</span><br><span class="line">built with OpenSSL 1.0.2d 9 Jul 2015</span><br><span class="line">TLS SNI support enabled</span><br><span class="line"></span><br><span class="line">configure arguments:</span><br><span class="line">--prefix=/usr/local/Cellar/nginx/1.8.0</span><br><span class="line">--with-http_ssl_module</span><br><span class="line">--with-pcre</span><br><span class="line">--with-ipv6</span><br><span class="line">--sbin-path=/usr/local/Cellar/nginx/1.8.0/bin/nginx</span><br><span class="line">--with-cc-opt=&#x27;-I/usr/local/Cellar/pcre/8.37/include -I/usr/local/Cellar/openssl/1.0.2d_1/include&#x27;</span><br><span class="line">--with-ld-opt=&#x27;-L/usr/local/Cellar/pcre/8.37/lib -L/usr/local/Cellar/openssl/1.0.2d_1/lib&#x27;</span><br><span class="line">--conf-path=/usr/local/etc/nginx/nginx.conf</span><br><span class="line">--pid-path=/usr/local/var/run/nginx.pid</span><br><span class="line">--lock-path=/usr/local/var/run/nginx.lock</span><br><span class="line">--http-client-body-temp-path=/usr/local/var/run/nginx/client_body_temp</span><br><span class="line">--http-proxy-temp-path=/usr/local/var/run/nginx/proxy_temp</span><br><span class="line">--http-fastcgi-temp-path=/usr/local/var/run/nginx/fastcgi_temp</span><br><span class="line">--http-uwsgi-temp-path=/usr/local/var/run/nginx/uwsgi_temp</span><br><span class="line">--http-scgi-temp-path=/usr/local/var/run/nginx/scgi_temp</span><br><span class="line">--http-log-path=/usr/local/var/log/nginx/access.log</span><br><span class="line">--error-log-path=/usr/local/var/log/nginx/error.log</span><br><span class="line">--with-http_gzip_static_module</span><br></pre></td></tr></table></figure>
<p>注意上面的几个配置：</p>
<ul>
<li>–conf-path : &#x2F;usr&#x2F;local&#x2F;etc&#x2F;nginx&#x2F;nginx.conf 配置文件</li>
<li>–http-log-path：访问日志</li>
<li>–error-log-path：错误日志</li>
</ul>
<h1 id="nginx-conf配置文件"><a href="#nginx-conf配置文件" class="headerlink" title="nginx.conf配置文件"></a>nginx.conf配置文件</h1><p>前面我们知道了，&#x2F;usr&#x2F;local&#x2F;etc&#x2F;nginx&#x2F;nginx.conf 是nginx服务运行过程中的具体配置文件<br>这一部分，我们将通过剖析nginx.conf文件，弄清下面几点：</p>
<ol>
<li>设置端口？</li>
<li>开启日志？修改日志存储路径？</li>
<li>配置反向代理映射规则？</li>
</ol>
<h2 id="文件内部结构"><a href="#文件内部结构" class="headerlink" title="文件内部结构"></a>文件内部结构</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#user  nobody;</span><br><span class="line">worker_processes  1;</span><br><span class="line"> </span><br><span class="line">#error_log  logs/error.log;</span><br><span class="line">#error_log  logs/error.log  notice;</span><br><span class="line">#error_log  logs/error.log  info;</span><br><span class="line"> </span><br><span class="line">#pid        logs/nginx.pid;</span><br><span class="line">  </span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">http &#123;</span><br><span class="line">    include       mime.types;</span><br><span class="line">    default_type  application/octet-stream;</span><br><span class="line"> </span><br><span class="line">    #log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;</span><br><span class="line">    #                  &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;</span><br><span class="line">    #                  &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;</span><br><span class="line"> </span><br><span class="line">    #access_log  logs/access.log  main;</span><br><span class="line"> </span><br><span class="line">    sendfile        on;</span><br><span class="line">    #tcp_nopush     on;</span><br><span class="line"> </span><br><span class="line">    #keepalive_timeout  0;</span><br><span class="line">    keepalive_timeout  65;</span><br><span class="line"> </span><br><span class="line">    #gzip  on;</span><br><span class="line"> </span><br><span class="line">    server &#123;</span><br><span class="line">        listen       8080;</span><br><span class="line">        server_name  localhost;</span><br><span class="line"> </span><br><span class="line">        #charset koi8-r;</span><br><span class="line"> </span><br><span class="line">        #access_log  logs/host.access.log  main;</span><br><span class="line"> </span><br><span class="line">        location / &#123;</span><br><span class="line">            root   html;</span><br><span class="line">            index  index.html index.htm;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        #error_page  404              /404.html;</span><br><span class="line"> </span><br><span class="line">        # redirect server error pages to the static page /50x.html</span><br><span class="line">        #</span><br><span class="line">        error_page   500 502 503 504  /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">            root   html;</span><br><span class="line">        &#125;</span><br><span class="line"> </span><br><span class="line">        # proxy the PHP scripts to Apache listening on 127.0.0.1:80</span><br><span class="line">        #</span><br><span class="line">        #location ~ \.php$ &#123;</span><br><span class="line">        #    proxy_pass   http://127.0.0.1;</span><br><span class="line">        #&#125;</span><br><span class="line"> </span><br><span class="line">        # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000</span><br><span class="line">        #</span><br><span class="line">        #location ~ \.php$ &#123;</span><br><span class="line">        #    root           html;</span><br><span class="line">        #    fastcgi_pass   127.0.0.1:9000;</span><br><span class="line">        #    fastcgi_index  index.php;</span><br><span class="line">        #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;</span><br><span class="line">        #    include        fastcgi_params;</span><br><span class="line">        #&#125;</span><br><span class="line"> </span><br><span class="line">        # deny access to .htaccess files, if Apache&#x27;s document root</span><br><span class="line">        # concurs with nginx&#x27;s one</span><br><span class="line">        #</span><br><span class="line">        #location ~ /\.ht &#123;</span><br><span class="line">        #    deny  all;</span><br><span class="line">        #&#125;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    # another virtual host using mix of IP-, name-, and port-based configuration</span><br><span class="line">    #</span><br><span class="line">    #server &#123;</span><br><span class="line">    #    listen       8000;</span><br><span class="line">    #    listen       somename:8080;</span><br><span class="line">    #    server_name  somename  alias  another.alias;</span><br><span class="line"> </span><br><span class="line">    #    location / &#123;</span><br><span class="line">    #        root   html;</span><br><span class="line">    #        index  index.html index.htm;</span><br><span class="line">    #    &#125;</span><br><span class="line">    #&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">    # HTTPS server</span><br><span class="line">    #</span><br><span class="line">    #server &#123;</span><br><span class="line">    #    listen       443 ssl;</span><br><span class="line">    #    server_name  localhost;</span><br><span class="line"> </span><br><span class="line">    #    ssl_certificate      cert.pem;</span><br><span class="line">    #    ssl_certificate_key  cert.key;</span><br><span class="line"> </span><br><span class="line">    #    ssl_session_cache    shared:SSL:1m;</span><br><span class="line">    #    ssl_session_timeout  5m;</span><br><span class="line"> </span><br><span class="line">    #    ssl_ciphers  HIGH:!aNULL:!MD5;</span><br><span class="line">    #    ssl_prefer_server_ciphers  on;</span><br><span class="line"> </span><br><span class="line">    #    location / &#123;</span><br><span class="line">    #        root   html;</span><br><span class="line">    #        index  index.html index.htm;</span><br><span class="line">    #    &#125;</span><br><span class="line">    #&#125;</span><br><span class="line">    include servers/*;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>nginx.conf文件内部结构：</p>
<ol>
<li>worker_processes</li>
<li>error_log</li>
<li>events</li>
<li>http<ol>
<li>access_log</li>
<li>server<ol>
<li>listen</li>
<li>server_name</li>
<li>access_log</li>
<li>location<br>注：https时，上面的server配置略有不同。</li>
</ol>
</li>
</ol>
</li>
</ol>
<h2 id="静态资源"><a href="#静态资源" class="headerlink" title="静态资源"></a>静态资源</h2><p>系统设计时，动态资源、静态资源，要在url上能够区分出来，这样才能使用nginx为静态资源提供单独的映射关系。<br>举例：下面就是动静资源分离的简单配置：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    location / &#123;</span><br><span class="line">        root /data/www;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    location /images/ &#123;</span><br><span class="line">        root /data;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="动态资源"><a href="#动态资源" class="headerlink" title="动态资源"></a>动态资源</h2><p>对于动态资源，nginx一般会把request转发到相应的服务器。<br>举例：下面把动态资源转发到其他服务器，静态资源直接指向本地。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass http://localhost:8080/;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    location ~ \.(gif|jpg|png)$ &#123;</span><br><span class="line">        root /data/images;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，location使用了process_pass配置，当proxy_pass指向多个ip地址时，可以使用 server group 配置，如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">upstream backend &#123;</span><br><span class="line">    server backend1.example.com       weight=5;</span><br><span class="line">    server backend2.example.com:8080;</span><br><span class="line">    server unix:/tmp/backend3;</span><br><span class="line"> </span><br><span class="line">    server backup1.example.com:8080   backup;</span><br><span class="line">    server backup2.example.com:8080   backup;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">server &#123;</span><br><span class="line">    location / &#123;</span><br><span class="line">        proxy_pass http://backend;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>Nginx限速模块探讨</title>
    <url>/2020/03/09/Nginx%E9%99%90%E9%80%9F%E6%A8%A1%E5%9D%97%E6%8E%A2%E8%AE%A8/</url>
    <content><![CDATA[<p>Nginx限速模块探讨，引用：</p>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/32391675">https://zhuanlan.zhihu.com/p/32391675</a></li>
<li><a href="http://tengine.taobao.org/book/index.html">http://tengine.taobao.org/book/index.html</a></li>
</ul>
<h1 id="核心算法"><a href="#核心算法" class="headerlink" title="核心算法"></a>核心算法</h1><p>在探讨Nginx限速模块之前，我们先来看看网络传输中常用两个的流量控制算法：漏桶算法和令牌桶算法。</p>
<h2 id="漏桶算法（leaky-bucket）"><a href="#漏桶算法（leaky-bucket）" class="headerlink" title="漏桶算法（leaky bucket）"></a>漏桶算法（leaky bucket）</h2><p>漏桶算法（leaky bucket）思想如图所示：<br><img src="/images/v2-2ff7a9dbc1242a4c1abf960efe116813_r.jpg"><br>一个形象的解释是：</p>
<ul>
<li>水（请求）从上方倒入水桶，从水桶下方流出（被处理）；</li>
<li>来不及流出的水存在水桶中（缓冲），同时水桶中的水以<strong>固定速率</strong>流出；</li>
<li>水桶满后，水溢出（丢弃）。<br>这个算法的核心是：缓存请求、匀速处理、多余的请求直接丢弃。</li>
</ul>
<h2 id="令牌桶算法（token-bucket）"><a href="#令牌桶算法（token-bucket）" class="headerlink" title="令牌桶算法（token bucket）"></a>令牌桶算法（token bucket）</h2><p>令牌桶算法（token bucket）思想如图所示：<br><img src="/images/v2-fe9b3489d9c7a462b5594faf2a80266b_r.jpg"><br>算法思想是：</p>
<ul>
<li>令牌以固定的速率产生，并缓存到令牌桶中；</li>
<li>令牌桶放满时，多余的令牌被丢弃；</li>
<li>请求要消耗等比例的令牌才能被处理（处理多少请求，消耗多少令牌）；</li>
<li>令牌不够时，请求被缓存。</li>
</ul>
<p>相比漏桶算法，令牌桶算法不同之处在于它不但有一只“桶”，还有个队列，这个桶是用来存放令牌的，队列才是用来存放请求的。</p>
<p>从作用上来说，漏桶算法和令牌桶算法最明显的区别就是是否允许<strong>突发流量（burst</strong>的处理，漏桶算法能够<strong>强行限制数据的实时传输（处理）速率</strong>，对突发流量不做额外处理；而令牌桶算法能够在<strong>限制数据的平均传输速率的同时允许某种程度的突发传输</strong>。</p>
<p>Nginx按请求速率限速模块使用的是漏桶算法，即能够强行保证请求的实时处理速度不会超过设置的阀值。</p>
<h1 id="Nginx限速模块"><a href="#Nginx限速模块" class="headerlink" title="Nginx限速模块"></a>Nginx限速模块</h1><p>Nginx主要有两种限速方式：按连接数限速（ngx_http_limit_conn_module）、按请求速率限速（ngx_http_limit_req_module）。我们着重讲解按请求速率限速。</p>
<h2 id="按连接数限速"><a href="#按连接数限速" class="headerlink" title="按连接数限速"></a>按连接数限速</h2><p>按连接数限速是指限制单个IP（或者其他的key）同时发起的连接数，超出这个限制后，Nginx将直接拒绝更多的连接。这个模块的配置比较好理解，详见<a href="http://nginx.org/en/docs/http/ngx_http_limit_conn_module.html">ngx_http_limit_conn_module官方文档”</a>。</p>
<h2 id="按请求速率限速"><a href="#按请求速率限速" class="headerlink" title="按请求速率限速"></a>按请求速率限速</h2><p>按请求速率限速是指限制单个IP（或者其他的Key）发送请求的速率，超出指定速率后，Nginx将直接拒绝更多的请求。采用<strong>leaky bucket</strong>算法实现。为深入理解这个模块，我们先从实验现象说起。开始之前我们先简单介绍一个该模块的配置方式，以下面的配置为例：<br><img src="/images/v2-4711d21160627b6aec17baefc8aa3374_720w.jpg"><br>使用 limit_req_zone 关键字，我们定义一个名为mylimit大小为10MB的共享内存区域（zone），用来存放限速相关的统计信息，限速的 Key 值为二进制的IP地址（$binary_remote_addr），限速上限（rate）为 2r&#x2F;s；接着我们使用 limit_req 关键字将上述规则作用到 &#x2F;search&#x2F; 上。 burst 和 nodelay 的作用稍后解释。</p>
<p>使用上述规则，对于 &#x2F;search&#x2F; 目录的访问，单个IP的访问速度被限制在2请求&#x2F;秒，超过这个限制的访问将直接被Nginx拒绝。</p>
<h3 id="实验1—毫秒级统计"><a href="#实验1—毫秒级统计" class="headerlink" title="实验1—毫秒级统计"></a>实验1—毫秒级统计</h3><p>我们有如下配置：<br><img src="/images/v2-3dd07cea19be343996f01cfbe276feac_720w.jpg"><br>上述规则限制了每个IP访问的速度为2r&#x2F;s，并将该规则作用于跟目录。如果单个IP在非常短的时间内并发发送多个请求，结果会怎样呢？<br><img src="/images/v2-fa8d8095439c4391ef426190c0449c89_720w.jpg"><br>我们使用单个IP在10ms内并发，并发送了6个请求，只有1个成功，剩下的5个都被拒绝。我们设置的速度是2r&#x2F;s，为什么只有1个成功呢？<strong>是因为Nginx的限速统计是基于毫秒的，我们限制的速度是2r&#x2F;s，转换一下就是500ms内单个IP只允许通过1个请求</strong>，从501ms开始才允许通过第二个请求。<br><img src="/images/v2-a923be79e5bfb07aa0e0540c718a967a_720w.jpg"></p>
<h3 id="实验2—burst允许缓存处理突发请求"><a href="#实验2—burst允许缓存处理突发请求" class="headerlink" title="实验2—burst允许缓存处理突发请求"></a>实验2—burst允许缓存处理突发请求</h3><p>实验1中我们看到，短时间内发送了大量的请求，Nginx按照毫秒级精度统计，超出限制的请求直接拒绝。这在实际场景中未免过于苛刻，真实网络环境中请求到来不是匀速的，很可能有请求“突发”的情况。Nginx考虑到了这种情况，可以通过<strong>burst</strong>关键字开启对突发请求的缓存处理，而不是直接拒绝。<br>来看我们的配置：<br><img src="/images/v2-2121faf3d728d14e88f9fdb71e84d114_720w.jpg"><br>我们加入了<strong>burst&#x3D;4</strong>，意思是每个key（此处是每个IP）最多允许4个突发请求的到来。如果单个IP在10ms内发送6个请求，结果会怎样呢？<br><img src="/images/v2-70b29aa0d30abec245bdea38088d7ded_720w.jpg"><br>相比实验1成功数增加了4个，与我们设置的burst的数目一直。具体处理流程是：1个请求被立即处理，4个请求被放到burst队列中，另外1个请求被拒绝。<strong>通过设置burst参数，我们是的Nginx限流具备了缓存处理突发流量的能力</strong>。</p>
<p>但是请注意，burst的作用是让多余的请求可以先放到队列里，慢慢处理。如果不加nodelay参数，队列里的请求<strong>不会立即处理</strong>，而是按照rate设置的速度，以毫秒级精确的速度慢慢处理。</p>
<h3 id="实验3—nodelay降级排队时间"><a href="#实验3—nodelay降级排队时间" class="headerlink" title="实验3—nodelay降级排队时间"></a>实验3—nodelay降级排队时间</h3><p>实验2中我们看到，通过设置burst参数，我们允许Nginx缓存处理一定程度的突发，多余的请求可以先放到队列里，慢慢处理，起到了平滑流量的作用。但是如果队列设置的比较大，请求排队的时间比较长，用户角度看来就是RT（<strong>响应时间 Response Time</strong>）变长了，这对用户很不友好。有什么解决办法呢？<br><strong>nodelay参数允许请求在排队的时候就立即被处理，也就是说只要请求能够进度burst队列，就会立即被后台worker处理</strong>，请注意，这意味着burst设置了nodelay是，系统瞬间的QPS可能会超过rate设置的阀值。<strong>nodelay</strong>参数要跟<strong>burst</strong>一起使用才有作用。</p>
<p>延续实验2的配置，我们加入nodelay选项：<br><img src="/images/v2-ceb16668eb99de86d5c812c9cfdbf612_720w.jpg"><br>单个IP在10ms内并发发送6个请求，结果如下：<br><img src="/images/v2-f73b1d56d201662def37be57528c60ff_720w.jpg"><br>跟实验2相比，请求成功率没变化，但是<strong>总体耗时变短了</strong>。这怎么解释呢？实验2中，有4个请求被放到burst队列当中，工作进程每隔500ms（rate&#x3D;2r&#x2F;s）取一个请求进行处理，最后一个请求要排队2s才会被处理；实验3中，请求放入队列和实验2是一样的，但不同的是，队列中的请求同时具有了被处理的资格，所以实验3中的5个请求可以说是同时开始被处理，花费时间自然变短了。</p>
<p>但是请注意，虽然设置burst和nodelay能够降低突发请求的处理时间，但是长期来看并不会提高吞吐量的上限，长期吞吐量的上限是由rate决定的，因为nodelay只能保证burst的请求被立即处理，但是Nginx会限制队列元素释放的速度，就像是限制了令牌桶中令牌产生的速度。</p>
<p>看到这里你可能会问，加入了nodelay参数之后的限速算法，到底算是哪一个“桶”，是漏桶算法还是令牌桶算法？当然还算是漏桶算法。考虑一种情况，令牌桶算法的token未耗尽时会怎么做呢？由于它有一个请求队列，所以会把接下来的请求缓存下来，缓存多少受限于队列大小。但此时缓存这些请求还有意义吗？如果server已经过载，缓存队列越来越长，RT越来越高，即使过了很久请求被处理，对用户来说也没有什么价值了。所以当token不够用时，最明智的做法就是直接拒绝用户的请求，这就成了漏桶算法！</p>
<h1 id="源码剖析"><a href="#源码剖析" class="headerlink" title="源码剖析"></a>源码剖析</h1><p>具体内容省略，有兴趣的同学可以参看原文或参看源码</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文主要讲解了Nginx按请求速率限速模块的用法和原理，其中burst和nodelay参数是容易引起误解的，虽然可通过burst允许缓存处理突发请求，结合nodelay能够降低突发请求的处理时间，但是长期来看他们并不会提高吞吐量的上限，长期吞吐量的上限是由rate决定的。需要特别注意的事，burst设置了nodelay时，系统时间的QPS可能会超过rate设置的阀值。</p>
]]></content>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE-分布式系统的监控</title>
    <url>/2020/06/04/SRE-%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%9B%91%E6%8E%A7/</url>
    <content><![CDATA[<h1 id="1、术语定义"><a href="#1、术语定义" class="headerlink" title="1、术语定义"></a>1、术语定义</h1><ul>
<li>监控（monitoring）：收集、处理、汇总，并且显示关于某个系统的实时量化数据，例如请求的数量和类型，错误的数量和类型，以及处理用时，应用服务器的存活时间等。</li>
<li>白盒监控（white-box monitoring）：依靠系统内部暴露的一些性能指标进行监控。包括日志分析、Java虚拟机提供的监控接口，或者一个列出内部统计数据的HTTP接口进行监控。</li>
<li>黑盒监控（black-box monitoring）：通过测试某种外部用户可见的系统行为进行监控。</li>
<li>监控后台（dashboard）：提供某个服务核心指标一览服务的应用程序。该应用程序可能会提供过滤功能、选择功能，但是最主要的功能时用来显示系统最重要的指标。该程序同时可以显示相应团队的一些信息，包括目前工单的数量、高优先级的Bug列表、目前的on-call工程师和最近进行的生产发布等。</li>
<li>警报（alert）：目标对象是某个人法向某个系统地址的一个通知。目的地可以包括工单系统、Email系统，或者某个传呼机。相应的，这些警报被分类为：工单、Email警报，以及紧急警报（page—）。</li>
<li>根源问题（root case）：系统中的某种缺陷。这个缺陷如果被修复，就可以保证这种问题不会再以同样的方式发生。</li>
<li>节点或者机器（node、machine）：指的是物理机、虚拟机，或者容器内运行的某个实例。</li>
<li>推送（push）：关于某个服务正在运行的软件或者其配置文件的任何改动。</li>
</ul>
<h1 id="2、为什么要监控"><a href="#2、为什么要监控" class="headerlink" title="2、为什么要监控"></a>2、为什么要监控</h1><ul>
<li>分析长期趋势：数据库目前的数据量，以及增长速度。又例如每日活跃用户的数量增长的速度。跨时间范围的比较，或者是观察实验组与控制组之间的区别。</li>
<li>处理故障报警</li>
<li>构建监控后台页面</li>
<li>临时性的回溯分析（在线调试）</li>
</ul>
<h1 id="3、对监控系统设置合理的预期"><a href="#3、对监控系统设置合理的预期" class="headerlink" title="3、对监控系统设置合理的预期"></a>3、对监控系统设置合理的预期</h1><ul>
<li>专职人员负责持续优化和改进监控系统</li>
<li>监控系统规则遵循简单的原则</li>
<li>不建议在监控系统中维护较为复杂的依赖关系</li>
</ul>
<h1 id="4、现象与原因"><a href="#4、现象与原因" class="headerlink" title="4、现象与原因"></a>4、现象与原因</h1><p>监控系统应该解决两个问题：什么东西出现故障，以及为什么出故障。<br>现象与原因的示例：<br><img src="/images/NeatReader-1591255832407.png"></p>
<p>现象与原因的区分是构建信噪比高的监控系统时最重要的概念。</p>
<h1 id="5、黑盒监控与白盒监控"><a href="#5、黑盒监控与白盒监控" class="headerlink" title="5、黑盒监控与白盒监控"></a>5、黑盒监控与白盒监控</h1><p>黑盒监控与白盒监控最简单的区别是：黑盒监控是面向现象的，代表了目前正在发生（而非预测会发生的）的问题，即系统现在有故障。百合监控则大量依赖对系统内部信息的检测，如系统日志、抓取提供指标的HTTP节点等。白盒监控系统因此可以检测到即将发生的问题以及那些重试锁掩盖的问题等。</p>
<h1 id="6、4个黄金指标"><a href="#6、4个黄金指标" class="headerlink" title="6、4个黄金指标"></a>6、4个黄金指标</h1><ul>
<li>延迟</li>
<li>流量</li>
<li>错误</li>
<li>饱和度</li>
</ul>
<h1 id="7、关于长尾问题"><a href="#7、关于长尾问题" class="headerlink" title="7、关于长尾问题"></a>7、关于长尾问题</h1><p>采用直方图的形式，比如：延迟为 0<del>10ms之间的请求数量有多少，30</del>100ms之间，100~300之间等。将直方图的边界定义为指数型增长是直观展现请求分布的最好方式。</p>
<h1 id="8、度量指标时采用合适的精度"><a href="#8、度量指标时采用合适的精度" class="headerlink" title="8、度量指标时采用合适的精度"></a>8、度量指标时采用合适的精度</h1><h1 id="9、简化，知道不能再简化"><a href="#9、简化，知道不能再简化" class="headerlink" title="9、简化，知道不能再简化"></a>9、简化，知道不能再简化</h1><p>设计监控系统时一定要追求简化：</p>
<ul>
<li>那些最能反映真实故障的规则应该越简单越好，可预测性强，非常可靠</li>
<li>那些不常用的数据收集、汇总，以及警报配置应该定时删除</li>
<li>收集到的信息，但是没有暴露给任何监控台，或者被任何警报规则使用的应该定时删除</li>
</ul>
<h1 id="10、监控系统的长期维护"><a href="#10、监控系统的长期维护" class="headerlink" title="10、监控系统的长期维护"></a>10、监控系统的长期维护</h1><p>在现代生产环境中，监控系统需要跟随不断演变的软件一起变化，软件经常重构，负载特性和性能目标也经常变化。</p>
<h1 id="11、小结"><a href="#11、小结" class="headerlink" title="11、小结"></a>11、小结</h1><p>健康的监控和警报系统应该是非常简单、易于理解的。紧急警报应该关注于现象，针对原因的一些启发性分析应该作为调试过程中的补充，而不应该进行报警。监控的技术栈层面越高，监控现象越容易，但是监控某些子系统（如数据库）的饱和度和性能参数可能要在该子系统内部直接进行。Email警报的价值通常极为有限，很容易变成噪声。我们应该倾向于构建一个良好的监控后台页面，直接显示所有的非紧急的异常情况。</p>
<p>长远来看，要建立一个成功的on-call轮值体系，以及构建一个稳定的产品需要选择那些正在发生和即将发生的问题来进行报警，设置一个可以实际达到的合理目标，保证监控系统可以支持快速的问题定位与检测。</p>
]]></content>
      <tags>
        <tag>SRE</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE指导思想</title>
    <url>/2020/04/27/SRE%E6%8C%87%E5%AF%BC%E6%80%9D%E6%83%B3/</url>
    <content><![CDATA[<h1 id="一、拥抱风险"><a href="#一、拥抱风险" class="headerlink" title="一、拥抱风险"></a>一、拥抱风险</h1><p>没有百分之百可靠的服务，如果专注于提高服务的可靠性，对服务本身或者用户来说，结果不一定会更好甚至更差。极端的提高可靠性将会带来成本的大幅提升：过分追求稳定性限制了新功能的开发速度和产品交付速度，并且很大程度地增加了成本，反过来又减少了团队可以提供新功能的数量。<br>另外，用户通常不会注意到一项服务在高可靠性和极端可靠性之间的差异，因为用户体验主要受较不可靠的组件主导，即木桶理论。<br>基于这几点，SRE旨在寻求快速创新和高效的服务运营业务之间的风险的平衡，而不是简单的将服务在线时间最大化。这样一来，我们可以优化用户的整体体验，平衡系统的功能、服务和性能。</p>
<h2 id="1、管理风险"><a href="#1、管理风险" class="headerlink" title="1、管理风险"></a>1、管理风险</h2><p>为了提高用户对系统的信息，我们要减少系统出现故障的几率。然而，经验表明，在构建系统的过程中，可靠性进一步提升的成本并不是线性增加的—-可靠性的下一个改进可能比之前的改进成本增加100倍。高昂的成本主要体现在：</p>
<ul>
<li>冗余物理服务器&#x2F;计算资源的成本</li>
</ul>
<p>通过投入冗余设备，我们可以进行常规的系统离线或其他预料之外的维护性操作。又或者可以利用一些空间来存储奇偶校验码块，以此来提供一定程度的数据持久性保证。</p>
<ul>
<li>机会成本</li>
</ul>
<p>这类成本由一个组织承担。当该组织分配工程资源来构建减少风险的系统或功能，而非哪些用户直接可用的功能时需要成本这些成本。这些工程师不能再从事为终端用户设计新功能和新产品的工作。</p>
<p>在SRE团队中，我们管理服务的可靠性很大程度上是通过管理风险来进行的。我们是将风险作为一个连续体来认知的。我们的目标是：明确地将运维风险与业务风险对应起来。我们会努力提高一项服务的可靠性，但不会超过该服务需要的可靠性。也就是说，当设定一个可用性目标为99.99%时，我们即使要超过这个目标，也不会超过太多，否则会浪费为系统增加新功能、清理技术债务或降低运营成本的机会。从某种意义上来说，我们把可用性目标同时看做风险的上限和下限。这种表达方式的主要优势在于它可以促进团队进行明确的、深思熟虑的风险讨论。</p>
<h2 id="2、度量服务的风险"><a href="#2、度量服务的风险" class="headerlink" title="2、度量服务的风险"></a>2、度量服务的风险</h2><p>Google 标准做法是通过一个客观的指标来体现一个待优化的系统属性。通过设立这样一个目标，我们可以客观的评价目前的系统表现以及追踪一段时间内的改进和退步。对于服务风险而言，单一的性能指标不能代表所有潜在的因素。服务故障可能会有很多潜在的影响，这些因素的一部分可能很难被合理的度量。为了使这个问题再我们运行的各种类型的系统中易于处理，并且保持一致，我们选择主要关注计划外停机这个指标。</p>
<p>对于大多数服务而言，最直接的能够代表风险承受能力的指标就是对于计划外停机时间的可接受水平。计划外停机时间是由服务预期的可用性水平所体现的，通常用提供“9”系列的数字来体现。</p>
<ul>
<li>基于时间的可用性<br>可用性 &#x3D; 系统正常运行时间&#x2F;（系统正常运行时间+停机时间）</li>
<li>合计可用性<br>可用性 &#x3D; 成功请求数&#x2F;总的请求数</li>
</ul>
<p>使用请求成功率指标量化计划外停机时间使得这种指标更适合在不直接服务终端用户的系统中使用。、</p>
<p>通常，我们会为一项服务设定季度性的可用性目标，每周甚至每天对性能进行跟踪。我们通过寻找、跟踪和调整重要的、不可避免的偏差来使服务达到一个高层次的可用性目标。</p>
<h2 id="3、服务的风险容忍度"><a href="#3、服务的风险容忍度" class="headerlink" title="3、服务的风险容忍度"></a>3、服务的风险容忍度</h2><p>在一个正式的环境或安全关键的系统中，服务的风险容忍度通常是直接根据基本产品或服务的定义建立的。</p>
<h3 id="1）消费者服务的风险容忍度"><a href="#1）消费者服务的风险容忍度" class="headerlink" title="1）消费者服务的风险容忍度"></a>1）消费者服务的风险容忍度</h3><p>消费者服务对应的产品团队，负责了解用户和业务，通过这个软对来讨论服务的可靠性要求。</p>
<p>  因素：</p>
<ul>
<li>需要的可用性水平是什么？</li>
<li>不同类型的失败对服务有不同的影响吗？</li>
<li>我们如何使用服务成本来帮助在风险曲线上定位这个服务？</li>
<li>有哪些其他重要的服务指标需要考虑？</li>
</ul>
<p>  可用性目标：</p>
<ul>
<li>用户期望的服务水平是什么？</li>
<li>这项服务是否直接关系到收入？</li>
<li>这是一个有偿服务，还是免费服务？</li>
<li>如果市场上有竞争对手，那些竞争对手提供的服务水平如何？</li>
<li>这项服务是针对消费者还是企业？</li>
</ul>
<p>  故障的类型：</p>
<ul>
<li>我们的业务对于服务的停机时间的容忍程度有多高？</li>
<li>持续的低故障率或者偶尔发生的全网中断哪一个更糟糕？<br>  这两种类型的故障可能会导致绝对数量上完全相同的错误被返回，但可能对于业务的影响相差很大。</li>
</ul>
<p>  成本：</p>
<ul>
<li>构建和运维可用性再多一个“9”的系统，收益会增加多少？</li>
<li>额外的收入是否能够抵消为了达到这一可靠性水平所付出的成本？<br>  当我们无法简单的解释可靠性和收入的关系时，可能会更难设置这些目标。</li>
</ul>
<h3 id="2）基础设施服务的风险容忍度"><a href="#2）基础设施服务的风险容忍度" class="headerlink" title="2）基础设施服务的风险容忍度"></a>2）基础设施服务的风险容忍度</h3><p>构建和运维基础设施组件的要求在许多方面是不同于消费者服务的。一个根本的区别是，基础设施组件有多个客户，而他们通常有很多不同的需求。</p>
<ul>
<li><p>可用性目标水平<br>不同的用户服务，其基础设施的可用性目标水平不一致，风险容忍度可能相当不同。讷讷够同时满足多种情况的要求的一种方法是将所有基础设置服务做得极为可靠。但在实际情况中，这些基础设置服务往往需要占用大量资源，超高可靠性的代价通常是非常昂贵的。</p>
</li>
<li><p>故障类型</p>
</li>
<li><p>成本</p>
</li>
</ul>
<h1 id="二、服务质量"><a href="#二、服务质量" class="headerlink" title="二、服务质量"></a>二、服务质量</h1><h2 id="1、-服务质量术语"><a href="#1、-服务质量术语" class="headerlink" title="1、 服务质量术语"></a>1、 服务质量术语</h2><ol>
<li>SLI，服务质量指标</li>
</ol>
<p>SLI 是指服务质量指标（indicator）：该服务的某项服务质量的一个具体量化指标。<br>大部分用户服务都将请求延迟作为一个关键SLI，其他的常见SLI包括错误率，系统吞吐量等等。这些度量通常是汇总过的：在某一个度量时间范围内将原始数据收集起来，计算速率、平均值、百分比等汇总数据。</p>
<ol start="2">
<li>SLO，服务质量目标</li>
</ol>
<p>SLO 是服务质量目标（Objective）：服务的某个SLI的目标值，或者目标范围。SLO的定义是SLI ≤ 目标值或者范围下线 ≤ SLI ≤ 范围上限。</p>
<ol start="3">
<li>SLA，服务质量协议</li>
</ol>
<p>SLA是服务质量协议（Agreement）：指服务与用户之间的一个明确的，或者不明确的协议，描述了在达到或者没有达到SLO之后的一个后果。这些后果可能是财务方面的（退款或者罚款），也可能是其他类型的。<br>SRE通常不会参与SLA的书写，因为SLA是与业务产品的决策紧密相关的。但是，SRE确实会参与帮助避免触发SLA中的惩罚性条款。同时SRE会参与制定具体的SLI：很明显，提供一个客户的方式来度量SLO是很重要的，否则大家就会产生分歧。<br>并不是所有的服务都有SLA，尤其是免费提供的服务。但是免费提供的服务如果出现不可用的情况，可能仍然会产生一系列的后果。所以不管服务是否具有SLA，定义SLI和SLO，并且用它们来管理服务质量都是很有价值的。</p>
<h2 id="2、指标在实践中的应用"><a href="#2、指标在实践中的应用" class="headerlink" title="2、指标在实践中的应用"></a>2、指标在实践中的应用</h2><h3 id="1）关键性SLI选择"><a href="#1）关键性SLI选择" class="headerlink" title="1）关键性SLI选择"></a>1）关键性SLI选择</h3><p>我们不应该将监控系统中的所有指标全部定义为SLI；只有理解用户对系统的真实需求才能真正决定哪些指标是否有用。指标过多会影响对那些真正重要的指标的关注，而选择指标过少则会导致某些重要的系统行为被忽略。<br>常见的服务，一般SLI通常会归类为以下几种：</p>
<ul>
<li>用户可见的服务系统，例如用户访问的Web站点服务，通常关心其可用性、延迟、以及吞吐量。</li>
<li>存储系统，通常强调延迟、可用性和数据持久性</li>
<li>大数据系统，一般关心吞吐量、延迟</li>
</ul>
<h3 id="2）指标的收集"><a href="#2）指标的收集" class="headerlink" title="2）指标的收集"></a>2）指标的收集</h3><p>利用监控系统，大部分指标数据都在服务器端被收集。或者利用某种日志分析系统，例如分析日志中HTTP 500所占的比例。客户端的数据的收集，也是有必要的，否则可能会错失一些不影响服务端但是对用户产生影响的指标。</p>
<h3 id="3）汇总"><a href="#3）汇总" class="headerlink" title="3）汇总"></a>3）汇总</h3><p>为了简化和使数据更可用，我们经常需要汇总原始度量数据。但是汇总过程应该非常小心。</p>
<blockquote>
<p>某些指标的汇总看起来是很简单的，例如每秒服务请求的数量，但是即使这种简单度量也需要在某个度量时间范围内进行汇总。该度量值是应该每秒获取一次，还是每分钟内的平均值？后者可能会掩盖仅仅持续几秒的一次请求峰值。假设某个系统在偶数秒处理200个请求，在其他时间请求为0。该服务与持续每秒处理100个请求的服务平均负载是一样的，但是在即时负载上却是两倍。同样的，平均请求延迟可能看起来很简单，但是却掩盖了一个重要的细节；很可能大部分请求都是很快的，但是长尾请求速度却很慢。</p>
</blockquote>
<h3 id="4）指标的标准化"><a href="#4）指标的标准化" class="headerlink" title="4）指标的标准化"></a>4）指标的标准化</h3><p>我们建议标准化一些常见的SLI，以避免每次都要重新评估它们。任何一个符合标准定义模板的服务可以不需要再次自己定义SLI。</p>
<ul>
<li>汇总间隔：每1分钟汇总一次</li>
<li>汇总范围：集群中的全部任务</li>
<li>度量频率：每10秒一次</li>
<li>包含哪些请求：从黑核监控任务发来的HTTP GET请求</li>
<li>数据如何获取：通过监控系统获取服务器端信息得到</li>
<li>数据访问延迟：从收到请求到最后一个字节被发出</li>
</ul>
<p>为了节约成本，应该为常见的指标构建一套可以重用的SLI模板，从而使得理解每个SLI更简单。</p>
<h2 id="3、目标在实践中的应用"><a href="#3、目标在实践中的应用" class="headerlink" title="3、目标在实践中的应用"></a>3、目标在实践中的应用</h2><p>我们应该从思考（或调研）用户最关心的方面入手，而非从现在什么能度量入手。用户真正关心的部分经常是度量起来很困难的，甚至是不可能的，所以我们需要以某种形式近似。然而，如果我们只是从可以简单度量的数值入手，最终的SLO的作用就会很有限。因此，与其选择指标，再想出对应的目标，不如从想要的目标返乡推导出具体的指标。</p>
<h3 id="1）目标的定义"><a href="#1）目标的定义" class="headerlink" title="1）目标的定义"></a>1）目标的定义</h3><p>为了更清晰的定义，SLO应该具体指出它们是如何被度量的，以及其有效条件。例如，我们可能说：</p>
<ul>
<li>99% 的 Get RPC 调用会在小于 100ms 的时间内完成（包括全部后端服务器）</li>
<li>99% 的 Get RPC 会在 100ms 内完成</li>
</ul>
<p>如果性能曲线也很重要的话，我们可以指定多个SLO目标：</p>
<ul>
<li>90% 的 Get RPC 会在 1ms内完成</li>
<li>99% 的Get RPC 会在 10ms 内完成</li>
<li>99.9% 的 Get RPC 会在 100ms 内完成</li>
</ul>
<p>如果我们同时具有批处理业务（关注吞吐量）以及在线交互用户（关注延迟），name可能应该为每种负载指定单独的SLO：</p>
<ul>
<li>95% 的批处理用户 Set RPC 应该在1s内完成</li>
<li>99% 的交互式用户Set RPC，并且RPC负载小于1KB的应该在10ms内完成</li>
</ul>
<p>要求SLO能够被100%满足是不正确，也是不现实的：过于强调这个会降低创新和部署的速度，增加一些成本过高、过于保守的方案。更好的方案是使用错误预算（对达不到SLO的容忍度），以天或者以周为单位计量。高层管理者可能同时也需要按月度或者季度的评估。（错误预算其实就是保证达到其他SLO的一个SLO！）</p>
<p>SLO不达标的频率可以用来与错误预算进行对比，利用这两个数值的差值可以知道新版本的发布。</p>
<h3 id="2）目标的选择"><a href="#2）目标的选择" class="headerlink" title="2）目标的选择"></a>2）目标的选择</h3><p>选择目标SLO不是一个纯粹的技术活动，因为这里还涉及产品和业务层面的决策，SLI和SLO（甚至SLA）的选择都应该直接反应该决策。同样的，有时候可能可以牺牲某些产品特性，以便满足人员、上线时间、硬件可用性，以及资金的限制。SRE应该积极参与这类讨论，提供有关可行性和风险性的建议，下面列出了一个有用的讨论：</p>
<ul>
<li>不要仅仅以目前的状态为基础选择目标，更要从全局出发，否则可能会导致团队被迫长期运维一个过时的系统，没有时间去推动架构重构等任务。</li>
<li>保持简单。SLI过于复杂的汇总模式可能会覆盖某种系统性能的变化，同时也更难以理解。</li>
<li>避免绝对值。虽然要求系统可以在没有任何延迟增长的情况下无限扩张，或者“永远”可能是很诱人的，但是这样的要求是不切实际的。就算有一个系统能够做到这一点，它也需要花很长的时间和成本来设计和构建，同时运维也很复杂。最关键的是，这可能比用户可以接受的标准要高太多。</li>
<li>SLO越少越好。应该仅仅选择足够的SLO来覆盖系统属性，一定要确保每一个SLO都是必不可少的：如果我们无法针对某个SLO达标问题说服开发团队，那么可能这个SLO就是不必要的。然而，不是所有的产品属性都能用SLO表达，用户的“满意度”就很难。</li>
<li>不要追求完美。我们可以随着时间流逝了解系统腥味之后优化SLO的定义。刚开始可以以一个宋丹的目标开始，主键收紧。这比一开始制定一个困难的目标，在出现问题时放松要好得多。</li>
</ul>
<p>SLO可以成为SRE和产品团队划分工作优先级的重要参考，因为SLO代表了用户体验的成都。好的SLO是对开发团队有效的、可行的激励机制。但是一个没有经过精心调校的SLO会导致浪费，某团队可能需要付出很大代价来维护一个过于激进的SLO，而如果SLO过于宋丹，则会导致产品效果很差。SLO是一个很重要的杠杆：要小心使用。</p>
<h3 id="3）控制手段"><a href="#3）控制手段" class="headerlink" title="3）控制手段"></a>3）控制手段</h3><p>SLI和SLO在决策系统运行时也非常有用：</p>
<ul>
<li>监控并且度量系统的SLI</li>
<li>比较SLI和SLO，以决定是否需要执行操作</li>
<li>如果需要执行操作，则要决定究竟什么操作需要被执行，以便满足目标</li>
<li>执行这些操作</li>
</ul>
<h3 id="4）SLO可以建立用户预期"><a href="#4）SLO可以建立用户预期" class="headerlink" title="4）SLO可以建立用户预期"></a>4）SLO可以建立用户预期</h3><p>通过公布SLO可以设置用户对系统行为的预期。用户经常希望知道他们可以预期的服务质量，以便理解该服务是否能够满足他们的要求。</p>
<ul>
<li>留出一定的安全区</li>
<li>实际SLO不要过高</li>
</ul>
<h2 id="4、协议在实践中的应用"><a href="#4、协议在实践中的应用" class="headerlink" title="4、协议在实践中的应用"></a>4、协议在实践中的应用</h2><p>起草一份SLA需要业务部门和法务部门选择合适的后果条款。SRE在这个过程中的作用是帮助这些部门理解SLA的SLO达标的概率和困难程度。需要针对SLO的建议也同样适用于SLA。最好在用户宣传方面保持保守，因为受众越广，修改和删除一个不合适或者很困难达到的SLA就越困难。</p>
<h1 id="三、减少琐事"><a href="#三、减少琐事" class="headerlink" title="三、减少琐事"></a>三、减少琐事</h1><h2 id="1、琐事的定义"><a href="#1、琐事的定义" class="headerlink" title="1、琐事的定义"></a>1、琐事的定义</h2><p>什么是琐事？琐事就是运维服务中手动性的，重复性的，可以被自动化、战术性，没有持久价值的工作。而且，琐事与服务呈线性关系的增长。并不是每件琐事都有以上全部特性，但是每件琐事都满足下列一个或多个属性：</p>
<ul>
<li>手动性</li>
<li>重复性</li>
<li>可以被自动化的</li>
<li>战术性的</li>
<li>没有持久价值</li>
<li>与服务同步线性增长</li>
</ul>
<h2 id="2、为什么琐事越少越好"><a href="#2、为什么琐事越少越好" class="headerlink" title="2、为什么琐事越少越好"></a>2、为什么琐事越少越好</h2><p>SRE的一个公开目标是保持每个SRE的工作时间中运维工作（即琐事）的比例低于50%。SRE至少花50%的时间在工程项目上，以减少未来的琐事或增加服务功能。增加服务功能包括提高可靠性、性能，或利用率，同时也会进一步消除琐事。</p>
<p>SRE公开50%这个目标是因为如果不加以控制，琐事会变得越来越多，以至于迅速占据我们每个人100%的时间。减少琐事和扩大服务规模的工作就是SRE中的E（Engineering）。对工程工作的关注使SRE可以再服务规模扩大的同时减少人数，并且比单纯的研发团队和单纯的运维工作团队能更有效的管理服务的秘诀。</p>
<p>不仅如此，招聘新的SRE时，我们也会引用上下文提及的50%规则，承诺新员工不会专门进行运维工作。我们通过禁止SRE组织或者其中任何小团队退化为专门从事运维工作的组织来实现这个承诺。</p>
<h3 id="琐事的计算"><a href="#琐事的计算" class="headerlink" title="琐事的计算"></a>琐事的计算</h3><p>如果我们想要将一个SRE花在琐事上的时间限制在50%，应该如何分配时间呢？<br>任何一个SRE在参与on-call时都会承担一定程度的琐事。一个典型的SRE每个周期中会有一周主on-call和一周副on-call的工作。因为，在一个六个人的轮流周期中，没六周中至少有两周需要专注于on-call和中断性事务的处理，这意味着潜在的琐事的最小值是一个SRE的工作时间的2&#x2F;6，也就是33%。如果是八人轮值，name最小值就是2&#x2F;8，即25%。<br>与此计算相一致，来自SRE的数据显示，琐事的最大来源就是中断性工作。另一个主要来源是on-call，紧随其后的事发布和数据更新。及时Google的发布和数据更新过程通常是高度自动化的，这个部分仍有许多改进空间。<br>当某个SRE报告自己承担了过量的琐事时，这通常意味着管理者需要在团队中更均衡地分布琐事负荷，同时应该鼓励该SRE找到自己满意的工程项目</p>
<h2 id="3、什么是工程工作"><a href="#3、什么是工程工作" class="headerlink" title="3、什么是工程工作"></a>3、什么是工程工作</h2><p>工程工作是一种新颖的、本质上需要主观判断的工作。它是符合长期策略的，会对你的服务进行长久性的改善的工作。工程工作通常是有创新性和创造性的，着重通过设计来解决问题，解决方案越通用越好。工程工作有助于使该团队或是整个SRE组织在维持同等人员配备的情况下接手更大或更多的服务。<br>典型的SRE活动分为如下几类：</p>
<ul>
<li>软件工程</li>
<li>系统工程</li>
<li>琐事</li>
<li>流程负担</li>
</ul>
<h2 id="4、辩证看待琐事"><a href="#4、辩证看待琐事" class="headerlink" title="4、辩证看待琐事"></a>4、辩证看待琐事</h2><p>琐事不会总是让每个人都不开心，特别是不太多的时候。已知的和重复性的工作有一种让人平静的功效。完成这些事可以带来一种满足感和快速胜利感。琐事可能是低风险低压力的活动，有些员工甚至喜欢做这种类型的工作。</p>
<p>琐事的存在并不总是坏事，但是每个人都必须清楚，在SRE所扮演的角色中，一定数量的琐事是不可避免的，这其实是任何工程类工作都具有的特点。少量的琐事存在不是什么大问题。但是一旦琐事的数量变多，就又害了。如果琐事特别繁重，那就应该非常担忧，大声抱怨。在许多琐事有害的原因中，有如下因素需要考虑：</p>
<ul>
<li>职业停滞：如果花在工程项目上的时间太少，你的职业发展会变慢，甚至停滞</li>
<li>士气低落：过多的琐事会导致过度劳累、厌倦和不满</li>
<li>造成误解：确保SRE的工程实践，如果琐事过多，会破坏SRE这种角色，造成误解</li>
<li>进展缓慢：琐事过多导致团队生产力下降，SRE忙于手工操作，新功能的发布就会变慢</li>
<li>开创先例：如果SRE过于愿意承担琐事，研发同事就更强项羽加入更多的琐事，有时候甚至将本来应该由研发团队承担的运维工作转给SRE来承担。其他团队也会开始指望SRE接受这样的工作</li>
<li>促进摩擦产生：如果团队中的琐事太多，其实就是在鼓励团队中最好的工程师开始发现更有价值的工作</li>
<li>违反承诺：为了项目工程而入职的新员工，以及转入SRE工作的老员工，会有被欺骗的感觉，不利于团队的士气</li>
</ul>
]]></content>
      <tags>
        <tag>SRE</tag>
      </tags>
  </entry>
  <entry>
    <title>SRE方法论</title>
    <url>/2020/02/21/SRE%E6%96%B9%E6%B3%95%E8%AE%BA/</url>
    <content><![CDATA[<h1 id="方法论"><a href="#方法论" class="headerlink" title="方法论"></a>方法论</h1><h2 id="1-确保长期关注运维研发工作"><a href="#1-确保长期关注运维研发工作" class="headerlink" title="1. 确保长期关注运维研发工作"></a>1. 确保长期关注运维研发工作</h2><ul>
<li>最多 50% 的运维值班内容，最少 50% 的运维开发内容</li>
<li>控制处理紧急事件的数量，运维人员有充分时间处理故障、恢复服务以及事后进行复盘报告</li>
<li>事前报警，但是事后总结更加重要：复盘故障发生、报警和处理的全过程，挖掘故障发生的根本原因，提供相应的预防或解决方案，对事不对人</li>
</ul>
<h2 id="2-产品稳定性和迭代速度之间的矛盾"><a href="#2-产品稳定性和迭代速度之间的矛盾" class="headerlink" title="2. 产品稳定性和迭代速度之间的矛盾"></a>2. 产品稳定性和迭代速度之间的矛盾</h2><ul>
<li>可靠性目标 SLO</li>
<li>错误预算</li>
<li>利用错误预算，加快产品迭代速度，同时保证服务质量</li>
<li>最终目标不是“零事故”，而是在稳定和迭代之间寻求平衡，开发和运维之间协作创新</li>
</ul>
<h2 id="3-监控系统"><a href="#3-监控系统" class="headerlink" title="3. 监控系统"></a>3. 监控系统</h2><ul>
<li>紧急报警（alert） ：业务正常运行受到影响，需要紧急处理</li>
<li>工单（ticket）：业务不受影响，可以延期处理</li>
<li>日志（logging）：记录应用运行日志，用于调试和事后分析</li>
</ul>
<h2 id="4-应急事件处理"><a href="#4-应急事件处理" class="headerlink" title="4. 应急事件处理"></a>4. 应急事件处理</h2><ul>
<li>MTTF（平均失败时间）</li>
<li>MTTR（平均恢复时间）</li>
<li>自动恢复系统，而非需要人工干预，可减少恢复时间</li>
<li>运维手册（playbook）</li>
</ul>
<h2 id="5-变更管理"><a href="#5-变更管理" class="headerlink" title="5. 变更管理"></a>5. 变更管理</h2><ul>
<li>渐进式发布机制（<a href="https://www.cnblogs.com/nulige/articles/10929182.html">几种常见的发布方式</a>：<ul>
<li>蓝绿发布（AB发布）<br>  将应用所在集群上的机器从逻辑上分为A&#x2F;B两组。在新版发布时，首先把A组的机器从负载均衡中摘除，再进行新版本的部署。此时，B组仍然继续提供服务。<br>  <img src="/images/1053682-20190527095123236-652009308.png"><br>  当A组升级完毕后，负载均衡重新接入A组，再把B组从负载列表中摘除，进行新版本的部署。A组重新提供服务。<br>  <img src="/images/1053682-20190527095313544-971610690.png"><br>  特点：<ul>
<li>如果出问题，影响面较广，或者说很难控制具体的影响面</li>
<li>发布策略简单</li>
<li>用户无感知，平滑过渡</li>
<li>升级&#x2F;回滚速度快<br>  缺点：</li>
<li>需要准备正常业务使用资源的两倍以上服务器，防止升级期间单组无法承载业务突发</li>
<li>短时间内浪费一定资源成本</li>
<li>基础设施无改动，增大升级稳定性<br>  蓝绿发布在早期物理服务器时代，还是比较昂贵的，由于云计算普及，成本也大大降低</li>
</ul>
</li>
<li>灰度发布<br>  灰度发布只升级部分服务，即让一部分用户继续用老版本，一部分用户开始用新版本，如果用户对新版本没什么意见，那么逐步扩大范围，把所有用户都迁移到新版本上面来。<br>  <img src="/images/1053682-20190527095414439-329213436.png"><br>  特点：<ul>
<li>保证整体系统稳定性，在初始灰度的时候就可以发现、调整问题，影响范围可控</li>
<li>新功能逐步评估性能，稳定性和健康状况，如果出问题影响范围很小，相对用户体验也少</li>
<li>用户无感知，平滑过渡<br>  缺点：</li>
<li>自动化要求高<br>  部署过程：</li>
<li>从LB摘掉灰度服务器，升级成功后再加入LB</li>
<li>少量用户流量到新版本</li>
<li>如果灰度服务器测试成功，升级剩余服务器<br>  灰度发布是通过切换线上并存版本之间的路由权重，逐步从一个版本切换为另一个版本的过程</li>
</ul>
</li>
<li>滚动发布<br>  滚动发布是指每次只升级一个或多个服务，升级完成后加入生产环境，不断执行这个过程，直到集群中的全部旧版本升级新版本。<br>  <img src="/images/1053682-20190527095443475-675603692.png"><ul>
<li>红色：正在更新的实例</li>
<li>蓝色：更新完成并加入集群的实例</li>
<li>绿色：正在运行的实例<br>  特点：</li>
<li>用户无感知，平滑过渡</li>
<li>节约资源<br>  缺点：</li>
<li>部署时间慢，取决于每阶段更新时间</li>
<li>发布策略较复杂</li>
<li>无法确定OK的环境，不易回滚<br>  部署过程：</li>
<li>先升级1个副本，主要做部署验证</li>
<li>每次升级副本，自动从LB上摘掉，升级成功后自动加入集群</li>
<li>事先需要有自动更新策略，分为若干次，每次数量&#x2F;百分比可配置</li>
<li>回滚是发布的逆过程，先从LB摘掉新版本，再升级老版本，这个过程一般时间比较长</li>
<li>自动化要求高</li>
</ul>
</li>
<li>小结<br>  综上所述，三种方式均可以做到平滑式升级，在升级过程中服务仍然保持服务的连续性，升级对外界是无感知的。那生产上选择哪种部署方法最合适呢？这取决于哪种方法最适合你的业务和技术需求。如果你们运维自动化能力储备不够，肯定是越简单越好，建议蓝绿发布，如果业务对用户依赖很强，建议灰度发布。如果是K8S平台，滚动更新是现成的方案，建议先直接使用<ul>
<li>蓝绿发布：两套环境交替升级，旧版本保留一定时间便于回滚</li>
<li>灰度发布：根据比例将老版本升级，例如80%用户访问是老版本，20%用户访问是新版本</li>
<li>滚动发布：按批次停止老版本实例，启动新版本实例</li>
</ul>
</li>
</ul>
</li>
<li>迅速而准确的检测到问题的发生</li>
<li>当出现问题时，安全迅速的回退改动</li>
</ul>
<h2 id="6-需求预测和容量规划"><a href="#6-需求预测和容量规划" class="headerlink" title="6. 需求预测和容量规划"></a>6. 需求预测和容量规划</h2><p>业务的容量规划，包括自然增长（随着用户使用量上升，资源使用量也上升），也包括一些非自然增长的因素（如新功能的发布，商业推广，以及其他商业因素在内）</p>
<ul>
<li>必须有一个准确的自然增长需求预测模型，需求预测的时间应该超过资源获取的时间</li>
<li>规划中必须有准确的非自然增长的需求来源的统计</li>
<li>必须有周期性的压力测试，以便准确的将系统原始资源信息与业务容量对应起来</li>
</ul>
<h2 id="7-资源部署"><a href="#7-资源部署" class="headerlink" title="7. 资源部署"></a>7. 资源部署</h2><p>资源部署是变更管理与容量规划的结合产物。</p>
<ul>
<li>资源部署和配置必须迅速完成</li>
<li>仅在必要的时候执行，因为资源有限且昂贵</li>
<li>保证部署和配置过程执行的正确性，否则资源不可用</li>
<li>部署和配置过程影响较大，会有较大幅度修改，必须执行一系列测试，确保可以正确的提供服务</li>
</ul>
<h2 id="8-效率与性能"><a href="#8-效率与性能" class="headerlink" title="8. 效率与性能"></a>8. 效率与性能</h2><ul>
<li>高效的利用资源，盈利的必要性</li>
<li>SRE负责容量的部署和配置，承担有关利用率的讨论和改进</li>
<li>服务的利用率指标依赖于服务的工作方式和对容量的配置与部署</li>
<li>关注服务的容量配置策略，提升资源利用率，可以有效的降低系统成本<br>业务总体资源的使用情况的关键驱动因素：<ul>
<li>用户需求</li>
<li>可用容量</li>
<li>软件的资源使用效率<br>SRE可以通过模型预测用户需求，合理部署和配置可用容量，同时可以改进软件以提升资源的使用效率。通过这三个因素可以大幅度提升服务的效率。<br>软件系统一般来说在负载上升的时候，会导致延迟升高。延迟升高其实和容量损失是一样的。当负载达到临界线的时候，一个逐渐变慢的系统最终会停止一切服务。SRE的目标是根据一个预设的延迟目标，部署和维护足够的容量。SRE和研发团队应该共同监控和优化整个系统的性能，这就相当于给服务增加容量和提升效率了。</li>
</ul>
</li>
</ul>
]]></content>
      <tags>
        <tag>SRE</tag>
      </tags>
  </entry>
  <entry>
    <title>ceph</title>
    <url>/2021/04/15/ceph/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>iptables详解及常用规则</title>
    <url>/2020/03/13/iptables%E8%AF%A6%E8%A7%A3%E5%8F%8A%E5%B8%B8%E7%94%A8%E8%A7%84%E5%88%99/</url>
    <content><![CDATA[<p>引用：</p>
<ul>
<li><a href="https://www.cnblogs.com/sunsky303/p/12327863.html">https://www.cnblogs.com/sunsky303/p/12327863.html</a></li>
</ul>
<h1 id="iptables简介"><a href="#iptables简介" class="headerlink" title="iptables简介"></a>iptables简介</h1><p>netfilter&#x2F;iptables（简称iptables）组成linux平台下的包过滤防火墙，与大多数的linux软件一样，这个包过滤防火墙是免费的，它可以代替昂贵的商业防火墙解决方案，<strong>完成封包过滤、封包重定向和网络地址转换（NAT）等功能</strong>。</p>
<h2 id="iptables基础"><a href="#iptables基础" class="headerlink" title="iptables基础"></a>iptables基础</h2><p>规则（rules）其实就是网络管理员预定义的条件，规则一般的定位为“如果数据包头符合这样的条件，就这样处理这个数据包”。规则存储在内核空间的信息包过滤表中，这些规则分别指定了源地址、目的地址、传输协议（如TCP、UDP、ICP）和服务类型（如HTTP、FTP和SMTP等）等。当数据包与规则匹配时，iptables就根据规则所定义的方法来处理这些数据包，如方形（accept）、拒绝（reject）和丢弃（drop）等。匹配防火墙的主要工作就是添加、修改和删除这些规则。</p>
<h2 id="iptables和netfilter的关系："><a href="#iptables和netfilter的关系：" class="headerlink" title="iptables和netfilter的关系："></a>iptables和netfilter的关系：</h2><p>这是第一个要说的地方，iptables和netfilter的关系是一个很容易让人搞不清楚的问题。<strong>iptables只是linux防火墙的管理工具而已，位于&#x2F;sbin&#x2F;iptables。真正实现防火墙功能的事netfilter，它是linux内核中实现包过滤的内部结构</strong>。</p>
<h2 id="iptables传输数据包的过程"><a href="#iptables传输数据包的过程" class="headerlink" title="iptables传输数据包的过程"></a>iptables传输数据包的过程</h2><ol>
<li>当一个数据包进入网卡时，它首先进入prerouting链，内核根据数据包目的IP判断是否需要传送出去</li>
<li>如果数据包就是进入本机的，它就会沿着图向下移动，到达input链。数据包到了input链后，任何进程都会收到它。本机上运行的程序可以发送书包，这些数据包会经过output链，然后到达postrouting链输出。</li>
<li>如果数据包是要转发出去，且内核允许转发，数据包就会如图所示向右移动，经过forward链，然后到达postrouting链输出。<br><img src="/images/7775566-d1ad7de240451282.png"></li>
</ol>
<h2 id="iptables的规则表和链"><a href="#iptables的规则表和链" class="headerlink" title="iptables的规则表和链"></a>iptables的规则表和链</h2><p>表（tables）提供特定的功能，<strong>iptables内置了4个表，filter、nat、mangle和raw表，分别用于包过滤、网络地址转换、包重构（修改）和数据跟踪处理</strong>。<br>链（chains）是数据包传播的路径，每一条链其实就是众多规则中的一个检查清单，每一条链中可以由一条或者多条规则。当一个数据包达到一个链时，iptables就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。如果满足，系统就会根据该条规则所定义的方法处理数据包；否则iptables将继续检查下一条规则，如果该数据包不符合链中的任意一条规则，iptables就会根据该链预定义的默认策略来处理数据包。<br>iptables采用“表”和“链”的分层结构。在REHL5中是<strong>四张表五个链</strong>。下面罗列一下这四张表和五个链。注意一定要明白这些表和链的关系及作用。<br><img src="/images/7775566-5ec887df41cf1861.png"></p>
<h2 id="规则表"><a href="#规则表" class="headerlink" title="规则表"></a>规则表</h2><h3 id="1-Filter表-—-三个链：-input、forward、output"><a href="#1-Filter表-—-三个链：-input、forward、output" class="headerlink" title="1. Filter表 — 三个链： input、forward、output"></a>1. Filter表 — 三个链： input、forward、output</h3><p>作用：过滤数据包<br>内核模块：iptables_filter</p>
<h3 id="2-Nat表-—-三个链：-prerouting、postrouting、output"><a href="#2-Nat表-—-三个链：-prerouting、postrouting、output" class="headerlink" title="2. Nat表 — 三个链： prerouting、postrouting、output"></a>2. Nat表 — 三个链： prerouting、postrouting、output</h3><p>作用：用于网络地址转换<br>内核模块：iptables_nat</p>
<h3 id="3-Mangle表-–-五个链：prerouting、postrouting、input、output、forward，即作用于全部链（chains）"><a href="#3-Mangle表-–-五个链：prerouting、postrouting、input、output、forward，即作用于全部链（chains）" class="headerlink" title="3. Mangle表 – 五个链：prerouting、postrouting、input、output、forward，即作用于全部链（chains）"></a>3. Mangle表 – 五个链：prerouting、postrouting、input、output、forward，即作用于全部链（chains）</h3><p>作用：修改数据包的服务类型、TTL，并且可以配置路由实现QPS<br>内核模块：iptables_mangle</p>
<h3 id="4-Raw表-–-两个链：output、rerouting"><a href="#4-Raw表-–-两个链：output、rerouting" class="headerlink" title="4. Raw表 – 两个链：output、rerouting"></a>4. Raw表 – 两个链：output、rerouting</h3><p>作用：决定数据包是否被状态跟踪机制处理<br>内核模块：iptables_raw</p>
<p><strong>重要&#x2F;常用的事Filter和Nat表</strong></p>
<h2 id="规则链"><a href="#规则链" class="headerlink" title="规则链"></a>规则链</h2><hr>
<ol>
<li>input — 进来的数据包应用此规则链中的策略</li>
<li>output – 外出的数据包应用此规则链中的策略</li>
<li>forward — 转发数据包时应用此规则链中的策略</li>
<li>prerouting – 对数据包坐路由选择前应用此规则链中的规则<br><strong>（记住！所有的数据包进来的时候都先由这个链处理）</strong></li>
<li>postrouting — 对数据包作路由选择后应用此规则链中的规则<br><strong>（所有的数据包出来的时候都先由这个链处理）</strong></li>
</ol>
<h2 id="规则表之间的优先顺序"><a href="#规则表之间的优先顺序" class="headerlink" title="规则表之间的优先顺序"></a>规则表之间的优先顺序</h2><p><strong>Raw –&gt; Mangle –&gt; Nat –&gt; Filter</strong></p>
<p>规则链之间的优先顺序（分三种情况）：</p>
<h3 id="第一种情况“入站数据流向"><a href="#第一种情况“入站数据流向" class="headerlink" title="第一种情况“入站数据流向"></a>第一种情况“入站数据流向</h3><p>从外界到达防火墙的数据包，先被prerouting规则处理（是否修改数据包地址等），之后会进行路由选择（判断该数据包应该发送何处），如果数据包的目的主机是防火墙本机（比如说Internet用户访问防火墙主机中的web服务器的数据包），那么内核将其传给input链进行处理（决定是否允许通过等），通过以后再交给系统上层的应用程序（比如Apache服务器）进行响应。</p>
<h3 id="第二种情况：转发数据流向"><a href="#第二种情况：转发数据流向" class="headerlink" title="第二种情况：转发数据流向"></a>第二种情况：转发数据流向</h3><p>来自外界的数据包达到防火墙后，首先被prerouting规则链处理，之后会进行路由选择，如果数据包的目的地址是其他外部地址（比如局域网用户通过网关访问QQ站点的数据包），则内核将其传递给forward链进行处理（是否转发或者拦截），然后再交给postrouting规则链（是都修改数据包的地址等）进行处理。</p>
<h3 id="第三种情况：出站数据流向"><a href="#第三种情况：出站数据流向" class="headerlink" title="第三种情况：出站数据流向"></a>第三种情况：出站数据流向</h3><p>防火墙本机向外部地址发送的数据包（比如防火墙主机中测试公网DNS服务器时），首先被output规则链处理，之后进行路由选择，然后传递给postrouting规则链（是否修改数据包的地址等）进行处理。</p>
<h2 id="管理和设置iptables规则"><a href="#管理和设置iptables规则" class="headerlink" title="管理和设置iptables规则"></a>管理和设置iptables规则</h2><p><img src="/images/7775566-fb35d0a138063159.jpg"><br><img src="/images/7775566-736f5a5882a69420.jpg"></p>
<h3 id="iptables的基本语法格式"><a href="#iptables的基本语法格式" class="headerlink" title="iptables的基本语法格式"></a>iptables的基本语法格式</h3><p>iptables [-t 表名] 命令选项 [链名] [条件匹配] [-j 目标动作或跳转]<br>说明：表名、链名由于指定iptables命令所操作的表和链，命令选项用于指定管理iptables规则的方式（比如：插入、增加、删除、查看等）；条件匹配用于指定对符合什么样的条件的数据包进行处理；目标动作或跳转用于指定数据包的处理方式（比如允许通过、拒绝、丢弃、跳转给其他链处理）。</p>
<h3 id="iptables命令的管理控制选项"><a href="#iptables命令的管理控制选项" class="headerlink" title="iptables命令的管理控制选项"></a>iptables命令的管理控制选项</h3><p>-A 在指定链的末尾添加（append）一条新的规则<br>-D 删除（delete）指定链中的某一条规则，可以按规则序号和内容删除<br>-I 在指定链中插入（insert）一条新的规则，默认在第一行添加<br>-R 修改、替换（replace）指定链中的某一条规则，可以按规则序号和内容替换<br>-L 列出（list）指定链中的所有的规则进行查看<br>-E 重命名用户定义的链，不改变链本身<br>-F 清空（flush）所有规则<br>-N 新建（new-chain）一条用户自己定义的规则链<br>-X 删除指定表中用户自定义的规则链（delete-chain）<br>-P 设置指定链的默认策略（policy）<br>-Z 将所有表的所有链的字节和数据包计数器清零<br>-n 使用数字形式（number）显示输出结果<br>-v 查看规则表详细信息（verbose）的信息<br>-V 查看版本（version）<br>-h 获取帮助（help）</p>
<h3 id="防火墙处理数据包的四种方式"><a href="#防火墙处理数据包的四种方式" class="headerlink" title="防火墙处理数据包的四种方式"></a>防火墙处理数据包的四种方式</h3><p>Accept 允许数据包通过<br>Drop 直接丢弃数据包，不给任何回应信息<br>Reject 拒绝数据包通过，必要时会给数据发送端一个响应的信息<br>Log 在&#x2F;var&#x2F;log&#x2F;messages文件中记录日志信息，然后将数据包传递给下一条规则</p>
<h3 id="iptables防火墙规则的保存和恢复"><a href="#iptables防火墙规则的保存和恢复" class="headerlink" title="iptables防火墙规则的保存和恢复"></a>iptables防火墙规则的保存和恢复</h3><p>iptables-save把规则保存到文件中，再由目录rc.d下的脚本（&#x2F;etc&#x2F;rc.d&#x2F;init.d&#x2F;iptables）自动装载<br>使用命令iptables-save来保存规则。<br>一般用iptables-save &gt; &#x2F;etc&#x2F;sysconfig&#x2F;iptables 生成保存规则的文件<br>也可以用 service iptables save，它能把规则自动保存在 &#x2F;etc&#x2F;sysconfig&#x2F;iptables中。<br>当计算机启动时，rc.d下的脚本将用命令iptables-restore调用这个文件，从而自动恢复规则。</p>
<h3 id="删除input链的第一条规则"><a href="#删除input链的第一条规则" class="headerlink" title="删除input链的第一条规则"></a>删除input链的第一条规则</h3><p>iptables -D input 1</p>
<h3 id="iptables防火墙常用的策略"><a href="#iptables防火墙常用的策略" class="headerlink" title="iptables防火墙常用的策略"></a>iptables防火墙常用的策略</h3><ol>
<li><p>拒绝进入防火墙的所有ICMP协议数据包<br>iptables -I INPUT -p icmp -j REJECT</p>
</li>
<li><p>允许防火墙转发除了ICMP协议之外的所有数据包<br>iptables -A FORWARD -p !icmp -j ACCEPT<br>说明：使用“!”可以将条件取反</p>
</li>
<li><p>拒绝转发来自192.168.1.10主机的数据，允许转发来自192.168.0.0&#x2F;16网段的数据<br>iptables -A FORWARD -s 192.168.1.11 -j REJECT<br>iptables -A FORWARD -s 192.168.0.0&#x2F;16 -j ACCEPT<br>说明：注意要把拒绝的放在前面</p>
</li>
<li><p>丢弃从外网接口（eth1）进入防火墙本机的源地址为私网地址的数据包<br>iptables -A INPUT -i eth1 -s 192.168.0.0&#x2F;16 -j DROP<br>iptables -A INPUT -i eth1 -s 172.16.0.0&#x2F;12 -j DROP<br>iptables -A INPUT -i eth1 -s 10.0.0.0&#x2F;8 -j DROP</p>
</li>
<li><p>封堵网段（192.168.1.0&#x2F;24），两个两个小时候解封<br>iptables -I INPUT -s 192.168.1.0&#x2F;24 -j DROP<br>iptabls -I FORWARD -s 192.168.1.0&#x2F;24 -j DROP<br>at now 2 hours at&gt;iptables -D INPUT 1 at&gt; iptables -D FORWARD 1</p>
</li>
<li><p>只允许管理员从202.13.0.0&#x2F;16网段使用SSH远程登录防火墙主机<br>iptables -A INPUT -p tcp –dport 22 -s 202.13.0.0&#x2F;16 -j ACCEPT<br>iptables -A INPUT -p tcp –dport 22 -j DROP</p>
</li>
<li><p>允许本机开放从tcp端口20-1024提供的应用服务<br>iptables -A INPUT -p tcp –dport 20:1024 -j ACCEPT<br>iptables -A OUTPUT -p tcp –sport 20:1024 -j ACCEPT</p>
</li>
<li><p>允许转发来自192.168.0.0&#x2F;16局域网段的DNS解析请求数据包<br>iptables -A FORWARD -s 192.168.0.0&#x2F;16 -p udp –dport 53 -j ACCEPT<br>iptables -A FORWARD -d 192.168.0.0&#x2F;16 -p udp –sport 53 -j ACCEPT</p>
</li>
<li><p>禁止其他主机ping防火墙主机，但是允许从防火墙上ping其他主机<br>iptables -I INPUT -p icmp –icmp-type Echo-Request -j DROP<br>iptables -I INPUT -p icmp –icmp-type Echo-Reply -j ACCEPT<br>iptables -I INPUT -p icmp –icmp-type destination-Unreachable -j ACCEPT</p>
</li>
<li><p>禁止转发来自MAC地址 00:0c:29:27:55:3F的主机的数据包<br>iptables -A FORWARD -m mac –mac-source 00:0c:29:27:55:3F -j DROP</p>
</li>
<li><p>允许防火墙主机对外开放TCP端口20、21、25、110以及被动模式FTP端口1250-1280<br>iptables -A INPUT -p tcp -m multiport –dport 20,21,25,110,1250:1280 -j ACCEPT</p>
</li>
<li><p>禁止转发源地址IP为192.168.1.20-192.168.1.99的TCP数据包<br>iptables -A FORWARD -p tcp -m iprange –src-range 192.168.1.20-192.168.1.99 -j DROP</p>
</li>
<li><p>禁止转发与正常TCP连接无关的非–syn请求数据包<br>iptables -A FORWARD -m state –state NEW -p tcp !–syn -j DROP<br>说明：“-m state”表示数据包的连接状态，“NEW”表示与任何连接无关</p>
</li>
<li><p>拒绝访问防火墙的新数据包，但允许响应连接或与已有连接相关的数据包<br>iptables -A INPUT -p tcp -m state –state NEW -j DROP<br>iptables -A INPUT -p tcp -m state –state ESTABLISHED,RELATED -j ACCEPT</p>
</li>
<li><p>只开放本机的Web服务（80）、FTP（20、21、20450-20480），放行外部主机发往服务器其他端口的应答数据包，将其他入栈数据包均予以丢弃处理<br>iptables -I INPUT -p tcp -m multiport –dport 20,21,80 -j ACCEPT<br>iptables -I INPUT -p tcp –dport 20450-20480 -j ACCEPT<br>iptables -I INPUT -p tcp -m state –state ESTABLISHED -j ACCEPT<br>iptables -P INPUT DROP</p>
</li>
</ol>
<h3 id="常用iptables规则"><a href="#常用iptables规则" class="headerlink" title="常用iptables规则"></a>常用iptables规则</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 1. 删除所有现有规则</span><br><span class="line"></span><br><span class="line">iptables -F</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 2. 设置默认的 chain 策略</span><br><span class="line"></span><br><span class="line">iptables -P INPUT DROP</span><br><span class="line"></span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line"></span><br><span class="line">iptables -P OUTPUT DROP</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 3. 阻止某个特定的 IP 地址</span><br><span class="line"></span><br><span class="line">#BLOCK_THIS_IP=&quot;x.x.x.x&quot;</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -s &quot;$BLOCK_THIS_IP&quot; -j DROP</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 4. 允许全部进来的（incoming）SSH</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -i eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 5. 只允许某个特定网络进来的 SSH</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp -s 192.168.200.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 6. 允许进来的（incoming）HTTP</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -i eth0 -p tcp --dport 80 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -o eth0 -p tcp --sport 80 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 7. 多端口（允许进来的 SSH、HTTP 和 HTTPS）</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -i eth0 -p tcp -m multiport --dports 22,80,443 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -o eth0 -p tcp -m multiport --sports 22,80,443 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 8. 允许出去的（outgoing）SSH</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -o eth0 -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 9. 允许外出的（outgoing）SSH，但仅访问某个特定的网络</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp -d 192.168.101.0/24 --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp --sport 22 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 10. 允许外出的（outgoing） HTTPS</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -o eth0 -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -i eth0 -p tcp --sport 443 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 11. 对进来的 HTTPS 流量做负载均衡</span><br><span class="line"></span><br><span class="line">#iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 0 -j DNAT --to-destination 192.168.1.101:443</span><br><span class="line"></span><br><span class="line">#iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 1 -j DNAT --to-destination 192.168.1.102:443</span><br><span class="line"></span><br><span class="line">#iptables -A PREROUTING -i eth0 -p tcp --dport 443 -m state --state NEW -m nth --counter 0 --every 3 --packet 2 -j DNAT --to-destination 192.168.1.103:443</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 12. 从内部向外部 Ping</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -p icmp --icmp-type echo-request -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 13. 从外部向内部 Ping</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 14. 允许环回（loopback）访问</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -i lo -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -o lo -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 15. 允许 packets 从内网访问外网</span><br><span class="line"></span><br><span class="line"># if eth1 is connected to external network (internet)</span><br><span class="line"></span><br><span class="line"># if eth0 is connected to internal network (192.168.1.x)</span><br><span class="line"></span><br><span class="line">iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 16. 允许外出的  DNS</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -p udp -o eth0 --dport 53 -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -p udp -i eth0 --sport 53 -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 17. 允许 NIS 连接</span><br><span class="line"></span><br><span class="line"># rpcinfo -p | grep ypbind ; This port is 853 and 850</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -p tcp --dport 111 -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -p udp --dport 111 -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -p tcp --dport 853 -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -p udp --dport 853 -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -p tcp --dport 850 -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -p udp --dport 850 -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 18. 允许某个特定网络 rsync 进入本机</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp -s 192.168.101.0/24 --dport 873 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 873 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 19. 仅允许来自某个特定网络的 MySQL 的链接</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp -s 192.168.200.0/24 --dport 3306 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 3306 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 20. 允许 Sendmail 或 Postfix</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -i eth0 -p tcp --dport 25 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -o eth0 -p tcp --sport 25 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 21. 允许 IMAP 和 IMAPS</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp --dport 143 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 143 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp --dport 993 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 993 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 22. 允许 POP3 和 POP3S</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp --dport 110 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 110 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp --dport 995 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 995 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 23. 防止 DoS 攻击</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -p tcp --dport 80 -m limit --limit 25/minute --limit-burst 100 -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 24. 设置 422 端口转发到 22 端口</span><br><span class="line"></span><br><span class="line">#iptables -t nat -A PREROUTING -p tcp -d 192.168.102.37 --dport 422 -j DNAT --to 192.168.102.37:22</span><br><span class="line"></span><br><span class="line">#iptables -A INPUT -i eth0 -p tcp --dport 422 -m state --state NEW,ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line">#iptables -A OUTPUT -o eth0 -p tcp --sport 422 -m state --state ESTABLISHED -j ACCEPT</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># 25. 为丢弃的包做日志（Log）</span><br><span class="line"></span><br><span class="line">iptables -N LOGGING</span><br><span class="line"></span><br><span class="line">iptables -A INPUT -j LOGGING</span><br><span class="line"></span><br><span class="line">iptables -A LOGGING -m limit --limit 2/min -j LOG --log-prefix &quot;IPTables Packet Dropped: &quot; --log-level 7</span><br><span class="line"></span><br><span class="line">iptables -A LOGGING -j DROP</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>linux</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title>细说ITIL的五个服务支持流程</title>
    <url>/2021/04/11/itil-5/</url>
    <content><![CDATA[<p>随着企业规模的扩张，企业IT系统正变得越来越复杂，其管理难度也在逐步增加。自信到企业业务发展以来，经过20多年的发展，从早期的OA、CRM到后来的ERP，再到今天DCS等系统，企业信息化进程一再深入，业务自动化程度大幅提高，极大的提升了企业运转效率。而作为一系列业务系统的支撑，企业对IT系统的管理确不够重视，长久以来，企业管理者“重建设，轻运维”，“重技术，轻管理”的思维导致了IT系统与业务系统的长期脱节，当业务系统变得越来越复杂，累赘到IT系统已经无法灵活适应企业业务的调整需求。同时，由于企业的IT系统建设没有一个清洗的规划，也导致企业IT运维成本居高不下，“救火式”的人工IT运维普遍存在。这种低效的IT运维模式不断严重影响企业业务运转效率，也降低了企业竞争力，已无法适应新时代企业的发展需求。企业需要更高效的IT运维管理系统来支撑企业的发展。</p>
<span id="more"></span>
<p>ITIL即IT基础架构库作为IT运维管理的标准库，自上世纪80年代末制定以来，已经发展到了第三个版本，改标准旨在于通过对企业流程进行梳理提升企业IT资源的利用率和服务质量。ITIL和ITSM之间的区别在于，ITIL只是在告诉我们什么该做，并没有给出具体办法，而ITSM则可以理解为一个行业的IT管理方法论，通过一套协同运作流程帮IT部门以合理的成本提供高质量的IT服务。</p>
]]></content>
  </entry>
  <entry>
    <title>kubeadm 常用命令行选项</title>
    <url>/2020/04/08/kubeadm-command/</url>
    <content><![CDATA[<h1 id="kubeadm-init"><a href="#kubeadm-init" class="headerlink" title="kubeadm init"></a>kubeadm init</h1><ul>
<li><p><a href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/">https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/</a></p>
<p>  此命令初始化一个 Kubernetes 控制节点。</p>
</li>
</ul>
<h2 id="“init”命令执行以下阶段"><a href="#“init”命令执行以下阶段" class="headerlink" title="“init”命令执行以下阶段"></a>“init”命令执行以下阶段</h2><ul>
<li>preflight Run pre-flight checks</li>
<li>kubelet-start Write kubelet settings and （re）start the kubelet</li>
<li>certs Certificate generation<ul>
<li>&#x2F;ca Generate the self-signed Kubernetes CA to provision identities for other Kubernetes components</li>
<li>&#x2F;apiserver Generate the certificate for serving the Kubernetes API</li>
<li>&#x2F;apiserver-kubelet-client Generate the certificate for the API server to connect to kubelet</li>
<li>&#x2F;front-proxy-ca Generate theself-signed CA to provision identities for front proxy</li>
<li>&#x2F;front-proxy-client Generate the certificate for the front proxy client</li>
<li>&#x2F;etcd-ca Generate the self-signed CA to provision identities for etcd</li>
<li>&#x2F;etcd-server Generate the certificate for serving etcd</li>
<li>&#x2F;etcd-peer Generate the certificate for etcd nodes to communicate with each other</li>
<li>&#x2F;etcd-healthcheck-client Generate the certificate for liveness probes to healthcheck etcd</li>
<li>&#x2F;apiserver&#x3D;etcd-client Generate the certificate for apiserver uses to access etcd</li>
</ul>
</li>
<li>kubeconfig Genera all kubeconfig files necessary to establish the control plane and the admin kubeconfig file<ul>
<li>&#x2F;admin Generate a kubeconfig file for the admin to use and for kubeadm itself</li>
<li>&#x2F;kubelet Generate a kubeconfig file for the kubelet to use <em>only</em> forcluster bootstrapping purpose</li>
<li>&#x2F;controller-manager Generate a kubeconfig file for the controller manager to use</li>
<li>&#x2F;scheduler Generate a kubeconfig file for the scheduler to use</li>
</ul>
</li>
<li>control-plane Generate all static Pod manifest files necessary to establish the control plane <ul>
<li>&#x2F;apiserver Generate the kube-apiserver static Pod manifest</li>
<li>&#x2F;controller-manager Generate the kube-controller-manager static pod manifest</li>
<li>&#x2F;scheduler Generate the kube-scheduler static pod manifest</li>
</ul>
</li>
<li>etcd Generate static Pod manifest file for local etcd<ul>
<li>&#x2F;local Generate the static Pod manifest file for a local，single-node local etcd instance</li>
</ul>
</li>
<li>upload-config Upload the kubeadm and kubelet configuration to a Configmap<ul>
<li>&#x2F;kubeadm Upload the kubeadm ClusterConfiguration to a configmap</li>
<li>&#x2F;kubelet Upload the kubelet component config to a configmap</li>
</ul>
</li>
<li>upload-certs Upload certificates to kubeadm-certs</li>
<li>mark-control-plane Mark a node as a control-plane</li>
<li>bootstrap-token Generate bootstrap tokens used to join a node to a cluster</li>
<li>addon Install required addons for passing Conformance tests<ul>
<li>&#x2F;coredns Install the CoreDNS addon to a Kubernetes cluster</li>
<li>&#x2F;kube-proxy Install the kube-proxy addon to a Kubernetes cluster</li>
</ul>
</li>
</ul>
<h2 id="选项"><a href="#选项" class="headerlink" title="选项"></a>选项</h2><ul>
<li>-apiserver-advertise-address string<br>  API服务器所公布的其正在监听的IP地址。如果未设置，则使用默认网络接口</li>
<li>-apiserver-bind-port int32 默认值：6443<br>  API服务器绑定的端口</li>
<li>-apiserver-cert-extra-sans stringSlice<br>  用于API Server 服务器证书的可选附加主题备用名称（SAN）。可以是IP地址和DNS名称</li>
<li>-cert-dir string 默认值：”&#x2F;etcd&#x2F;kubernetes&#x2F;pki”<br>  保存和存储证书的路径</li>
<li>-certificate-key string<br>  用于加密kubeadm-certs Secret 中的控制平面证书的密钥</li>
<li>-config string<br>  kubeadm 配置文件的路径</li>
<li>-control-plane-endpoint string<br>  为控制平面指定一个稳定的IP地址或DNS名称</li>
<li>-cri-socket string<br>  要连接的CRI套接字的路径。如果为空，则kubeadm将尝试自动监测此值；仅当安装了多个CRI或具有非标准CRI插槽时，才使用此选项</li>
<li>-dry-run<br>  不做任何更改，只输出将要执行的操作</li>
<li>-k，–experimental-kustomize string<br>  用于存储kustomize为静态pod清单所提供的补丁的路径</li>
<li>-feature-gates string<br>  一组用来描述各种功能特性的键值（key&#x3D;value）对。选项是：IPv6DualStack&#x3D;true|false (ALPHA - default&#x3D;false)</li>
<li>-h，–help<br>  init操作的帮助命令</li>
<li>-ignore-preflight-errors stringSlice<br>  错误将显示为警告的检查列表；例如：IsPrivilegedUser,Swap。取值 all 时将忽略检查中的所有错误</li>
<li>–image-repository string 默认值：”k8s.gcr.io”<br>  选择用于拉取控制平面镜像的默认仓库</li>
<li>–kubernetes-version string<br>  为控制平面选择一个特定的 kubernetes 版本</li>
<li>–node-name string<br>  指定节点的名称</li>
<li>–pod-network-cidr string<br>  指明pod网络可以使用的IP地址段。如果设置了这个参数，控制平面将会为没一个节点自动分配CIDRs</li>
<li>–service-cidr string<br>  为服务的虚拟IP地址指定另外的IP地址段</li>
<li>–skip-certificate-key-print<br>  不要打印用于加密控制平面证书的密钥</li>
<li>–skip-phases stringSlice<br>  要跳过的阶段列表</li>
<li>–skip-token-print<br>  要跳过打印 <code>kubeadm init</code> 生成的默认引导令牌</li>
<li>–token string<br>  这个令牌用于建立控制平面与工作节点间的双向通信，格式为 [a-z0-9]{6}.[a-z0-9]{16}</li>
<li>–token-ttl duration<br>  令牌被自动删除之前的持续时间（例如 1s，2m，3h）。如果设置为0，则令牌将永不过期</li>
<li>–upload-certs<br>  将控制平面证书上传到kubeadm-certs Secret</li>
<li>–rootfs string<br>  [实验] 到 ‘真实’ 主机根文件系统的路径，<strong>从父命令继承的选项</strong></li>
</ul>
<h2 id="init命令的工作流程"><a href="#init命令的工作流程" class="headerlink" title="init命令的工作流程"></a>init命令的工作流程</h2><p><code>kubeadm init</code> 命令通过执行下列步骤来启动一个 kubernetes 控制平面节点</p>
<ol>
<li>在做出变更前运行一系列的预检项来验证系统状态。一些检查项目仅仅触发警告，其他的则会被视为错误并且推出kubeadm，除非问题得到解决或者用户指定了<br> –ignore-preflight-error&#x3D;<list-of-error> 参数</li>
<li>生成一个自签名的CA证书（或者使用现有的证书，如果提供的话）来为集群中的每一个组件建立身份标识。如果用户已经通过 <code>--cert-dir</code> 配置的证书目录（默认为 <code>/etc/kubernetes/pki</code>）提供了他们自己的CA证书以及&#x2F;或者密钥，那么将会跳过这个步骤，正如文档 <a href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#custom-certificates">使用自定义证书</a>所述。如果指定了 <code>--apiserver-cert-extra-sans</code> 参数，APIserver的证书将会有额外的SAN条目，如果必要的话，将会被转为小写。</li>
<li>将 kubeconfig 文件写入 <code>/etc/kubernetes</code> 目录以便kubelet、controller-manager和scheduler用来连接到APIServer，它们每一个都有自己的身份标识，同时生成一个名为 <code>admin.conf</code> 的独立kubeconfig 文件，用于管理操作。</li>
<li>为APIServer、controller-manager和scheduler生成静态POD的清单文件（manifest）。假设没有提供一个外部的etcd服务的话，也会为etcd生成一份额外的静态POD清单文件。</li>
</ol>
<p>静态POD的清单文件被写入到 <code>/etc/kubernetes/manifest</code> 目录；kubelet会监视这个目录以便在系统启动的时候创建pod。</p>
<p>一旦控制平面的POD都运行起来，<code>kubeadm init</code>的工作流程就继续往下执行。</p>
<ol>
<li>对控制平面节点应用labels和taints标记以便不会在它上面运行其他的工作负载。</li>
<li>生成令牌以便其他节点以后可以使用这个令牌向控制平面节点注册他们自己。（可先），用户可以通过 <code>--token</code> 提供一个令牌，正如文档 <img src="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/" alt="kubeadm token">所述。</li>
<li>为了使得节点能够遵照<img src="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/" alt="Bootstrap Tokens"> 和 <img src="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/" alt="LTS Bootstrap"> 这两份文档中描述的机制加入到集群中，kubeadm会执行所有的必要配置： -创建一份configmap提供添加集群节点所需的信息，并未该configmap设置相关的RBAC访问规则。-使得Bootstrap Tokens可以访问CSR签名API。-对新的CSR请求配置为自动签发。</li>
</ol>
<h2 id="在-kubeadm-中使用-init-phases"><a href="#在-kubeadm-中使用-init-phases" class="headerlink" title="在 kubeadm 中使用 init phases"></a>在 kubeadm 中使用 init phases</h2><p>Kubeadm 允许您使用 <code>kubeadm init phases</code> 命令分阶段创建控制平面节点。<br>要查看阶段和子阶段的有序列表，可以调用 <code>kubeadm init --help</code>。该列表将位于帮助屏幕的顶部，每个阶段旁边都有一个描述。注意，通过调用 <code>kubeadm init</code>， 所有阶段和子阶段都将按照此确切顺序执行。<br>某些阶段具有唯一的标志，因此，如果要查看可用选项的列表，请添加 <code>--help</code> ，例如：<br><code>sudo kubead init phase control=plane controller-manager --help</code><br>您也可以使用 <code>--help</code> 查看特定父阶段的子阶段列表：<br><code>sudo kubeadm init phase control-plane --help</code><br><code>kubeadm init</code> 还公开了一个名为 <code>--skip-phases</code> 的参数，该参数可用于跳过某些阶段。参数接受阶段名称列表，并且这些名称可以从上面的有序列表中获取。<br>例如：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo kubeadm init phase control-plane all --config=configfile.yaml</span><br><span class="line">sudo kubeadm init phase etcd local --config=configfile.yaml</span><br><span class="line"># 您现在可以修改控制平面和 etcd 清单文件</span><br><span class="line">sudo kubeadm init --skip-phases=control-plane,etcd --config=configfile.yaml</span><br></pre></td></tr></table></figure>
<p>该示例将执行的操作是基于 configfile.yaml 中的配置在 <code>/etc/kubernetes/manifest</code> 中写入控制平面和etcd的清单文件。这允许您修改文件，然后使用 –skip-phases 跳过这些阶段。通过调用最后一个命令，您将使用自定义清单文件创造一个控制平面节点。</p>
<h2 id="结合一个配置文件来使用kubeadm-init"><a href="#结合一个配置文件来使用kubeadm-init" class="headerlink" title="结合一个配置文件来使用kubeadm init"></a>结合一个配置文件来使用kubeadm init</h2><p>通过一份配置文件而不是使用命令行参数来配置 kubeadm init 命令是可能的，但是一些更加高级的功能只能通过配置文件设定。这份配置文件通过 –config 选项参数指定。<br>可以使用 <code>kubead config print</code> 命令打印出默认配置。<br><strong>推荐</strong>使用 <code>kubeadm config migrate</code> 命令将旧的 v1beta1 版本的配置迁移到 v1beta2 版本。</p>
<h2 id="添加-kube-proxy-参数"><a href="#添加-kube-proxy-参数" class="headerlink" title="添加 kube-proxy 参数"></a>添加 kube-proxy 参数</h2><p>kubeadm配置中有关 kube-proxy 的说明请查看：<img src="https://godoc.org/k8s.io/kubernetes/pkg/proxy/apis/config#KubeProxyConfiguration" alt="-kube-proxy"><br>使用 kubeadm 启用IPVS模式的说明请查看： <img src="https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/ipvs/README.md" alt="-IPVS"></p>
<h2 id="向控制平面组件传递自定义的命令行参数"><a href="#向控制平面组件传递自定义的命令行参数" class="headerlink" title="向控制平面组件传递自定义的命令行参数"></a>向控制平面组件传递自定义的命令行参数</h2><p>有关向控制平面组件传递命令行参数的说明请查看：<img src="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/" alt="控制平面命令行参数"></p>
<h2 id="使用自定义的镜像"><a href="#使用自定义的镜像" class="headerlink" title="使用自定义的镜像"></a>使用自定义的镜像</h2><p>默认情况下，kubeadm会从 k8s.gcr.io 仓库拉取镜像。如果请求的Kubernetes 版本是 CI label（例如 ci&#x2F;latest），则使用 gcr.io&#x2F;kubernetes-ci-images。<br>您可以通过使用<img src="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm-init/#config-file" alt="带有配置文件的kubeadm">来重写此操作。<br>允许的自定义功能有：</p>
<ul>
<li>使用其他的 imageRepository 来代替 k8s.gcr.io</li>
<li>将 useHyperKubeImage 设置为 true，使用 HyperKube 镜像</li>
<li>为etcd或DNS附件提供特定的 imageRepository 和 imageTag<br>请注意配置文件中的配置项 kubernetesVersion 或者命令行参数 –kubernetes-version 会影响到镜像的版本。</li>
</ul>
<h2 id="将控制平面证书上传到集群"><a href="#将控制平面证书上传到集群" class="headerlink" title="将控制平面证书上传到集群"></a>将控制平面证书上传到集群</h2>]]></content>
      <tags>
        <tag>kubeadm</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>kubeadm概述</title>
    <url>/2020/04/07/kubeadm%E6%A6%82%E8%BF%B0/</url>
    <content><![CDATA[<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li><a href="https://www.52wiki.cn/Doc/Read/id/1687.html">https://www.52wiki.cn/Doc/Read/id/1687.html</a></li>
<li><a href="https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/</a></li>
<li><a href="https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/">https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/</a></li>
<li><a href="https://blog.51cto.com/crushlinux/2473003">https://blog.51cto.com/crushlinux/2473003</a></li>
<li><a href="https://feisky.gitbooks.io/kubernetes/content/components/kubeadm.html">https://feisky.gitbooks.io/kubernetes/content/components/kubeadm.html</a></li>
<li><a href="https://github.com/cookeem/kubeadm-ha">https://github.com/cookeem/kubeadm-ha</a></li>
</ul>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>kubeadm 是一个工具，它提供了 <code>kubeadm init</code> 以及 <code>kubeadm join</code> 这两个命令作为快速创建 Kubernetes 集群的最佳实践。</p>
<p>kubeadm 通过执行必要的操作来启动和运行一个最小可用的集群。它被故意设计为只关心启动集群，而不是准备节点环境的工作。同样的，诸如安装各种各样的可有可无的插件，例如 Kubernetes 控制面板、监控解决方案以及特定云提供商的插件，这些都不在它负责的范围。<br>相反，我们期望由一个基于 Kubernetes 从更高层设计的更加合适的工具来做这些事情；并且，理想情况下，使用 kubeadm 作为所有部署的基础将会使得创建一个符合期望的集群变得容易。</p>
<p>换言之，kubeadm 是一个基础设施工具，简化 Kubernetes 集群的部署和创建，而基于 Kubernetes 高层的设计则不在 kubeadm 负责范围之内。</p>
<p>kubeadm 常用命令</p>
<ul>
<li>kubeadm init 启动一个 Kubernetes 主节点（master）</li>
<li>kubeadm join 启动一个 Kubernetes 工作节点并将其加入到集群</li>
<li>kubeadm upgrade 更新一个 Kubernetes 集群到最新版本</li>
<li>kubeadm config 如果你使用 kubeadm v1.7.x 或者更低版本，你需要对你的集群做一些配置以便使用 <code>kubeadm upgrade</code> 命令</li>
<li>kubeadm token 使用 <code>kubeadm join</code>来管理令牌</li>
<li>kubeadm reset 还原之前使用 <code>kubeadm init</code> 或者 <code>kubeadm join</code> 对节点产生的改变</li>
<li>kubeadm version 打印kubeadm版本信息</li>
<li>kubeadm alpha 预览一组可用的新功能以便从社区搜集反馈</li>
</ul>
]]></content>
      <tags>
        <tag>kubeadm</tag>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title>运维思考</title>
    <url>/2020/02/20/opsthink/</url>
    <content><![CDATA[<h1 id="运维管理思考"><a href="#运维管理思考" class="headerlink" title="运维管理思考"></a>运维管理思考</h1><h2 id="技术管理"><a href="#技术管理" class="headerlink" title="技术管理"></a>技术管理</h2>]]></content>
      <tags>
        <tag>ops</tag>
      </tags>
  </entry>
  <entry>
    <title>supervisor</title>
    <url>/2020/03/21/supervisor/</url>
    <content><![CDATA[<h1 id="supervisor介绍"><a href="#supervisor介绍" class="headerlink" title="supervisor介绍"></a>supervisor介绍</h1><p>进程守护</p>
<h1 id="supervisor组件"><a href="#supervisor组件" class="headerlink" title="supervisor组件"></a>supervisor组件</h1><h2 id="supervisord"><a href="#supervisord" class="headerlink" title="supervisord"></a>supervisord</h2><p>supervisord是supervisor的服务端程序：启动supervisor程序，启动supervisor管理的子进程，响应来自client的请求，重启闪退或异常退出的子进程，把子进程的stderr或stdout记录到日志文件中，生成和处理event。</p>
<h2 id="supervisorctl"><a href="#supervisorctl" class="headerlink" title="supervisorctl"></a>supervisorctl</h2><p>supervisorctl是客户端程序，有一个类似shell的命令，可以查看子进程状态，启动、停止、重启子进程，获取running子进程的列表等。supervisorctl不仅可以连接到本机的supervd，还可以连接远程的supervisord，本机通过UNIX socke连接，远程通过TCP socket连接。supervisorctl和supervd之间的通信是通过XML_RPC完成的</p>
<h2 id="Web-Server"><a href="#Web-Server" class="headerlink" title="Web Server"></a>Web Server</h2><p>Web Server可以在界面上可以进城，Web Server是通过XML_RPC来实现的。</p>
<h2 id="XML-RPC接口"><a href="#XML-RPC接口" class="headerlink" title="XML_RPC接口"></a>XML_RPC接口</h2><p>用于远程调用的接口</p>
<h1 id="配置文件详解"><a href="#配置文件详解" class="headerlink" title="配置文件详解"></a>配置文件详解</h1><h2 id="unix-http-server"><a href="#unix-http-server" class="headerlink" title="[unix_http_server]"></a>[unix_http_server]</h2><table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>file=/tmp/supervisor.sock</td>
<td>socket文件的路径，supervisorctl用XML_RPC和supervisord通信就是通过它进行的。如果不设置的话，supervisorctl也就不能用了不设置的话，默认为none。 非必须设置</td>
</tr>
<tr>
<td>;chmod=0700</td>
<td>这个简单，就是修改上面的那个socket文件的权限为0700不设置的话，默认为0700。 非必须设置</td>
</tr>
<tr>
<td>;chown=nobody:nogroup</td>
<td>这个一样，修改上面的那个socket文件的属组为user.group不设置的话，默认为启动supervisord进程的用户及属组。非必须设置</td>
</tr>
<tr>
<td>;username=user</td>
<td>使用supervisorctl连接的时候，认证的用户不设置的话，默认为不需要用户。 非必须设置</td>
</tr>
<tr>
<td>;password=123</td>
<td>和上面的用户名对应的密码，可以直接使用明码，也可以使用SHA加密如：{SHA}82ab876d1387bfafe46cc1c8a2ef074eae50cb1d默认不设置。非必须设置</td>
</tr>
</tbody>
</table>

<h2 id="inet-http-server"><a href="#inet-http-server" class="headerlink" title="[inet_http_server]"></a>[inet_http_server]</h2><table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>;port=127.0.0.1:9001</td>
<td>这个是侦听的IP和端口，侦听所有IP用 :9001或*:9001。 这个必须设置，只要上面的[inet_http_server]开启了，就必须设置它</td>
</tr>
<tr>
<td>;username=user</td>
<td>这个和上面的uinx_http_server一个样。非必须设置</td>
</tr>
<tr>
<td>;password=123</td>
<td>这个也一个样。非必须设置</td>
</tr>
</tbody>
</table>

<h2 id="supervisord-1"><a href="#supervisord-1" class="headerlink" title="[supervisord]"></a>[supervisord]</h2><table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>logfile=/tmp/supervisord.log</td>
<td>这个是supervisord这个主进程的日志路径，注意和子进程的日志不搭嘎。默认路径$CWD/supervisord.log，$CWD是当前目录。。非必须设置</td>
</tr>
<tr>
<td>logfile_maxbytes=50MB</td>
<td>这个是上面那个日志文件的最大的大小，当超过50M的时候，会生成一个新的日志文件。当设置为0时，表示不限制文件大小默认值是50M，非必须设置。</td>
</tr>
<tr>
<td>logfile_backups=10</td>
<td>日志文件保持的数量，上面的日志文件大于50M时，就会生成一个新文件。文件数量大于10时，最初的老文件被新文件覆盖，文件数量将保持为10当设置为0时，表示不限制文件的数量。默认情况下为10。。。非必须设置</td>
</tr>
<tr>
<td>loglevel=info</td>
<td>日志级别，有critical, error, warn, info, debug, trace, or blather等。默认为info。。。非必须设置项</td>
</tr>
<tr>
<td>pidfile=/tmp/supervisord.pid</td>
<td>supervisord的pid文件路径。默认为$CWD/supervisord.pid。。。非必须设置</td>
</tr>
<tr>
<td>nodaemon=false</td>
<td>如果是true，supervisord进程将在前台运行。默认为false，也就是后台以守护进程运行。。。非必须设置</td>
</tr>
<tr>
<td>minfds=1024</td>
<td>这个是最少系统空闲的文件描述符，低于这个值supervisor将不会启动。系统的文件描述符在这里设置cat /proc/sys/fs/file-max。默认情况下为1024。。。非必须设置</td>
</tr>
<tr>
<td>minprocs=200</td>
<td>最小可用的进程描述符，低于这个值supervisor也将不会正常启动。ulimit  -u这个命令，可以查看linux下面用户的最大进程数。默认为200。。。非必须设置</td>
</tr>
<tr>
<td>;umask=022</td>
<td>进程创建文件的掩码。默认为022。。非必须设置项</td>
</tr>
<tr>
<td>;user=chrism</td>
<td>这个参数可以设置一个非root用户，当我们以root用户启动supervisord之后。我这里面设置的这个用户，也可以对supervisord进行管理。默认情况是不设置。。。非必须设置项</td>
</tr>
<tr>
<td>;identifier=supervisor</td>
<td>这个参数是supervisord的标识符，主要是给XML_RPC用的。当你有多个supervisor的时候，而且想调用XML_RPC统一管理，就需要为每个supervisor设置不同的标识符了。默认是supervisord。。。非必需设置</td>
</tr>
<tr>
<td>;directory=/tmp</td>
<td>这个参数是当supervisord作为守护进程运行的时候，设置这个参数的话，启动supervisord进程之前，会先切换到这个目录。默认不设置。。。非必须设置</td>
</tr>
<tr>
<td>;nocleanup=true</td>
<td>这个参数当为false的时候，会在supervisord进程启动的时候，把以前子进程产生的日志文件(路径为AUTO的情况下)清除掉。有时候咱们想要看历史日志，当然不想日志被清除了。所以可以设置为true。默认是false，有调试需求的同学可以设置为true。。。非必须设置</td>
</tr>
<tr>
<td>;childlogdir=/tmp</td>
<td>当子进程日志路径为AUTO的时候，子进程日志文件的存放路径。默认路径是这个东西，执行下面的这个命令看看就OK了，处理的东西就默认路径python -c &quot;import tempfile;print tempfile.gettempdir()&quot;。非必须设置</td>
</tr>
<tr>
<td>;environment=KEY=&quot;value&quot;</td>
<td>这个是用来设置环境变量的，supervisord在linux中启动默认继承了linux的环境变量，在这里可以设置supervisord进程特有的其他环境变量。supervisord启动子进程时，子进程会拷贝父进程的内存空间内容。 所以设置的这些环境变量也会被子进程继承。小例子：environment=name=&quot;haha&quot;,age=&quot;hehe&quot;。默认为不设置。。。非必须设置</td>
</tr>
<tr>
<td>;strip_ansi=false</td>
<td>这个选项如果设置为true，会清除子进程日志中的所有ANSI 序列。什么是ANSI序列呢？就是我们的\n,\t这些东西。默认为false。。。非必须设置</td>
</tr>
</tbody>
</table>

<h2 id="rpcinterface-supervisor"><a href="#rpcinterface-supervisor" class="headerlink" title="[rpcinterface:supervisor]"></a>[rpcinterface:supervisor]</h2><table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>serverurl=unix:///tmp/supervisor.sock</td>
<td>这个是supervisorctl本地连接supervisord的时候，本地UNIX socket路径，注意这个是和前面的[unix_http_server]对应的默认值就是unix:///tmp/supervisor.sock 非必须设置</td>
</tr>
<tr>
<td>;serverurl=http://127.0.0.1:9001</td>
<td>这个是supervisorctl远程连接supervisord的时候，用到的TCP socket路径注意这个和前面的[inet_http_server]对应。默认就是http://127.0.0.1:9001 非必须项</td>
</tr>
<tr>
<td>;username=chris</td>
<td>用户名默认空。非必须设置</td>
</tr>
<tr>
<td>;password=123</td>
<td>密码默认空。。非必须设置</td>
</tr>
<tr>
<td>;prompt=mysupervisor</td>
<td>输入用户名密码时候的提示符默认supervisor。。非必须设置</td>
</tr>
<tr>
<td>;history_file=~/.sc_history</td>
<td>这个参数和shell中的history类似，我们可以用上下键来查找前面执行过的命令。默认是no file的。。所以我们想要有这种功能，必须指定一个文件。。。非必须设置</td>
</tr>
</tbody>
</table>

<h2 id="program-theprogramname"><a href="#program-theprogramname" class="headerlink" title="[program:theprogramname]"></a>[program:theprogramname]</h2><table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>;command=/bin/cat</td>
<td>这个就是我们的要启动进程的命令路径了，可以带参数例子：/home/test.py -a 'hehe'有一点需要注意的是，我们的command只能是那种在终端运行的进程，不能是守护进程。这个想想也知道了，比如说command=service httpd start。httpd这个进程被linux的service管理了，我们的supervisor再去启动这个命令。这已经不是严格意义的子进程了。这个是个必须设置的项</td>
</tr>
<tr>
<td>;process_name=%(program_name)s</td>
<td>这个是进程名，如果我们下面的numprocs参数为1的话，就不用管这个参数了，它默认值%(program_name)s也就是上面的那个program冒号后面的名字，但是如果numprocs为多个的话，那就不能这么干了。想想也知道，不可能每个进程都用同一个进程名吧。</td>
</tr>
<tr>
<td>;numprocs=1</td>
<td>启动进程的数目。当不为1时，就是进程池的概念，注意process_name的设置。默认为1    。。非必须设置</td>
</tr>
<tr>
<td>;directory=/tmp</td>
<td>进程运行前，会前切换到这个目录。默认不设置。。。非必须设置</td>
</tr>
<tr>
<td>;umask=022</td>
<td>进程掩码，默认none，非必须</td>
</tr>
<tr>
<td>;priority=999</td>
<td>子进程启动关闭优先级，优先级低的，最先启动，关闭的时候最后关闭。默认值为999。非必须设置</td>
</tr>
<tr>
<td>;autostart=true</td>
<td>如果是true的话，子进程将在supervisord启动后被自动启动。默认就是true。非必须设置</td>
</tr>
<tr>
<td>;autorestart=unexpected</td>
<td>这个是设置子进程挂掉后自动重启的情况，有三个选项，false,unexpected 和true。如果为false的时候，无论什么情况下，都不会被重新启动，如果为unexpected，只有当进程的退出码不在下面的exitcodes里面定义的退出码的时候，才会被自动重启。当为true的时候，只要子进程挂掉，将会被无条件的重启</td>
</tr>
<tr>
<td>;startsecs=1</td>
<td>这个选项是子进程启动多少秒之后，此时状态如果是running，则我们认为启动成功了。默认值为1 。。非必须设置</td>
</tr>
<tr>
<td>;startretries=3</td>
<td>当进程启动失败后，最大尝试启动的次数。。当超过3次后，supervisor将把此进程的状态置为FAIL。默认值为3 。。非必须设置</td>
</tr>
<tr>
<td>;exitcodes=0,2</td>
<td>注意和上面的的autorestart=unexpected对应。。exitcodes里面的定义的退出码是expected的。</td>
</tr>
<tr>
<td>;stopsignal=QUIT</td>
<td>进程停止信号，可以为TERM, HUP, INT, QUIT, KILL, USR1, or USR2等信号默认为TERM 。。当用设定的信号去干掉进程，退出码会被认为是expected。非必须设置</td>
</tr>
<tr>
<td>;stopwaitsecs=10</td>
<td>这个是当我们向子进程发送stopsignal信号后，到系统返回信息给supervisord，所等待的最大时间。 超过这个时间，supervisord会向该子进程发送一个强制kill的信号。默认为10秒。。非必须设置</td>
</tr>
<tr>
<td>;stopasgroup=false</td>
<td>这个东西主要用于，supervisord管理的子进程，这个子进程本身还有子进程。那么我们如果仅仅干掉supervisord的子进程的话，子进程的子进程有可能会变成孤儿进程。所以咱们可以设置可个选项，把整个该子进程的整个进程组都干掉。 设置为true的话，一般killasgroup也会被设置为true。需要注意的是，该选项发送的是stop信号。默认为false。。非必须设置。。</td>
</tr>
<tr>
<td>;killasgroup=false</td>
<td>这个和上面的stopasgroup类似，不过发送的是kill信号</td>
</tr>
<tr>
<td>;user=chrism</td>
<td>如果supervisord是root启动，我们在这里设置这个非root用户，可以用来管理该program。默认不设置。。。非必须设置项</td>
</tr>
<tr>
<td>;redirect_stderr=true</td>
<td>如果为true，则stderr的日志会被写入stdout日志文件中默认为false，非必须设置</td>
</tr>
<tr>
<td>;stdout_logfile=/a/path</td>
<td>子进程的stdout的日志路径，可以指定路径，AUTO，none等三个选项。设置为none的话，将没有日志产生。设置为AUTO的话，将随机找一个地方生成日志文件，而且当supervisord重新启动的时候，以前的日志文件会被清空。当 redirect_stderr=true的时候，sterr也会写进这个日志文件</td>
</tr>
<tr>
<td>;stdout_logfile_maxbytes=1MB</td>
<td>日志文件最大大小，和[supervisord]中定义的一样。默认为50</td>
</tr>
<tr>
<td>;stdout_logfile_backups=10</td>
<td>和[supervisord]定义的一样。默认10</td>
</tr>
<tr>
<td>;stdout_capture_maxbytes=1MB</td>
<td>这个东西是设定capture管道的大小，当值不为0的时候，子进程可以从stdout发送信息，而supervisor可以根据信息，发送相应的event。默认为0，为0的时候表达关闭管道。。。非必须项</td>
</tr>
<tr>
<td>;stdout_events_enabled=false</td>
<td>当设置为ture的时候，当子进程由stdout向文件描述符中写日志的时候，将触发supervisord发送PROCESS_LOG_STDOUT类型的event。默认为false。。。非必须设置</td>
</tr>
<tr>
<td>;stderr_logfile=/a/path</td>
<td>这个东西是设置stderr写的日志路径，当redirect_stderr=true。这个就不用设置了，设置了也是白搭。因为它会被写入stdout_logfile的同一个文件中。默认为AUTO，也就是随便找个地存，supervisord重启被清空。。非必须设置</td>
</tr>
<tr>
<td>;stderr_logfile_maxbytes=1MB</td>
<td>这个出现好几次了，就不重复了</td>
</tr>
<tr>
<td>;stderr_logfile_backups=10</td>
<td>这个也是</td>
</tr>
<tr>
<td>;stderr_capture_maxbytes=1MB</td>
<td>这个一样，和stdout_capture一样。 默认为0，关闭状态</td>
</tr>
<tr>
<td>;stderr_events_enabled=false</td>
<td>这个也是一样，默认为false</td>
</tr>
<tr>
<td>;environment=A=&quot;1&quot;,B=&quot;2&quot;</td>
<td>这个是该子进程的环境变量，和别的子进程是不共享的</td>
</tr>
<tr>
<td>;serverurl=AUTO</td>
<td></td>
</tr>
</tbody>
</table>

<h2 id="eventlistener-theeventlistenername"><a href="#eventlistener-theeventlistenername" class="headerlink" title="[eventlistener:theeventlistenername]"></a>[eventlistener:theeventlistenername]</h2><table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>;command=/bin/eventlistener</td>
<td>这个和上面的program一样，表示listener的可执行文件的路径</td>
</tr>
<tr>
<td>;process_name=%(program_name)s</td>
<td>这个也一样，进程名，当下面的numprocs为多个的时候，才需要。否则默认就OK了</td>
</tr>
<tr>
<td>;numprocs=1</td>
<td>相同的listener启动的个数</td>
</tr>
<tr>
<td>;events=EVENT</td>
<td>event事件的类型，也就是说，只有写在这个地方的事件类型。才会被发送</td>
</tr>
<tr>
<td>;buffer_size=10</td>
<td>这个是event队列缓存大小，单位不太清楚，楼主猜测应该是个吧。当buffer超过10的时候，最旧的event将会被清除，并把新的event放进去。默认值为10。。非必须选项</td>
</tr>
<tr>
<td>;directory=/tmp</td>
<td>进程执行前，会切换到这个目录下执行默认为不切换。。。非必须</td>
</tr>
<tr>
<td>;umask=022</td>
<td>淹没，默认为none，不说了</td>
</tr>
<tr>
<td>;priority=-1</td>
<td>启动优先级，默认-1，也不扯了</td>
</tr>
<tr>
<td>;autostart=true</td>
<td>是否随supervisord启动一起启动，默认true</td>
</tr>
<tr>
<td>;autorestart=unexpected</td>
<td>是否自动重启，和program一个样，分true,false,unexpected等，注意unexpected和exitcodes的关系</td>
</tr>
<tr>
<td>;startsecs=1</td>
<td>也是一样，进程启动后跑了几秒钟，才被认定为成功启动，默认1</td>
</tr>
<tr>
<td>;startretries=3</td>
<td>失败最大尝试次数，默认3</td>
</tr>
<tr>
<td>;exitcodes=0,2</td>
<td>期望或者说预料中的进程退出码，</td>
</tr>
<tr>
<td>;stopsignal=QUIT</td>
<td>干掉进程的信号，默认为TERM，比如设置为QUIT，那么如果QUIT来干这个进程那么会被认为是正常维护，退出码也被认为是expected中的</td>
</tr>
<tr>
<td>;stopwaitsecs=10</td>
<td>max num secs to wait b4 SIGKILL (default 10)</td>
</tr>
<tr>
<td>;stopasgroup=false</td>
<td>send stop signal to the UNIX process group (default false)</td>
</tr>
<tr>
<td>;killasgroup=false</td>
<td>SIGKILL the UNIX process group (def false)</td>
</tr>
<tr>
<td>;user=chrism</td>
<td>设置普通用户，可以用来管理该listener进程。默认为空。。非必须设置</td>
</tr>
<tr>
<td>;redirect_stderr=true</td>
<td>为true的话，stderr的log会并入stdout的log里面。默认为false。。。非必须设置</td>
</tr>
<tr>
<td>;stdout_logfile=/a/path</td>
<td>这个不说了，好几遍了</td>
</tr>
<tr>
<td>;stdout_logfile_maxbytes=1MB</td>
<td>这个也是</td>
</tr>
<tr>
<td>;stdout_logfile_backups=10</td>
<td>这个也是</td>
</tr>
<tr>
<td>;stdout_events_enabled=false</td>
<td>这个其实是错的，listener是不能发送event</td>
</tr>
<tr>
<td>;stderr_logfile=/a/path</td>
<td>这个也是</td>
</tr>
<tr>
<td>;stderr_logfile_maxbytes=1MB</td>
<td>这个也是</td>
</tr>
<tr>
<td>;stderr_logfile_backups</td>
<td>这个不说了</td>
</tr>
<tr>
<td>;stderr_events_enabled=false</td>
<td>这个也是错的，listener不能发送event</td>
</tr>
<tr>
<td>;environment=A=&quot;1&quot;,B=&quot;2&quot;</td>
<td>这个是该子进程的环境变量。默认为空。。。非必须设置</td>
</tr>
<tr>
<td>;serverurl=AUTO</td>
<td>override serverurl computation (childutils)</td>
</tr>
</tbody>
</table>

<h2 id="group-thegroupname"><a href="#group-thegroupname" class="headerlink" title="[group:thegroupname]"></a>[group:thegroupname]</h2><p>这个东西就是给programs分组，划分到组里面的program。我们就不用一个一个去操作了，我们可以对组名进行统一的操作。 注意：program被划分到组里面之后，就相当于原来的配置从supervisor的配置文件里消失了。。。supervisor只会对组进行管理，而不再会对组里面的单个program进行管理了</p>
<table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>;programs=progname1,progname2</td>
<td>组成员，用逗号分开。这个是个必须的设置项</td>
</tr>
<tr>
<td>;priority=999</td>
<td>优先级，相对于组和组之间说的默认999。。非必须选项</td>
</tr>
</tbody>
</table>

<h2 id="include"><a href="#include" class="headerlink" title="[include]"></a>[include]</h2><p>这个东西挺有用的，当我们要管理的进程很多的时候，写在一个文件里面就有点大了。我们可以把配置信息写到多个文件中，然后include过来</p>
<table>
<thead>
<tr>
<th>参数选项</th>
<th>中文白话</th>
</tr>
</thead>
<tbody>
<tr>
<td>;files = relative/directory/*.ini</td>
<td></td>
</tr>
</tbody>
</table>

<h1 id="配置文件实例"><a href="#配置文件实例" class="headerlink" title="配置文件实例"></a>配置文件实例</h1><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[program:wfpconfigsystem]</span><br><span class="line">environment=PATH=&quot;/data/java/jdk1.7.0_79/bin:%(ENV_PATH)s&quot;</span><br><span class="line">command=java -jar -server -Xms5G -Xmx5G -XX:MaxPermSize=2G -XX:+AggressiveOpts -XX:MaxDirectMemorySize=2G -Xss256k -XX:+AggressiveOpts -XX:+UseBiasedLocking -XX:+UseFastAccessorMethods -XX:+DisableExplicitGC -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=75 /data/wfpconfigsystem/ecloud-wfp-1.0.0.jar NoLoadCache</span><br><span class="line">;process_name=%(program_name)s ; process_name expr (default %(program_name)s)</span><br><span class="line">numprocs=1                    ; number of processes copies to start (def 1)</span><br><span class="line">directory=/data/wfpconfigsystem</span><br><span class="line">;umask=022                     ; umask for process (default None)</span><br><span class="line">;priority=999                  ; the relative start priority (default 999)</span><br><span class="line">autostart=true                ; start at supervisord start (default: true)</span><br><span class="line">autorestart=true</span><br><span class="line">startsecs=1                   ; # of secs prog must stay up to be running (def. 1)</span><br><span class="line">startretries=3                ; max # of serial start failures when starting (default 3)</span><br><span class="line">;autorestart=unexpected        ; when to restart if exited after running (def: unexpected)</span><br><span class="line">;exitcodes=0                   ; &#x27;expected&#x27; exit codes used with autorestart (default 0)</span><br><span class="line">;stopsignal=QUIT               ; signal used to kill process (default TERM)</span><br><span class="line">;stopwaitsecs=10               ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">;stopasgroup=false             ; send stop signal to the UNIX process group (default false)</span><br><span class="line">;killasgroup=false             ; SIGKILL the UNIX process group (def false)</span><br><span class="line">;user=chrism                   ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr=true          ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile=/data/wfpconfigsystem/supervisord.log</span><br><span class="line">;stdout_logfile_maxbytes=1MB   ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">;stdout_logfile_backups=10     ; # of stdout logfile backups (0 means none, default 10)</span><br><span class="line">;stdout_capture_maxbytes=1MB   ; number of bytes in &#x27;capturemode&#x27; (default 0)</span><br><span class="line">;stdout_events_enabled=false   ; emit events on stdout writes (default false)</span><br><span class="line">;stdout_syslog=false           ; send stdout to syslog with process name (default false)</span><br><span class="line">;stderr_logfile=/a/path        ; stderr log path, NONE for none; default AUTO</span><br><span class="line">;stderr_logfile_maxbytes=1MB   ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">;stderr_logfile_backups=10     ; # of stderr logfile backups (0 means none, default 10)</span><br><span class="line">;stderr_capture_maxbytes=1MB   ; number of bytes in &#x27;capturemode&#x27; (default 0)</span><br><span class="line">;stderr_events_enabled=false   ; emit events on stderr writes (default false)</span><br><span class="line">;stderr_syslog=false           ; send stderr to syslog with process name (default false)</span><br><span class="line">;environment=A=&quot;1&quot;,B=&quot;2&quot;       ; process environment additions (def no adds)</span><br><span class="line">;serverurl=AUTO </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[program:whitelist-sync]</span><br><span class="line">environment=JAVA_HOME=&quot;/data/java/jdk1.8.0_144&quot;</span><br><span class="line">command=/data/java/jdk1.8.0_144/jre/bin/java -jar /data/whitelist-sync/sync-whitelist-0.0.1-SNAPSHOT.jar</span><br><span class="line">;process_name=%(program_name)s ; process_name expr (default %(program_name)s)</span><br><span class="line">numprocs=1                    ; number of processes copies to start (def 1)</span><br><span class="line">directory=/data/whitelist-sync</span><br><span class="line">;umask=022                     ; umask for process (default None)</span><br><span class="line">;priority=999                  ; the relative start priority (default 999)</span><br><span class="line">autostart=true                ; start at supervisord start (default: true)</span><br><span class="line">autorestart=true</span><br><span class="line">startsecs=1                   ; # of secs prog must stay up to be running (def. 1)</span><br><span class="line">startretries=3                ; max # of serial start failures when starting (default 3)</span><br><span class="line">;autorestart=unexpected        ; when to restart if exited after running (def: unexpected)</span><br><span class="line">;exitcodes=0                   ; &#x27;expected&#x27; exit codes used with autorestart (default 0)</span><br><span class="line">;stopsignal=QUIT               ; signal used to kill process (default TERM)</span><br><span class="line">;stopwaitsecs=10               ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">;stopasgroup=false             ; send stop signal to the UNIX process group (default false)</span><br><span class="line">;killasgroup=false             ; SIGKILL the UNIX process group (def false)</span><br><span class="line">;user=chrism                   ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr=true          ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile=/data/whitelist-sync/supervisord.log</span><br><span class="line">;stdout_logfile_maxbytes=1MB   ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">;stdout_logfile_backups=10     ; # of stdout logfile backups (0 means none, default 10)</span><br><span class="line">;stdout_capture_maxbytes=1MB   ; number of bytes in &#x27;capturemode&#x27; (default 0)</span><br><span class="line">;stdout_events_enabled=false   ; emit events on stdout writes (default false)</span><br><span class="line">;stdout_syslog=false           ; send stdout to syslog with process name (default false)</span><br><span class="line">;stderr_logfile=/a/path        ; stderr log path, NONE for none; default AUTO</span><br><span class="line">;stderr_logfile_maxbytes=1MB   ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">;stderr_logfile_backups=10     ; # of stderr logfile backups (0 means none, default 10)</span><br><span class="line">;stderr_capture_maxbytes=1MB   ; number of bytes in &#x27;capturemode&#x27; (default 0)</span><br><span class="line">;stderr_events_enabled=false   ; emit events on stderr writes (default false)</span><br><span class="line">;stderr_syslog=false           ; send stderr to syslog with process name (default false)</span><br><span class="line">;environment=A=&quot;1&quot;,B=&quot;2&quot;       ; process environment additions (def no adds)</span><br><span class="line">;serverurl=AUTO                ; override serverurl computation (childutils)</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[program:EdbmigrationFileService]</span><br><span class="line">environment=ASPNETCORE_ENVIRONMENT=Production</span><br><span class="line">environment=DOTNET_PRINT_TELEMETRY_MESSAGE=false</span><br><span class="line">command=/usr/bin/dotnet /data/EdbmigrationFileService/EdbmigrationFileService.dll</span><br><span class="line">;process_name=%(program_name)s ; process_name expr (default %(program_name)s)</span><br><span class="line">numprocs=1                    ; number of processes copies to start (def 1)</span><br><span class="line">directory=/data/EdbmigrationFileService</span><br><span class="line">;umask=022                     ; umask for process (default None)</span><br><span class="line">;priority=999                  ; the relative start priority (default 999)</span><br><span class="line">autostart=true                ; start at supervisord start (default: true)</span><br><span class="line">autorestart=true</span><br><span class="line">startsecs=1                   ; # of secs prog must stay up to be running (def. 1)</span><br><span class="line">startretries=3                ; max # of serial start failures when starting (default 3)</span><br><span class="line">;autorestart=unexpected        ; when to restart if exited after running (def: unexpected)</span><br><span class="line">;exitcodes=0                   ; &#x27;expected&#x27; exit codes used with autorestart (default 0)</span><br><span class="line">;stopsignal=QUIT               ; signal used to kill process (default TERM)</span><br><span class="line">;stopwaitsecs=10               ; max num secs to wait b4 SIGKILL (default 10)</span><br><span class="line">;stopasgroup=false             ; send stop signal to the UNIX process group (default false)</span><br><span class="line">;killasgroup=false             ; SIGKILL the UNIX process group (def false)</span><br><span class="line">;user=chrism                   ; setuid to this UNIX account to run the program</span><br><span class="line">redirect_stderr=true          ; redirect proc stderr to stdout (default false)</span><br><span class="line">stdout_logfile=/data/EdbmigrationFileService/supervisord.log</span><br><span class="line">;stdout_logfile_maxbytes=1MB   ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">;stdout_logfile_backups=10     ; # of stdout logfile backups (0 means none, default 10)</span><br><span class="line">;stdout_capture_maxbytes=1MB   ; number of bytes in &#x27;capturemode&#x27; (default 0)</span><br><span class="line">;stdout_events_enabled=false   ; emit events on stdout writes (default false)</span><br><span class="line">;stdout_syslog=false           ; send stdout to syslog with process name (default false)</span><br><span class="line">;stderr_logfile=/a/path        ; stderr log path, NONE for none; default AUTO</span><br><span class="line">;stderr_logfile_maxbytes=1MB   ; max # logfile bytes b4 rotation (default 50MB)</span><br><span class="line">;stderr_logfile_backups=10     ; # of stderr logfile backups (0 means none, default 10)</span><br><span class="line">;stderr_capture_maxbytes=1MB   ; number of bytes in &#x27;capturemode&#x27; (default 0)</span><br><span class="line">;stderr_events_enabled=false   ; emit events on stderr writes (default false)</span><br><span class="line">;stderr_syslog=false           ; send stderr to syslog with process name (default false)</span><br><span class="line">;environment=A=&quot;1&quot;,B=&quot;2&quot;       ; process environment additions (def no adds)</span><br><span class="line">;serverurl=AUTO                ; override serverurl computation (childutils)</span><br></pre></td></tr></table></figure>

<h1 id="service文件"><a href="#service文件" class="headerlink" title="service文件"></a>service文件</h1><p>&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;supervisord.service</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=Supervisord Daemon</span><br><span class="line">After=network.target</span><br><span class="line">  </span><br><span class="line">[Service]</span><br><span class="line">Type=forking</span><br><span class="line">ExecStart=/usr/bin/supervisord -c /etc/supervisord/supervisord.conf</span><br><span class="line">ExecStop=/usr/bin/supervisorctl -c /etc/supervisord/supervisord.conf shutdown</span><br><span class="line">ExecReload=/usr/bin/supervisorctl -c /etc/supervisord/supervisord.conf reload</span><br><span class="line">killMode=process</span><br><span class="line">Restart=on-failure</span><br><span class="line">RestartSec=5s</span><br><span class="line">  </span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br></pre></td></tr></table></figure>

<h1 id="supervisor集群管理开源项目"><a href="#supervisor集群管理开源项目" class="headerlink" title="supervisor集群管理开源项目"></a>supervisor集群管理开源项目</h1><p><a href="https://github.com/Gamegos/cesi">cesi</a></p>
<p>修改supervisord.conf配置文件</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[inet_http_server]</span><br><span class="line">port=*:30000</span><br><span class="line">username=admin</span><br><span class="line">password=Centaur*123</span><br></pre></td></tr></table></figure>

<p>安装cesi</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yum install -y git epel-release</span><br><span class="line">yum install -y python34 python34-pip python34-venv</span><br><span class="line">export CESI_SETUP_PATH=/data/cesi</span><br><span class="line">mkdir -p $&#123;CESI_SETUP_PATH&#125;</span><br><span class="line">cd $&#123;CESI_SETUP_PATH&#125;</span><br><span class="line">wget https://github.com/gamegos/cesi/releases/download/v2.6.8/cesi-extended.tar.gz -O cesi.tar.gz</span><br><span class="line">tar -xvf cesi.tar.gz</span><br><span class="line">python3 -m venv venv</span><br><span class="line">source venv/bin/activate</span><br><span class="line">yum install -y gcc libffi-devel python34-devel</span><br><span class="line">pip3 install -r requirements.txt</span><br><span class="line">python3 $&#123;CESI_SETUP_PATH&#125;/cesi/run.py --config-file $&#123;CESI_SETUP_PATH&#125;/defaults/cesi.conf.toml</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>supervisor</tag>
      </tags>
  </entry>
  <entry>
    <title>技术团队的组织文化</title>
    <url>/2020/03/16/%E6%8A%80%E6%9C%AF%E5%9B%A2%E9%98%9F%E7%9A%84%E7%BB%84%E7%BB%87%E6%96%87%E5%8C%96/</url>
    <content><![CDATA[<p>from: 58到家 刘晓飞</p>
<h1 id="团队文化"><a href="#团队文化" class="headerlink" title="团队文化"></a>团队文化</h1><p>文化建设不分团队大小。<br>团队的是非观，提倡什么、反对什么。<br>人是团队文化的核心。<br><strong>规章制度告诉我们什么不能做，文化倡导我们做什么。两者并不冲突。</strong></p>
<h1 id="怎样构建团队文化"><a href="#怎样构建团队文化" class="headerlink" title="怎样构建团队文化"></a>怎样构建团队文化</h1><ol>
<li>Teamleader要以身作则</li>
<li>要找对的人</li>
<li>强调团队的价值和荣誉感，让员工有自豪感</li>
<li>合理的考核和激励制度</li>
<li>氛围、公平、成长、成就</li>
</ol>
<h1 id="Owner精神"><a href="#Owner精神" class="headerlink" title="Owner精神"></a>Owner精神</h1><ul>
<li>有组织</li>
<li>有思考 – 不仅是完成任务</li>
<li>有目标</li>
<li>有原则</li>
<li>有执行</li>
<li>敢决策</li>
<li>敢担责</li>
</ul>
<h2 id="价值体现"><a href="#价值体现" class="headerlink" title="价值体现"></a>价值体现</h2><ul>
<li>保持团持久的战斗力</li>
<li>保持团队的凝聚力</li>
<li>保持生产团队的自豪感</li>
</ul>
<h2 id="如何培养"><a href="#如何培养" class="headerlink" title="如何培养"></a>如何培养</h2><ul>
<li>树立自身的owner精神</li>
<li>给与充分信任 – 犯错不一定是坏事</li>
<li>给予足够的关怀</li>
<li>重贡献轻成就</li>
<li>多建议少决策</li>
<li>多鼓励少批评 – 要做到帮助性、建设性的批评</li>
<li>分享团队成果 – 团队的荣辱与我相关</li>
<li>贯宣公司战略</li>
</ul>
<h2 id="如何管理owner团队"><a href="#如何管理owner团队" class="headerlink" title="如何管理owner团队"></a>如何管理owner团队</h2><ul>
<li>管理动作要一致</li>
<li>坚决淘汰消极分子</li>
<li>合理的考核制度</li>
<li>不断强调团队的价值</li>
<li>工作上学会放手（不放松）</li>
<li>要做团队的保护伞</li>
</ul>
<h2 id="注意的问题"><a href="#注意的问题" class="headerlink" title="注意的问题"></a>注意的问题</h2><ul>
<li>不要期望每个人都有owner精神</li>
<li>要信任但要有监督</li>
<li>学会放手但不要置身事外</li>
</ul>
]]></content>
      <tags>
        <tag>管理</tag>
      </tags>
  </entry>
  <entry>
    <title>清空收藏夹</title>
    <url>/2020/03/12/%E6%B8%85%E7%A9%BA%E6%94%B6%E8%97%8F%E5%A4%B9/</url>
    <content><![CDATA[<pre><code>“收藏即看过”！
清理收藏夹内容，转移至博客，重新学习一遍！
</code></pre>
<p>测试数据</p>
]]></content>
      <tags>
        <tag>学习</tag>
      </tags>
  </entry>
  <entry>
    <title>纪念我的父亲</title>
    <url>/2020/02/22/%E7%BA%AA%E5%BF%B5%E6%88%91%E7%9A%84%E7%88%B6%E4%BA%B2/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>自动化创建OpenStack镜像研究</title>
    <url>/2021/04/15/%E8%87%AA%E5%8A%A8%E5%8C%96%E5%88%9B%E5%BB%BAOpenStack%E9%95%9C%E5%83%8F%E7%A0%94%E7%A9%B6/</url>
    <content><![CDATA[<p>There are several tools that are designed to automate image creation.</p>
<ul>
<li><a href="https://vault.centos.org/">https://vault.centos.org/</a></li>
<li><a href="http://cloud.centos.org/centos/7/images/">http://cloud.centos.org/centos/7/images/</a></li>
<li><a href="https://docs.openstack.org/image-guide">https://docs.openstack.org/image-guide</a></li>
<li><a href="http://www.chenshake.com/do-a-centos-7-6-mirror-image-of-a/">http://www.chenshake.com/do-a-centos-7-6-mirror-image-of-a/</a><span id="more"></span></li>
</ul>
<h1 id="Diskimage-builder"><a href="#Diskimage-builder" class="headerlink" title="Diskimage-builder"></a>Diskimage-builder</h1>]]></content>
  </entry>
  <entry>
    <title>长连接KeepAlive</title>
    <url>/2020/03/11/%E9%95%BF%E8%BF%9E%E6%8E%A5KeepAlive/</url>
    <content><![CDATA[<p>引用：</p>
<ul>
<li><a href="https://www.cnblogs.com/gotodsp/p/6366163.html">https://www.cnblogs.com/gotodsp/p/6366163.html</a></li>
</ul>
<h1 id="1-HTTP协议和TCP-IP协议的关系"><a href="#1-HTTP协议和TCP-IP协议的关系" class="headerlink" title="1. HTTP协议和TCP&#x2F;IP协议的关系"></a>1. HTTP协议和TCP&#x2F;IP协议的关系</h1><p>HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议，在传输层使用TCP协议，在网络层使用IP协议。IP协议主要解决网络路由和寻址的问题，TCP协议主要解决如何在IP层之上可靠地传递数据包，使得网络上接收端收到发送端所发出的所有包，并且顺序与发送顺序一致。TCP协议是可靠地，面向连接的。</p>
<h1 id="2-如何理解HTTP协议是无状态的"><a href="#2-如何理解HTTP协议是无状态的" class="headerlink" title="2. 如何理解HTTP协议是无状态的"></a>2. 如何理解HTTP协议是无状态的</h1><p>HTTP协议是无状态的，指的是协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。也就是说，打开一个服务器上的网页和上一次打开这个服务器上的网页之间没有任何联系。HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。</p>
<h1 id="3-什么时候长连接、短连接？"><a href="#3-什么时候长连接、短连接？" class="headerlink" title="3. 什么时候长连接、短连接？"></a>3. 什么时候长连接、短连接？</h1><p>在HTTP&#x2F;1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，任务结束就中断连接。当客户端浏览器访问的某个HTML或其他类型的Web页中包含有其他的Web资源（如Javascript文件、图像文件、CSS文件等），每遇到这样一个Web资源，浏览器就会重新建立一个HTTP会话。<br>而从HTTP&#x2F;1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头加入这行代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Connecti:keep-alive</span><br></pre></td></tr></table></figure>
<p>在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的TCP连接不会关闭，客户端再次访问这个服务器时，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接需要客户端和服务端都支持长连接。<br>HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。</p>
<h2 id="3-1-TCP连接"><a href="#3-1-TCP连接" class="headerlink" title="3.1 TCP连接"></a>3.1 TCP连接</h2><p>当网络通信时采用TCP协议时，在真正的读写操作之前，客户端与服务器端之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时可以释放这个连接。连接的建立依靠“三次握手，而释放则需要”四次握手，所以每个连接的建立都是需要资源消耗和时间消耗的。<br>经典的三次握手建立连接示意图：<br><img src="/images/keepalive-1.jpg"><br><img src="/images/593345-20170204231325729-1747734402.png"></p>
<p>经典的四次握手关闭连接示意图：<br><img src="/images/keepalive-2.jpg"><br><img src="/images/593345-20170204231344745-1817753534.jpg"></p>
<h2 id="3-2-TCP短连接"><a href="#3-2-TCP短连接" class="headerlink" title="3.2 TCP短连接"></a>3.2 TCP短连接</h2><p>模拟一下TCP短连接的情况：client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次请求就完成了。这时候双方任意都可以发起close操作，不过一般都是client先发起close操作。上述可知，短连接一般只会在client&#x2F;server间传递一次请求操作。<br>短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段。</p>
<h2 id="3-3-TCP长连接"><a href="#3-3-TCP长连接" class="headerlink" title="3.3 TCP长连接"></a>3.3 TCP长连接</h2><p>我们再模拟一次长连接的情况：client向server发起连接，server接受连接，双方建立连接，client与server完成一次请求后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。<br>TCP的保活功能主要为服务器应用提供。如果客户端已经消失而连接未断开，则会使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，此时服务器将永远等待客户端的数据。保活功能就是试图在服务器端检测到这种半开放的连接。<br>如果一个给定的连接在两小时内没有任何动作，服务器就向客户发送一个探测报文，根据客户端主机响应探测4个客户端状态：</p>
<ul>
<li>客户主机依然正常运行，且服务器可达。此时客户的TCP响应正常，服务器将保活定时器复位。</li>
<li>客户主机已经崩溃，并且关闭或者正在重启。上述情况下客户端都不能响应TCP。服务端将无法收到客户端对探测的响应。服务器总共发送10个这样的探测，每个间隔75秒。若服务器没有收到任何一个响应，它就认为客户端已经关闭并终止连接。</li>
<li>客户端崩溃已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。</li>
<li>客户端正常运行，但是服务器不可达。这种情况与第二种状态类似。</li>
</ul>
<h1 id="4-长连接和短连接的优点和缺点"><a href="#4-长连接和短连接的优点和缺点" class="headerlink" title="4. 长连接和短连接的优点和缺点"></a>4. 长连接和短连接的优点和缺点</h1><p>由上可以看出，长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户端适合使用长连接。在长连接的应用场景下，client端一般不会主动关闭连接，当client与server之间的连接一直不关闭，随着客户端连接越来越多，server会保持过多连接，这时候server端需要采取一些策略，比如关闭一些长时间没有请求发生的连接，这样可以避免一些恶意连接导致server端服务受损；如果条件允许则可以限制每个客户端的最大长连接数，这样可以完全避免恶意的客户端拖垮整体后端服务。<br>短连接对于服务器来说管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户端请求频繁，将在TCP的建立和关闭操作上浪费较多的时间和带宽。<br>长连接和短连接的产生在于client和server采取的关闭策略。不同的应用场景适合采用不同的策略。<br>由上可以看出，长连接可以省去较多的TCP建立和关闭的操作，减少浪费，节约时间。对于频繁请求资源的客户来说，较适合长连接。不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接数越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写时间发生的连接，这样可以避免一些恶意连接导致server端服务手段；如果条件再允许就可以以客户端为颗粒度，限制每个客户端的最大长连接，这样可以完全避免某个客户端连累后端服务。<br>短连接赋予服务器来说，管理较为简单，存在的连接都是有用的连接，不需要额外的控制手段。但如果客户请求频繁，将在TCP的建立和关闭操作上康菲时间和带宽。<br>长连接和短连接的产生在于client和server采取的关闭策略，具体的应用场景采用具体的策略，没有十全十美的选择，只有合适的选择。</p>
<p>长连接短连接操作过程<br>短连接的操作步骤是：<br>建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接<br>长连接的操作步骤是：<br>建立连接——数据传输…（保持连接）…数据传输——关闭连接<br>什么时候用长连接，短连接？  　　<br>长连接多用于操作频繁，点对点的通讯，而且连接数不能太多情况，。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。<br>而像WEB网站的http服务一般都用短链接，因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好。</p>
<p>http和socket之长连接和短连接区别<br><a href="http://www.jianshu.com/p/b68d2b26f5f4">http://www.jianshu.com/p/b68d2b26f5f4</a></p>
<p>HTTP持久连接<br><a href="https://zh.wikipedia.org/wiki/HTTP%E6%8C%81%E4%B9%85%E8%BF%9E%E6%8E%A5">https://zh.wikipedia.org/wiki/HTTP%E6%8C%81%E4%B9%85%E8%BF%9E%E6%8E%A5</a></p>
]]></content>
      <tags>
        <tag>tcp</tag>
        <tag>http</tag>
      </tags>
  </entry>
</search>
